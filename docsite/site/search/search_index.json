{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Quantitative Imaging Toolkit (QIT) Welcome to the main documentation site for the Quantitative Imaging Toolkit (QIT), a software package for visualization, exploration, and analysis of neuroimaging datasets. QIT supports a variety of magnetic resonance imaging (MRI) with advanced capabilities for mapping brain microstructure and connectivity using diffusion MRI, and it also has an expanding set of microscopy tools. You can learn more from the pages below, and you can download the latest version on the Installation page. What is QIT? QIT is a software package of computational tools for the modeling, analysis, and visualization of scientific imaging data. It was specifically developed for tractography and microstructure analysis of diffusion magnetic resonance imaging datasets, but it has capabilities that are generally useful for other imaging modalities as well. It supports many different data types, including multi-channel volumetric datasets, multi-label masks, curves, triangle meshes, geometric primitives, tabular data, and spatial transformations. QIT provides an application called qitview for interactive 3D rendering and data analysis, as well as, a suite of command line tools available through a program named qit that provides a way to do batch processing and scripting. In addition, QIT also provides ways to integrate of these tools into grid computing environments and scientific workflows. Who develops QIT? QIT is designed, implemented, and supported by Ryan P. Cabeen, PhD as a software platform for neuroimaging research. You can learn more about research applications of QIT at https://cabeen.io . Development started in 2012 to support neuroimaging projects in the lab of David H. Laidlaw in the Department of Computer Science at Brown University . Currently, QIT is actively being developed and applied in the lab of Dr. Arthur Toga at the USC Laboratory of Neuro Imaging Resource . This work has been supported by the Brown University Graduate Award in Brain Science, National Institutes of Health (grant number P41EB015922), and the Chan Zuckerberg Imaging Scientist Program through grant number 2020-225670 from the Chan Zuckerberg Initiative DAF, an advised fund of Silicon Valley Community Foundation. Comments, criticism, and concerns are appreciated and can be directed cabeen@gmail.com .","title":"Home"},{"location":"#the-quantitative-imaging-toolkit-qit","text":"Welcome to the main documentation site for the Quantitative Imaging Toolkit (QIT), a software package for visualization, exploration, and analysis of neuroimaging datasets. QIT supports a variety of magnetic resonance imaging (MRI) with advanced capabilities for mapping brain microstructure and connectivity using diffusion MRI, and it also has an expanding set of microscopy tools. You can learn more from the pages below, and you can download the latest version on the Installation page.","title":"The Quantitative Imaging Toolkit (QIT)"},{"location":"#what-is-qit","text":"QIT is a software package of computational tools for the modeling, analysis, and visualization of scientific imaging data. It was specifically developed for tractography and microstructure analysis of diffusion magnetic resonance imaging datasets, but it has capabilities that are generally useful for other imaging modalities as well. It supports many different data types, including multi-channel volumetric datasets, multi-label masks, curves, triangle meshes, geometric primitives, tabular data, and spatial transformations. QIT provides an application called qitview for interactive 3D rendering and data analysis, as well as, a suite of command line tools available through a program named qit that provides a way to do batch processing and scripting. In addition, QIT also provides ways to integrate of these tools into grid computing environments and scientific workflows.","title":"What is QIT?"},{"location":"#who-develops-qit","text":"QIT is designed, implemented, and supported by Ryan P. Cabeen, PhD as a software platform for neuroimaging research. You can learn more about research applications of QIT at https://cabeen.io . Development started in 2012 to support neuroimaging projects in the lab of David H. Laidlaw in the Department of Computer Science at Brown University . Currently, QIT is actively being developed and applied in the lab of Dr. Arthur Toga at the USC Laboratory of Neuro Imaging Resource . This work has been supported by the Brown University Graduate Award in Brain Science, National Institutes of Health (grant number P41EB015922), and the Chan Zuckerberg Imaging Scientist Program through grant number 2020-225670 from the Chan Zuckerberg Initiative DAF, an advised fund of Silicon Valley Community Foundation. Comments, criticism, and concerns are appreciated and can be directed cabeen@gmail.com .","title":"Who develops QIT?"},{"location":"citation/","text":"How to cite QIT If you use QIT in your research, please: acknowledge with a reference to the Quantitative Imaging Toolkit (QIT) include a link to the website https://cabeen.io/qitwiki list any specific modules used and cite them accordingly (see usage page) cite the following abstract if for publication or grant proposal: Cabeen, R. P., Laidlaw, D. H., and Toga, A. W. (2018). Quantitative Imaging Toolkit: Software for Interactive 3D Visualization, Data Exploration, and Computational Analysis of Neuroimaging Datasets. Proceedings of the International Society for Magnetic Resonance in Medicine (ISMRM), 2854. @article{cabeen2018quantitative, title={Quantitative imaging toolkit: software for interactive 3D visualization, data exploration, and computational analysis of neuroimaging datasets}, author={Cabeen, Ryan P and Laidlaw, David H and Toga, Arthur W}, journal={ISMRM-ESMRMB Abstracts}, pages={12--14}, year={2018} }","title":"Citation"},{"location":"citation/#how-to-cite-qit","text":"If you use QIT in your research, please: acknowledge with a reference to the Quantitative Imaging Toolkit (QIT) include a link to the website https://cabeen.io/qitwiki list any specific modules used and cite them accordingly (see usage page) cite the following abstract if for publication or grant proposal: Cabeen, R. P., Laidlaw, D. H., and Toga, A. W. (2018). Quantitative Imaging Toolkit: Software for Interactive 3D Visualization, Data Exploration, and Computational Analysis of Neuroimaging Datasets. Proceedings of the International Society for Magnetic Resonance in Medicine (ISMRM), 2854. @article{cabeen2018quantitative, title={Quantitative imaging toolkit: software for interactive 3D visualization, data exploration, and computational analysis of neuroimaging datasets}, author={Cabeen, Ryan P and Laidlaw, David H and Toga, Arthur W}, journal={ISMRM-ESMRMB Abstracts}, pages={12--14}, year={2018} }","title":"How to cite QIT"},{"location":"colormaps/","text":"Applying colormaps in QIT QIT has a flexible system for coloring data to visualize volumes and vertex attributes on meshes and curves. At a high level, a colormap is a procedure for creating colors from numerical values associated with a dataset. There are three basic types of colormaps: solid , discrete , scalar , and vector , which each take different types of input data and produce different ranges of colors. In this section, we well discuss the basic colormap features that are available and how to use them in qitview . Choosing a colormap First, we will go over the basic interface for choosing colormaps. You can use QIT colormaps to visualize many different types of data, but the user interface is meant to be the same whether you are coloring a volume, mesh, or curves object. The image above shows you you can specify the coloring of a curves object. First, you check and select the data in the data list (A), then you open a control panel to see the coloring options (B), and optionally edit the colormap in the Settings menu (C). In the control panel, the three most important options are: Attribute : the vertex attribute used as input to the colormap Colortype : whether the colormap is solid , discrete , scalar , and vector Colormap : the specific colormap to use (depends on the Colortype So in this example, each vertex of the curves object is colored by extracting an FA value, converting it into a scalar , and supplying it to the scalar1 colormap to produce a reddish color. Each scalar colormap has an input range, consisting of a minimum and maximum value. You can detect these values from your specific dataset by clicking the Auto Min/Max button or selecting the Detect Intensity Range option in the Data menu. Now that you know how to choose a colormap, we will discuss the three types of colormaps and how you can edit them. Solid colormaps A solid colormap is the simplest kind of colormap. They are defined by a single color that is used to color all possible input data values. If you are just interested in the structure of your data, or you need to distinguish multiple objects in the view, solid colormap may be useful. If you open the Settings>Solid Colormaps menu, you can edit the available solid colormaps (see image shown above). There are many provided by default, which you can modify as you like. The image above shows the Name combobox for choosing the colormap to edit. Once you have selected your desired colormap, you can change the associated color by selecting the Set\u2026 button. The changes are not automatically applied to the data, so you should then select the Apply button to update the rendering. Discrete colormaps A discrete colormap is a colormap that takes integer-valued labels and maps them onto a discrete set of colors. These are useful for visualizing multi-label masks, where each label is associated with a different region-of-interest. They can also be useful for viewing the results of applying a segmentation algorithm to a mesh, curves, or volume object. If you open the Settings>Discrete Colormaps menu, you can edit the available discrete colormaps. There are many provided by default, which you can modify as you like. The menu above shows the Name combobox for choosing the colormap to edit. Once you have selected your desired colormap, can see that there is a collection of names, labels, alpha values, and colors. You can edit these values by clicking on them. If you craft a colormap by hand, you may also want to save it to a file using the Export button. You can then load it back in next time you use qitview with the Import button. The changes are not automatically applied to the data, so you should then select the Apply button to update the rendering. Scalar colormaps A scalar colormap is a colormap that takes a floating point number and maps it onto a some continuous range of colors. These are useful for visualizing model parameters and geometric features of a dataset. They are also a bit more complex than other colormaps, due to the many ways you can specify the mapping. Next, we will review how to define your own scalar colormaps. If you open the Settings>Scalar Colormaps menu, you can edit the available scalar colormaps. There are many provided by default, which you can modify as you like. The menu above shows the Name combobox for choosing the colormap to edit. Once you have selected your desired colormap, can a panel with many options. First, there is a normalization function, which is defined by the graph shown above. Second, there is a coloring, which comes from a fixed set provided with QIT. The normalization function is a user defined mapping from your dataset to values ranging from zero to one. You data is first passed through this normalization function, and the resulting values are used to select a color by interpolating the coloring, e.g. grayscale shown above. In practice, you need to specify the normalization function to make a useful colormap by changing the Min , the Max , and the transfer function. You can also set the min/max values manually in the colormap editor, or you can set them automatically from the data control panel or the Data>Detect Intensity Range menu. The transfer function is an advanced feature. You can usually leave it as it is, but if you would like to change it, read on. The image above shows an advanced colormap that uses custom ranges, a custom transfer function, and a different coloring. First, we set the coloring to jet using the Coloring combobox (A). There are dozens of other scalar colormaps, so please explore then options if you are curious. The panel below will then illustrate the coloring you chose. Then, we set the minimum (B) and maximum (C) to 1 and 10, respectively. Then, we specified the colormap by clicking on the panel and dragging control points around (D). You can remove the most recently added control point by selecting Pop , remove all control points using Reset , invert the transfer function using Invert . You can also mirror the min/max values using Mirror , e.g. to change the range to -10 to 10. You can save a PNG image depicting the colormap by selecting the Colorbar option, e.g. this can be used for making figures. If you craft a colormap by hand, you may also want to save it to a file using the Export button. You can then load it back in next time you use qitview with the Import button. The changes are not automatically applied to the data, so you should then select the Apply button to update the rendering (E). Vector colormaps vector colormaps provide a way to convert vector-valued data into colors. This feature is still under development, so there is only an RGB colormap available at the moment. This colormap take a 3D vector input and converts the x, y, and z values to red, green, and blue, respectively. Zero is mapped to the minimum color channel intensity, and one is mapped to the maximum color channel intensity. If the data is negative, the absolute value is taken, and large values are clamped to one.","title":"Colormaps"},{"location":"colormaps/#applying-colormaps-in-qit","text":"QIT has a flexible system for coloring data to visualize volumes and vertex attributes on meshes and curves. At a high level, a colormap is a procedure for creating colors from numerical values associated with a dataset. There are three basic types of colormaps: solid , discrete , scalar , and vector , which each take different types of input data and produce different ranges of colors. In this section, we well discuss the basic colormap features that are available and how to use them in qitview .","title":"Applying colormaps in QIT"},{"location":"colormaps/#choosing-a-colormap","text":"First, we will go over the basic interface for choosing colormaps. You can use QIT colormaps to visualize many different types of data, but the user interface is meant to be the same whether you are coloring a volume, mesh, or curves object. The image above shows you you can specify the coloring of a curves object. First, you check and select the data in the data list (A), then you open a control panel to see the coloring options (B), and optionally edit the colormap in the Settings menu (C). In the control panel, the three most important options are: Attribute : the vertex attribute used as input to the colormap Colortype : whether the colormap is solid , discrete , scalar , and vector Colormap : the specific colormap to use (depends on the Colortype So in this example, each vertex of the curves object is colored by extracting an FA value, converting it into a scalar , and supplying it to the scalar1 colormap to produce a reddish color. Each scalar colormap has an input range, consisting of a minimum and maximum value. You can detect these values from your specific dataset by clicking the Auto Min/Max button or selecting the Detect Intensity Range option in the Data menu. Now that you know how to choose a colormap, we will discuss the three types of colormaps and how you can edit them.","title":"Choosing a colormap"},{"location":"colormaps/#solid-colormaps","text":"A solid colormap is the simplest kind of colormap. They are defined by a single color that is used to color all possible input data values. If you are just interested in the structure of your data, or you need to distinguish multiple objects in the view, solid colormap may be useful. If you open the Settings>Solid Colormaps menu, you can edit the available solid colormaps (see image shown above). There are many provided by default, which you can modify as you like. The image above shows the Name combobox for choosing the colormap to edit. Once you have selected your desired colormap, you can change the associated color by selecting the Set\u2026 button. The changes are not automatically applied to the data, so you should then select the Apply button to update the rendering.","title":"Solid colormaps"},{"location":"colormaps/#discrete-colormaps","text":"A discrete colormap is a colormap that takes integer-valued labels and maps them onto a discrete set of colors. These are useful for visualizing multi-label masks, where each label is associated with a different region-of-interest. They can also be useful for viewing the results of applying a segmentation algorithm to a mesh, curves, or volume object. If you open the Settings>Discrete Colormaps menu, you can edit the available discrete colormaps. There are many provided by default, which you can modify as you like. The menu above shows the Name combobox for choosing the colormap to edit. Once you have selected your desired colormap, can see that there is a collection of names, labels, alpha values, and colors. You can edit these values by clicking on them. If you craft a colormap by hand, you may also want to save it to a file using the Export button. You can then load it back in next time you use qitview with the Import button. The changes are not automatically applied to the data, so you should then select the Apply button to update the rendering.","title":"Discrete colormaps"},{"location":"colormaps/#scalar-colormaps","text":"A scalar colormap is a colormap that takes a floating point number and maps it onto a some continuous range of colors. These are useful for visualizing model parameters and geometric features of a dataset. They are also a bit more complex than other colormaps, due to the many ways you can specify the mapping. Next, we will review how to define your own scalar colormaps. If you open the Settings>Scalar Colormaps menu, you can edit the available scalar colormaps. There are many provided by default, which you can modify as you like. The menu above shows the Name combobox for choosing the colormap to edit. Once you have selected your desired colormap, can a panel with many options. First, there is a normalization function, which is defined by the graph shown above. Second, there is a coloring, which comes from a fixed set provided with QIT. The normalization function is a user defined mapping from your dataset to values ranging from zero to one. You data is first passed through this normalization function, and the resulting values are used to select a color by interpolating the coloring, e.g. grayscale shown above. In practice, you need to specify the normalization function to make a useful colormap by changing the Min , the Max , and the transfer function. You can also set the min/max values manually in the colormap editor, or you can set them automatically from the data control panel or the Data>Detect Intensity Range menu. The transfer function is an advanced feature. You can usually leave it as it is, but if you would like to change it, read on. The image above shows an advanced colormap that uses custom ranges, a custom transfer function, and a different coloring. First, we set the coloring to jet using the Coloring combobox (A). There are dozens of other scalar colormaps, so please explore then options if you are curious. The panel below will then illustrate the coloring you chose. Then, we set the minimum (B) and maximum (C) to 1 and 10, respectively. Then, we specified the colormap by clicking on the panel and dragging control points around (D). You can remove the most recently added control point by selecting Pop , remove all control points using Reset , invert the transfer function using Invert . You can also mirror the min/max values using Mirror , e.g. to change the range to -10 to 10. You can save a PNG image depicting the colormap by selecting the Colorbar option, e.g. this can be used for making figures. If you craft a colormap by hand, you may also want to save it to a file using the Export button. You can then load it back in next time you use qitview with the Import button. The changes are not automatically applied to the data, so you should then select the Apply button to update the rendering (E).","title":"Scalar colormaps"},{"location":"colormaps/#vector-colormaps","text":"vector colormaps provide a way to convert vector-valued data into colors. This feature is still under development, so there is only an RGB colormap available at the moment. This colormap take a 3D vector input and converts the x, y, and z values to red, green, and blue, respectively. Zero is mapped to the minimum color channel intensity, and one is mapped to the maximum color channel intensity. If the data is negative, the absolute value is taken, and large values are clamped to one.","title":"Vector colormaps"},{"location":"datasets/","text":"What types of datasets work with QIT QIT is primarily designed to work with imaging data, but it also supports a variety of other datatypes that are useful in analyzing imaging data, such surfaces, curves, vectors, spatial transformations, etc. This page describes these various datatypes, how they\u2019re used, and what file formats are supported. Volume Volume is a dataset representing imaging data, whether that data is planar (2D) or volumetric (3D). If you have worked with images before, this will be familiar; however, there are several important differences. Most images encountered day-to-day are essentially a grid of pixels that each have a color, typically encoded by red, green, and blue intensities. By contrast, a Volume represents a 3D dimensional grid of voxels (volume elements) that each have an associated Vect . Sometimes this type of vector-valued volume is called a 4D image, where the fourth dimension identifies an entry of the vector at each voxel. Another way that a Volume is different from a typical image is that it has a spatial coordinate system. Most images have no need to store a pixel\u2019s size or the position of an image in space, but these can be important in scientific imaging. Therefore, we include a coordinate system that to describe the spatial layout of the 3D grid represented by each Volume , which includes: a vector indicating the position in space for the first voxel a quaternion indicating the rotation of the grid a vector indicating the size of the voxel These provide a connection between voxels on the 3D grid and their positions in 3D space (world coordinates). This is crucially important when analyzing volumetric data in conjunction with geometric objects and other volumes, which may have a different underlying 3D grid. Because the coordinate system includes a rotation, it can be quite confusing to discuss voxels and their world coordinates. To help clarify this, we have adopted the naming conventions for voxels and their positions. Each voxel is identified by a Sample object that stores three indices (i,j,k) that correspond to a location on the 3D grid. The position of each voxel is represented by a Vect that stores (x,y,z) in world coordinates. Supported file formats: Input: NIfTI , VTK , ImageIO , Text, UC Davis Stack Output: NIfTI , VTK , ImageIO , Text, UC Davis Stack , NRRD Mask Mask is a dataset for storing volumetric labels. A Mask has a similar voxel grid structure to a Volume , except the value stored at each voxel is instead an Integer label. These labels are meant to discriminate between foreground and background voxels, and well as to indicate multiple labels for region-of-interest analysis. Mask and Volume datasets can look similar when saved to disk, but they get treated very differently in QIT. For example, when rendering an image slice, a Mask will use a discrete colormap with different values for each label, while a Volume will use a scalar colormap with continuously changing values assigned to pixel intensities. Furthermore, QIT allows you to manually draw on masks, and some modules require data to be loaded as a Mask . However, it you accidentally load a Mask as a Volume , there are menu items for converting between the two. Supported file formats: same as Volume Mesh Mesh is a dataset representing a 3D triangulated surface. Each Mesh consists of a collection of Vertex objects, representing the vertices of the triangles, and a collection of Face objects representing the triangles themselves. The 3D position (and any other desired attribute) of the vertex is stored as named Vect objects associated with each Vertex . The only required attribute is coord , which represents the 3D position of the Vertex ; otherwise, attributes may have any name and Vect dimension. The triangles are expected to form a manifold surface mesh, meaning they are locally flat and that each edge is shared by at most two faces. The order of Vertex objects in each face reflects the orientation, and the edges of coincident faces should have opposite directions. A mesh may have many disconnected components or even form a \u201ctriangle soup\u201d of unconnected Face objects. They are commonly used to represent boundaries between structures, level sets, and glyphs. They are especially useful for illustrating 3D structures that are difficult to visualize from volumetric slices. Supported file formats: Input: VTK , GeomView OFF , Freesurfer , MINC OBJ , Freesurfer Mesh Output: VTK , STL , GeomView OFF , Freesurfer , MINC OBJ , Lightwave OBJ Curves Curves is a dataset representing 3D space curves. A Curves object consists of many Curve objects, which are each a sequence of points and line segments connecting them (also known as a simple polygonal chain or poly-line). Each point along each curve also has named Vect attributes. The only required attribute is coord , which represents the 3D position of the point. There\u2019s no constraint placed on the spacing between points, and the attributes can have any dimension. Typically, they are used to represent streamlines, iso-curves, or manually drawn landmark paths. In particular, tractography produces Curves from diffusion MR volumes and include a variety of diffusion-derived attributes. Supported file formats: Input: [http://www.vtk.org/VTK/img/file-formats.pdf VTK], [http://mrtrix.readthedocs.io/en/latest/getting_started/image_data.html MRtrix], [http://trackvis.org TrackVis], [http://www.bic.mni.mcgill.ca/users/mishkin/mni_obj_format.pdf MNI OBJ], [http://graphics.stanford.edu/projects/dti/software/pdb_format.html PDB], [http://jdtournier.github.io/mrtrix-0.2/appendix/mrtrix.html MRtrix], Brown VRL, Text Output: [http://www.vtk.org/VTK/img/file-formats.pdf VTK], [http://mrtrix.readthedocs.io/en/latest/getting_started/image_data.html MRtrix], [http://trackvis.org TrackVis], [http://www.bic.mni.mcgill.ca/users/mishkin/mni_obj_format.pdf MNI OBJ], [http://graphics.stanford.edu/projects/dti/software/pdb_format.html PDB], [http://jdtournier.github.io/mrtrix-0.2/appendix/mrtrix.html MRtrix], Brown VRL, Text, CSV Solids Solids is a dataset for represent regions of space using simple geometric objects. These are typically used for querying other datatypes. Each Solids is a collection of Sphere and Box objects. Each Sphere has a 3D center and radius, and any point inside the Sphere is considered part of the Solids . Each Box is a 3D axis-aligned cuboid with a 3D position and some width, length, and height. Any point inside the Box is considered part of the Solids . Solids objects are useful for querying spatial data, e.g. by selecting objects that are contained or intersect the solid. Because these kinds of queries can be repeated many times, the components of a Solid are kept simple for efficiency. Supported file formats: Input: JSON Output: JSON Vects Vects is a dataset representing a list of Vect objects. Vect is a basic vector dataset that represents an array of double floating point values. This could be 3D positions in space, the intensities of an image, parameters of some model, etc. This makes a Vect an important building block for the more complicated datasets described later in this page. Many Vect operations are supported, including addition, subtraction, scalar multiplication, dot products, norms, etc. When a Vect represents 3D spatial quantity, we use a right handed coordinate system when displayed. There are many uses for collections of vectors, for example, they can represent spatial landmarks, samples of some function, etc. Because other datasets consist of Vect objects, Vects is also useful for querying and communication between more complex datasets. Mathematically, a Vects consisting of 3D Vect s can be described: Supported file formats: Input: Text, CSV, RAW (64-bit floating point binary), Freesurfer Annot , Freesurfer Curv Output: Text, CSV, JSON Table Table is a dataset representing data organized into rows and named columns. Each column has a name, each row has a unique identifier, and each element in the table is either a String or an empty value null . Each row can be extracted to a Record object, which represents the row\u2019s entries with a key-value mapping. A Table is typically used to store statistical results and lookups that specify the relationship among labels, names, and colors. In practice, the entries can be a mixture of alphabetical and numerical strings. Supported file formats: Input: Text, CSV, [http://www.grahamwideman.com/gw/brain/fs/surfacefileformats.htm Freesurfer Annot] Output: Text, CSV, JSON Matrix Matrix is a dataset representing a two dimensional array of double floating point values. They are useful for representing linear transforms, graph connectivity, pairwise distances, and implementing algorithms that use linear algebra. Many Matrix operations are supported, including multiplication, determinants, singular value decompositions, eigenvalue decompositions, etc. Supported file formats: Input: Text, CSV, JSON, DTI-TK affine Output: Text, CSV, JSON Affine Affine is a dataset representing an affine spatial transformation, which is a linear transformation combined with a translation. Besides using them to transform Vects , they can be composed, inverted, orthogonalized, etc. They are most commonly used for linear and rigid-body registration of other datasets. A Matrix can be used as an Affine as well. Supported file formats: same as Matrix Deformation Deformation is a dataset representing arbitrary spatial transformations, that is, a mapping from 3D points to 3D points. The Deformation is can be created from either an Affine or a sampled function stored in a Volume . If a sampled function is used, the Volume representation is interpolated to produce a smooth function. These are typically used for non-linear registration, e.g. for deforming an image to some typical reference image. Supported file formats: same as Volume same as Affine Dataset Dataset is a generic type representing any data that doesn\u2019t fit into the above categories. For example, this could be parameters of a statistical model, such as a Gaussian mixture model. For simplicity, these are always saved to JSON. They aren\u2019t used by many modules though. Supported file formats: JSON","title":"Datasets"},{"location":"datasets/#what-types-of-datasets-work-with-qit","text":"QIT is primarily designed to work with imaging data, but it also supports a variety of other datatypes that are useful in analyzing imaging data, such surfaces, curves, vectors, spatial transformations, etc. This page describes these various datatypes, how they\u2019re used, and what file formats are supported.","title":"What types of datasets work with QIT"},{"location":"datasets/#volume","text":"Volume is a dataset representing imaging data, whether that data is planar (2D) or volumetric (3D). If you have worked with images before, this will be familiar; however, there are several important differences. Most images encountered day-to-day are essentially a grid of pixels that each have a color, typically encoded by red, green, and blue intensities. By contrast, a Volume represents a 3D dimensional grid of voxels (volume elements) that each have an associated Vect . Sometimes this type of vector-valued volume is called a 4D image, where the fourth dimension identifies an entry of the vector at each voxel. Another way that a Volume is different from a typical image is that it has a spatial coordinate system. Most images have no need to store a pixel\u2019s size or the position of an image in space, but these can be important in scientific imaging. Therefore, we include a coordinate system that to describe the spatial layout of the 3D grid represented by each Volume , which includes: a vector indicating the position in space for the first voxel a quaternion indicating the rotation of the grid a vector indicating the size of the voxel These provide a connection between voxels on the 3D grid and their positions in 3D space (world coordinates). This is crucially important when analyzing volumetric data in conjunction with geometric objects and other volumes, which may have a different underlying 3D grid. Because the coordinate system includes a rotation, it can be quite confusing to discuss voxels and their world coordinates. To help clarify this, we have adopted the naming conventions for voxels and their positions. Each voxel is identified by a Sample object that stores three indices (i,j,k) that correspond to a location on the 3D grid. The position of each voxel is represented by a Vect that stores (x,y,z) in world coordinates. Supported file formats: Input: NIfTI , VTK , ImageIO , Text, UC Davis Stack Output: NIfTI , VTK , ImageIO , Text, UC Davis Stack , NRRD","title":"Volume"},{"location":"datasets/#mask","text":"Mask is a dataset for storing volumetric labels. A Mask has a similar voxel grid structure to a Volume , except the value stored at each voxel is instead an Integer label. These labels are meant to discriminate between foreground and background voxels, and well as to indicate multiple labels for region-of-interest analysis. Mask and Volume datasets can look similar when saved to disk, but they get treated very differently in QIT. For example, when rendering an image slice, a Mask will use a discrete colormap with different values for each label, while a Volume will use a scalar colormap with continuously changing values assigned to pixel intensities. Furthermore, QIT allows you to manually draw on masks, and some modules require data to be loaded as a Mask . However, it you accidentally load a Mask as a Volume , there are menu items for converting between the two. Supported file formats: same as Volume","title":"Mask"},{"location":"datasets/#mesh","text":"Mesh is a dataset representing a 3D triangulated surface. Each Mesh consists of a collection of Vertex objects, representing the vertices of the triangles, and a collection of Face objects representing the triangles themselves. The 3D position (and any other desired attribute) of the vertex is stored as named Vect objects associated with each Vertex . The only required attribute is coord , which represents the 3D position of the Vertex ; otherwise, attributes may have any name and Vect dimension. The triangles are expected to form a manifold surface mesh, meaning they are locally flat and that each edge is shared by at most two faces. The order of Vertex objects in each face reflects the orientation, and the edges of coincident faces should have opposite directions. A mesh may have many disconnected components or even form a \u201ctriangle soup\u201d of unconnected Face objects. They are commonly used to represent boundaries between structures, level sets, and glyphs. They are especially useful for illustrating 3D structures that are difficult to visualize from volumetric slices. Supported file formats: Input: VTK , GeomView OFF , Freesurfer , MINC OBJ , Freesurfer Mesh Output: VTK , STL , GeomView OFF , Freesurfer , MINC OBJ , Lightwave OBJ","title":"Mesh"},{"location":"datasets/#curves","text":"Curves is a dataset representing 3D space curves. A Curves object consists of many Curve objects, which are each a sequence of points and line segments connecting them (also known as a simple polygonal chain or poly-line). Each point along each curve also has named Vect attributes. The only required attribute is coord , which represents the 3D position of the point. There\u2019s no constraint placed on the spacing between points, and the attributes can have any dimension. Typically, they are used to represent streamlines, iso-curves, or manually drawn landmark paths. In particular, tractography produces Curves from diffusion MR volumes and include a variety of diffusion-derived attributes. Supported file formats: Input: [http://www.vtk.org/VTK/img/file-formats.pdf VTK], [http://mrtrix.readthedocs.io/en/latest/getting_started/image_data.html MRtrix], [http://trackvis.org TrackVis], [http://www.bic.mni.mcgill.ca/users/mishkin/mni_obj_format.pdf MNI OBJ], [http://graphics.stanford.edu/projects/dti/software/pdb_format.html PDB], [http://jdtournier.github.io/mrtrix-0.2/appendix/mrtrix.html MRtrix], Brown VRL, Text Output: [http://www.vtk.org/VTK/img/file-formats.pdf VTK], [http://mrtrix.readthedocs.io/en/latest/getting_started/image_data.html MRtrix], [http://trackvis.org TrackVis], [http://www.bic.mni.mcgill.ca/users/mishkin/mni_obj_format.pdf MNI OBJ], [http://graphics.stanford.edu/projects/dti/software/pdb_format.html PDB], [http://jdtournier.github.io/mrtrix-0.2/appendix/mrtrix.html MRtrix], Brown VRL, Text, CSV","title":"Curves"},{"location":"datasets/#solids","text":"Solids is a dataset for represent regions of space using simple geometric objects. These are typically used for querying other datatypes. Each Solids is a collection of Sphere and Box objects. Each Sphere has a 3D center and radius, and any point inside the Sphere is considered part of the Solids . Each Box is a 3D axis-aligned cuboid with a 3D position and some width, length, and height. Any point inside the Box is considered part of the Solids . Solids objects are useful for querying spatial data, e.g. by selecting objects that are contained or intersect the solid. Because these kinds of queries can be repeated many times, the components of a Solid are kept simple for efficiency. Supported file formats: Input: JSON Output: JSON","title":"Solids"},{"location":"datasets/#vects","text":"Vects is a dataset representing a list of Vect objects. Vect is a basic vector dataset that represents an array of double floating point values. This could be 3D positions in space, the intensities of an image, parameters of some model, etc. This makes a Vect an important building block for the more complicated datasets described later in this page. Many Vect operations are supported, including addition, subtraction, scalar multiplication, dot products, norms, etc. When a Vect represents 3D spatial quantity, we use a right handed coordinate system when displayed. There are many uses for collections of vectors, for example, they can represent spatial landmarks, samples of some function, etc. Because other datasets consist of Vect objects, Vects is also useful for querying and communication between more complex datasets. Mathematically, a Vects consisting of 3D Vect s can be described: Supported file formats: Input: Text, CSV, RAW (64-bit floating point binary), Freesurfer Annot , Freesurfer Curv Output: Text, CSV, JSON","title":"Vects"},{"location":"datasets/#table","text":"Table is a dataset representing data organized into rows and named columns. Each column has a name, each row has a unique identifier, and each element in the table is either a String or an empty value null . Each row can be extracted to a Record object, which represents the row\u2019s entries with a key-value mapping. A Table is typically used to store statistical results and lookups that specify the relationship among labels, names, and colors. In practice, the entries can be a mixture of alphabetical and numerical strings. Supported file formats: Input: Text, CSV, [http://www.grahamwideman.com/gw/brain/fs/surfacefileformats.htm Freesurfer Annot] Output: Text, CSV, JSON","title":"Table"},{"location":"datasets/#matrix","text":"Matrix is a dataset representing a two dimensional array of double floating point values. They are useful for representing linear transforms, graph connectivity, pairwise distances, and implementing algorithms that use linear algebra. Many Matrix operations are supported, including multiplication, determinants, singular value decompositions, eigenvalue decompositions, etc. Supported file formats: Input: Text, CSV, JSON, DTI-TK affine Output: Text, CSV, JSON","title":"Matrix"},{"location":"datasets/#affine","text":"Affine is a dataset representing an affine spatial transformation, which is a linear transformation combined with a translation. Besides using them to transform Vects , they can be composed, inverted, orthogonalized, etc. They are most commonly used for linear and rigid-body registration of other datasets. A Matrix can be used as an Affine as well. Supported file formats: same as Matrix","title":"Affine"},{"location":"datasets/#deformation","text":"Deformation is a dataset representing arbitrary spatial transformations, that is, a mapping from 3D points to 3D points. The Deformation is can be created from either an Affine or a sampled function stored in a Volume . If a sampled function is used, the Volume representation is interpolated to produce a smooth function. These are typically used for non-linear registration, e.g. for deforming an image to some typical reference image. Supported file formats: same as Volume same as Affine","title":"Deformation"},{"location":"datasets/#dataset","text":"Dataset is a generic type representing any data that doesn\u2019t fit into the above categories. For example, this could be parameters of a statistical model, such as a Gaussian mixture model. For simplicity, these are always saved to JSON. They aren\u2019t used by many modules though. Supported file formats: JSON","title":"Dataset"},{"location":"dmri-tutorial/","text":"Diffusion MRI analysis with QIT This page provides a tutorial for diffusion tensor imaging (DTI) with QIT. You\u2019ll learn how to install QIT, download sample data, estimate and visualize DTI data, and perform manual seed-based tractography. Setup Before starting the tutorial, you\u2019ll need a few things. First, we will make sure you have the necessary dependencies; then, we will download and install QIT; finally, we will download the sample data. Installation First, make sure you\u2019ve installed QIT and its dependencies by following the [[Installation]] instructions. You won\u2019t need the advanced dependencies for this tutorial, so you can skip that. Downloading the sample dataset Next, we\u2019ll download the sample dataset, which is available here: http://cabeen.io/download/dmri.tutorial.data.zip After you decompress the archive, you should find these files inside: input/dwi.nii.gz : a diffusion weighted MR image volume input/mask.nii.gz : a brain mask input/bvecs.txt : a b-vectors file input/bvals.txt : a b-values file There are other files in the archive, but the ones above are strictly required for the tutorial. This dataset represents a basic 60 direction single shell diffusion MRI scan acquired on a 1.5T scanner. The dataset is described in more detail here Tutorial Now that we have installed QIT and downloaded the sample data, we\u2019ll go over how to do a basic DTI analysis. Starting QIT First, we\u2019ll start qitview . You can do this by running qitview or qitview.py by either double clicking in your file explorer or executing them on the command line . Once you\u2019ve started the program, you should see console messages about the progress and a window that looks like the image below. There are three sections to the viewer: Data Stage : the panel on the left, where data is visualized Data Workspace : the panel on the top right, where a list of loaded data is shown Data Controls : the panel on the bottom right, where you control how the selected data is visualized File loader Next, we\u2019ll open the file loader. You can find the file loader by clicking the File menu and then clicking \u201cLoad Files\u2026\u201d: Load tutorial data Next, we\u2019ll load the sample data for the tutorial. The file loader lets you open a list of files in batch mode. You can add files to the list by clicking the Add more files button and selecting the file in the file chooser. Each entry in the list displays the file type and file name. The file type is automatically detected in this data, so you don\u2019t need to change the file type (but be careful about the datatype using QIT later on). You should add these files to the list: input/bvecs.txt input/dwi.nii.gz input/mask.nii.gz After that, you can load the files by clicking Load files into workspace : View the data Next, we\u2019ll visualize the volumetric data. First, you should select dwi.nii.gz and click the box next to it. You can change the view by clicking on the data stage and dragging the mouse. Then, you should open the Slice Rendering panel and uncheck the boxes next to Slice I and Slice J . Now, only an axial slice should be visible. Opening a Module Next, we\u2019ll do some data processing. QIT has many modules that each implement a different algorithm. We\u2019ll start by fitting diffusion tensors to the tutorial dataset. You can open the VolumeTensorFit module by opening this menu: Fitting Tensors Now, you should see a window for the VolumeTensorFit module. This provides a way to specify the input data, parameter settings, and the destination for the output. You should make sure the input is set to dwi.nii.gz and gradients is set to bvecs.txt . Then, you should open the Optional Input panel and set mask to mask.nii.gz . Then, you should select the Apply button. The module will then ask for a name for the output, which you can set to models.dti : Glyph Visualization Now, you should have models.dti in your workspace. Next, we\u2019ll create create a glyph visualization. You should first select models.dti , click the box next to it, and expand the Glyph Rendering panel. Then click the Visible checkbox and select TensorEllipsoid from the dropdown menu: Tensor Ellipsoid Glyphs Now, you should see ellipsoid glyphs representing the DTI volume: Extract a tensor model feature Next, we\u2019ll extract a scalar feature from the DTI volume. For this, you should open the VolumeModelFeature module, set the input to models.dti , ensure feature is set to FA , and select Apply . It will ask for a name for the output, which you can set to fa : Visualize fractional anisotropy Now, we\u2019ll visualize the fractional anisotropy image. You should first select fa in the workspace and check the box next to it. Then you should open the Slice Rendering panel and change the colormap to scalar2 . You have to do this because scalar1 is being used for dwi.nii.gz and they have different intensity ranges. Create a mask Next, we\u2019ll create a mask for seed-based tractography. You should Right-Click (or Control+Click) the fa volume and select Create Mask from the contextual menu. You will be asked for a name for the new mask, which you can set to seed . This mask will be used for initiating tractography. In the next section, we\u2019ll draw a region in the mask. Draw a mask Next, we\u2019re going to draw a region in the mask. First, you should select the mask seed , make sure the box next to it is checked, and then open the Drawing panel. You can change the drawing mode and characteristics of the stencil with this panel, but for now, you can leave the settings as they are. Now, you can draw on the mask by dragging the mouse over the image while holding down Alt+Control (or Option+Control on macOS). If you hover the mouse while holding Alt+Control, it will show you a drawing stencil, and clicking will stamp the highlighted voxels. You can draw a region like the one shown below. If you make a mistake, you can hold down Shift+Alt+Control to erase instead. Extracting tracks Next, we\u2019ll create a tractography model using the seed mask. First, you should open the VolumeModelTrackStreamline module, as shown below. You should set the input to models.dti , open the Optional Input panel, and set seedMask to seed . In the next section, we\u2019ll update some more parameters. We\u2019ll set a couple more parameters. First, close the Optional Input panel and open the Parameters panel. Then, you can set samples to 5 , which will specify that five seeds per voxel should be used. Then, you can set and min to 0.15 , which will specify that tracking should only proceed when fractional anisotropy is above 0.15. Then, you can select \u201cApply\u201d to create the tracks. It will ask for a name for the output, which you can set to tracks . Visualize tracks Now, we have tractography curves. You can show the results by selecting tracks and checking the box next to it. These are 3D curves, so you can rotate the camera to see their shape. You can also open the Rendering panel to change how the curves are visualized. Conclusion Congratulations! You\u2019ve completed your first analysis of diffusion MRI data. Feel free to experiment with other modules and settings in qitview or ask around if you\u2019d like to know more about what else you can do.","title":"Diffusion MRI Tutorial"},{"location":"dmri-tutorial/#diffusion-mri-analysis-with-qit","text":"This page provides a tutorial for diffusion tensor imaging (DTI) with QIT. You\u2019ll learn how to install QIT, download sample data, estimate and visualize DTI data, and perform manual seed-based tractography.","title":"Diffusion MRI analysis with QIT"},{"location":"dmri-tutorial/#setup","text":"Before starting the tutorial, you\u2019ll need a few things. First, we will make sure you have the necessary dependencies; then, we will download and install QIT; finally, we will download the sample data.","title":"Setup"},{"location":"dmri-tutorial/#installation","text":"First, make sure you\u2019ve installed QIT and its dependencies by following the [[Installation]] instructions. You won\u2019t need the advanced dependencies for this tutorial, so you can skip that.","title":"Installation"},{"location":"dmri-tutorial/#downloading-the-sample-dataset","text":"Next, we\u2019ll download the sample dataset, which is available here: http://cabeen.io/download/dmri.tutorial.data.zip After you decompress the archive, you should find these files inside: input/dwi.nii.gz : a diffusion weighted MR image volume input/mask.nii.gz : a brain mask input/bvecs.txt : a b-vectors file input/bvals.txt : a b-values file There are other files in the archive, but the ones above are strictly required for the tutorial. This dataset represents a basic 60 direction single shell diffusion MRI scan acquired on a 1.5T scanner. The dataset is described in more detail here","title":"Downloading the sample dataset"},{"location":"dmri-tutorial/#tutorial","text":"Now that we have installed QIT and downloaded the sample data, we\u2019ll go over how to do a basic DTI analysis.","title":"Tutorial"},{"location":"dmri-tutorial/#starting-qit","text":"First, we\u2019ll start qitview . You can do this by running qitview or qitview.py by either double clicking in your file explorer or executing them on the command line . Once you\u2019ve started the program, you should see console messages about the progress and a window that looks like the image below. There are three sections to the viewer: Data Stage : the panel on the left, where data is visualized Data Workspace : the panel on the top right, where a list of loaded data is shown Data Controls : the panel on the bottom right, where you control how the selected data is visualized","title":"Starting QIT"},{"location":"dmri-tutorial/#file-loader","text":"Next, we\u2019ll open the file loader. You can find the file loader by clicking the File menu and then clicking \u201cLoad Files\u2026\u201d:","title":"File loader"},{"location":"dmri-tutorial/#load-tutorial-data","text":"Next, we\u2019ll load the sample data for the tutorial. The file loader lets you open a list of files in batch mode. You can add files to the list by clicking the Add more files button and selecting the file in the file chooser. Each entry in the list displays the file type and file name. The file type is automatically detected in this data, so you don\u2019t need to change the file type (but be careful about the datatype using QIT later on). You should add these files to the list: input/bvecs.txt input/dwi.nii.gz input/mask.nii.gz After that, you can load the files by clicking Load files into workspace :","title":"Load tutorial data"},{"location":"dmri-tutorial/#view-the-data","text":"Next, we\u2019ll visualize the volumetric data. First, you should select dwi.nii.gz and click the box next to it. You can change the view by clicking on the data stage and dragging the mouse. Then, you should open the Slice Rendering panel and uncheck the boxes next to Slice I and Slice J . Now, only an axial slice should be visible.","title":"View the data"},{"location":"dmri-tutorial/#opening-a-module","text":"Next, we\u2019ll do some data processing. QIT has many modules that each implement a different algorithm. We\u2019ll start by fitting diffusion tensors to the tutorial dataset. You can open the VolumeTensorFit module by opening this menu:","title":"Opening a Module"},{"location":"dmri-tutorial/#fitting-tensors","text":"Now, you should see a window for the VolumeTensorFit module. This provides a way to specify the input data, parameter settings, and the destination for the output. You should make sure the input is set to dwi.nii.gz and gradients is set to bvecs.txt . Then, you should open the Optional Input panel and set mask to mask.nii.gz . Then, you should select the Apply button. The module will then ask for a name for the output, which you can set to models.dti :","title":"Fitting Tensors"},{"location":"dmri-tutorial/#glyph-visualization","text":"Now, you should have models.dti in your workspace. Next, we\u2019ll create create a glyph visualization. You should first select models.dti , click the box next to it, and expand the Glyph Rendering panel. Then click the Visible checkbox and select TensorEllipsoid from the dropdown menu:","title":"Glyph Visualization"},{"location":"dmri-tutorial/#tensor-ellipsoid-glyphs","text":"Now, you should see ellipsoid glyphs representing the DTI volume:","title":"Tensor Ellipsoid Glyphs"},{"location":"dmri-tutorial/#extract-a-tensor-model-feature","text":"Next, we\u2019ll extract a scalar feature from the DTI volume. For this, you should open the VolumeModelFeature module, set the input to models.dti , ensure feature is set to FA , and select Apply . It will ask for a name for the output, which you can set to fa :","title":"Extract a tensor model feature"},{"location":"dmri-tutorial/#visualize-fractional-anisotropy","text":"Now, we\u2019ll visualize the fractional anisotropy image. You should first select fa in the workspace and check the box next to it. Then you should open the Slice Rendering panel and change the colormap to scalar2 . You have to do this because scalar1 is being used for dwi.nii.gz and they have different intensity ranges.","title":"Visualize fractional anisotropy"},{"location":"dmri-tutorial/#create-a-mask","text":"Next, we\u2019ll create a mask for seed-based tractography. You should Right-Click (or Control+Click) the fa volume and select Create Mask from the contextual menu. You will be asked for a name for the new mask, which you can set to seed . This mask will be used for initiating tractography. In the next section, we\u2019ll draw a region in the mask.","title":"Create a mask"},{"location":"dmri-tutorial/#draw-a-mask","text":"Next, we\u2019re going to draw a region in the mask. First, you should select the mask seed , make sure the box next to it is checked, and then open the Drawing panel. You can change the drawing mode and characteristics of the stencil with this panel, but for now, you can leave the settings as they are. Now, you can draw on the mask by dragging the mouse over the image while holding down Alt+Control (or Option+Control on macOS). If you hover the mouse while holding Alt+Control, it will show you a drawing stencil, and clicking will stamp the highlighted voxels. You can draw a region like the one shown below. If you make a mistake, you can hold down Shift+Alt+Control to erase instead.","title":"Draw a mask"},{"location":"dmri-tutorial/#extracting-tracks","text":"Next, we\u2019ll create a tractography model using the seed mask. First, you should open the VolumeModelTrackStreamline module, as shown below. You should set the input to models.dti , open the Optional Input panel, and set seedMask to seed . In the next section, we\u2019ll update some more parameters. We\u2019ll set a couple more parameters. First, close the Optional Input panel and open the Parameters panel. Then, you can set samples to 5 , which will specify that five seeds per voxel should be used. Then, you can set and min to 0.15 , which will specify that tracking should only proceed when fractional anisotropy is above 0.15. Then, you can select \u201cApply\u201d to create the tracks. It will ask for a name for the output, which you can set to tracks .","title":"Extracting tracks"},{"location":"dmri-tutorial/#visualize-tracks","text":"Now, we have tractography curves. You can show the results by selecting tracks and checking the box next to it. These are 3D curves, so you can rotate the camera to see their shape. You can also open the Rendering panel to change how the curves are visualized.","title":"Visualize tracks"},{"location":"dmri-tutorial/#conclusion","text":"Congratulations! You\u2019ve completed your first analysis of diffusion MRI data. Feel free to experiment with other modules and settings in qitview or ask around if you\u2019d like to know more about what else you can do.","title":"Conclusion"},{"location":"freesurfer/","text":"Exploring Freesurfer data with QIT This page provides a tutorial for visualizing [https://surfer.nmr.mgh.harvard.edu/ FreeSurfer] results with QIT. You\u2019ll learn how to visualize volumetric segmentations of T1 MR images and visualize surface models of cortical and subcortical structures. Setup Before starting the tutorial, you\u2019ll need a few things. First, we will make sure you have the necessary dependencies; then, we will download and install QIT; finally, we will download the sample data. Installation First, make sure you\u2019ve installed QIT and its dependencies by following the [[Installation]] instructions. You won\u2019t need the advanced dependencies for this tutorial, so you can skip that. Downloading the sample dataset Next, we\u2019ll download the sample dataset, which is available here: http://cabeen.io/download/fs.tutorial.zip After you decompress the archive, you should find these directories: freesurfer : the subject directory produced by FreeSurfer fsimport : the FreeSurfer data imported to a QIT-compatible format There are other files in the archive, but the ones above are strictly required for the tutorial. This dataset was processed from a 1mm isotropic MPRAGE T1-weighted MRI acquired on a 1.5T scanner. The dataset is described in more detail here Optional: Converting FreeSurfer results FreeSurfer uses its own file formats, and to visualize the results in QIT, you have to import them to more general purpose file formats. To simplify the tutorial, we have already done this for you and saved the results to fsimport . However, if you want to visualize other datasets, you will have to similarly import that data using a command like this: qit FreesurferImport --input freesurfer --brain --segmentations --surfaces --output fsimport This program assumes you\u2019ve installed FreeSurfer on your machine and put the binaries on the path, like described in the advanced section of the [[Installation]] page. Tutorial Now that we have installed QIT and downloaded the sample data, we\u2019ll go over how to visualize the volumetric and geometric models produced by FreeSurfer. Starting QIT First, we\u2019ll start qitview . You can do this by running qitview or qitview.py by either double clicking in your file explorer or executing them on the command line . Once you\u2019ve started the program, you should see console messages about the progress and a window that looks like the image below. There are three sections to the viewer: Data Stage : the panel on the left, where data is visualized Data Workspace : the panel on the top right, where a list of loaded data is shown Data Controls : the panel on the bottom right, where you control how the selected data is visualized File loader Next, we\u2019ll open the file loader. You can find the file loader by clicking the File menu and then clicking \u201cLoad Files\u2026\u201d. Next, we\u2019ll load the sample data for the tutorial. The file loader lets you open a list of files in batch mode. You can add files to the list by clicking the Add more files button (A) and selecting the file in the file chooser. Each entry in the list displays the file type and file name. You should add these files to the list: fsimport/brain.nii.gz set to datatype Volume fsimport/aparc+aseg.nii.gz set to datatype Mask fsimport/scgm/rois.nii.gz set to datatype Mask fsimport/surfaces/lh.pial.vtk set to datatype Mesh fsimport/surfaces/rh.pial.vtk set to datatype Mesh After that, you need to set the file types to the types listed above (B). Then, you can load the files by clicking Load files into workspace (C): View the T1-weighted MRI Next, we will visualize the input T1-weighted MRI dataset. First, click the box next to brain.nii.gz (A). Next, we will isolate a coronal slice by first unchecking the Slice I and Slice K checkboxes (B) and then changing the Slice J index to 155 (C): View the volumetric segmentation Next, we\u2019ll visualize the volumetric segmentation. First, you should select aparc+aseg.nii.gz and click the box next to it. Next, you should change the colormap to freesurfer . Now you should see an overlay segmentation on the T1 image: Extract subcortical surfaces Next, we\u2019ll visualize the subcortical structures. First, you should select scgm.rois.nii.gz and click the box next to it. Then, you should open the module MaskMarchingCubes . You should set the input to scgm.rois.nii.gz (A) and select the Apply button (B). The module will then ask you for a name for the new mesh object, which you can call scgm : Smooth the subcortical surfaces The surface models created by the previous step are jagged due to aliasing, so next, we\u2019ll smooth surface to reduce the discretization effects. First, you should open the model MeshSmooth . You should ensure the input is scgm (A) and then set the output to scgm . This will replace the original scgm with the new smoothed version. Then select the Apply button to run the module (C): View the subcortical surfaces Next, we will visualize the surface models of the subcortical structures. First, you should select the scgm object and select the box next to the name (A). This needs some color to distinguish the different structures, so we can change the Attribute to label (B) and then change the Colormap to discrete (C). This should now show meshes with distinct colors for each structure: View the cortical surface Next, we will visualize the cortical surface. First, you should hide the other objects by unchecking their visibility boxes. Then, you should check the boxes next to lh.pial.vtk and rh.pial.vtk. . This should show the two hemispheres: View cortical thickness Next, we will visualize cortical thickness on the surface. First, you should change the Attribute to thickness (A), then change the Colortype to scalar (B), and then change the Colormap to scalar2 . We need to change the scalar colormap because the T1 MRI is being visualized using scalar1 . Next, you should select the Auto Min/Max button to adjust the colormap range to match the cortical thickness values. Currently, it\u2019s showing grayscale, so you should select the Edit Scalar Colormap to pick something else (E): Change the cortical thickness coloring Now, you should see a colormap editor window. You should first change the Name to scalar2 (A) and then change the Colormap to diverging (B). You can also experiment with other colormaps besides that one. Then, to apply the colormap, you should select the Apply button (C): View the cortical parcellation Finally, we will view the cortical parcellation from the Desikan-Killiany atlas. You should first make sure the lh.pial.vtk object is selected. Then, you should change the Attribute to aparc (A), and change the Colortype to discrete (B), and then change the Colormap to freesurfer . You should then see regions visualized on the cortical surface: Extract statistics (Optional) You can also use QIT to extract statistical tables from a FreeSurfer subject directory: $ qfsmeas freesurfer fsmap The output directory fsmap will contain a number of CSV files storing morphometric variables for a variety of cortical, white matter, and subcortical structures. These are a bit more friendly for loading data into R, etc. than what FreeSurfer provides by default. For example: $ ls fsmap BA.area.csv aparc.a2009s.volume.csv BA.meancurv.csv aparc.area.csv BA.thickness.csv aparc.meancurv.csv BA.volume.csv aparc.thickness.csv aparc.a2009s.area.csv aparc.volume.csv aparc.a2009s.meancurv.csv aseg.stats.csv aparc.a2009s.thickness.csv wmparc.stats.csv $ head fsmap/aseg.stats.csv name,value Left-Lateral-Ventricle,4355 Left-Inf-Lat-Vent,289 Left-Cerebellum-White-Matter,12850 Left-Cerebellum-Cortex,42203 Left-Thalamus-Proper,6235 Left-Caudate,3086 Left-Putamen,5655 Left-Pallidum,1594 3rd-Ventricle,677 Conclusion Congratulations! You\u2019ve completed the tutorial for visualizing FreeSurfer results. Feel free to experiment with other surface attributes and settings in qitview or ask around if you\u2019d like to know more about what else you can do.","title":"Freesurfer Tutorial"},{"location":"freesurfer/#exploring-freesurfer-data-with-qit","text":"This page provides a tutorial for visualizing [https://surfer.nmr.mgh.harvard.edu/ FreeSurfer] results with QIT. You\u2019ll learn how to visualize volumetric segmentations of T1 MR images and visualize surface models of cortical and subcortical structures.","title":"Exploring Freesurfer data with QIT"},{"location":"freesurfer/#setup","text":"Before starting the tutorial, you\u2019ll need a few things. First, we will make sure you have the necessary dependencies; then, we will download and install QIT; finally, we will download the sample data.","title":"Setup"},{"location":"freesurfer/#installation","text":"First, make sure you\u2019ve installed QIT and its dependencies by following the [[Installation]] instructions. You won\u2019t need the advanced dependencies for this tutorial, so you can skip that.","title":"Installation"},{"location":"freesurfer/#downloading-the-sample-dataset","text":"Next, we\u2019ll download the sample dataset, which is available here: http://cabeen.io/download/fs.tutorial.zip After you decompress the archive, you should find these directories: freesurfer : the subject directory produced by FreeSurfer fsimport : the FreeSurfer data imported to a QIT-compatible format There are other files in the archive, but the ones above are strictly required for the tutorial. This dataset was processed from a 1mm isotropic MPRAGE T1-weighted MRI acquired on a 1.5T scanner. The dataset is described in more detail here","title":"Downloading the sample dataset"},{"location":"freesurfer/#optional-converting-freesurfer-results","text":"FreeSurfer uses its own file formats, and to visualize the results in QIT, you have to import them to more general purpose file formats. To simplify the tutorial, we have already done this for you and saved the results to fsimport . However, if you want to visualize other datasets, you will have to similarly import that data using a command like this: qit FreesurferImport --input freesurfer --brain --segmentations --surfaces --output fsimport This program assumes you\u2019ve installed FreeSurfer on your machine and put the binaries on the path, like described in the advanced section of the [[Installation]] page.","title":"Optional: Converting FreeSurfer results"},{"location":"freesurfer/#tutorial","text":"Now that we have installed QIT and downloaded the sample data, we\u2019ll go over how to visualize the volumetric and geometric models produced by FreeSurfer.","title":"Tutorial"},{"location":"freesurfer/#starting-qit","text":"First, we\u2019ll start qitview . You can do this by running qitview or qitview.py by either double clicking in your file explorer or executing them on the command line . Once you\u2019ve started the program, you should see console messages about the progress and a window that looks like the image below. There are three sections to the viewer: Data Stage : the panel on the left, where data is visualized Data Workspace : the panel on the top right, where a list of loaded data is shown Data Controls : the panel on the bottom right, where you control how the selected data is visualized","title":"Starting QIT"},{"location":"freesurfer/#file-loader","text":"Next, we\u2019ll open the file loader. You can find the file loader by clicking the File menu and then clicking \u201cLoad Files\u2026\u201d. Next, we\u2019ll load the sample data for the tutorial. The file loader lets you open a list of files in batch mode. You can add files to the list by clicking the Add more files button (A) and selecting the file in the file chooser. Each entry in the list displays the file type and file name. You should add these files to the list: fsimport/brain.nii.gz set to datatype Volume fsimport/aparc+aseg.nii.gz set to datatype Mask fsimport/scgm/rois.nii.gz set to datatype Mask fsimport/surfaces/lh.pial.vtk set to datatype Mesh fsimport/surfaces/rh.pial.vtk set to datatype Mesh After that, you need to set the file types to the types listed above (B). Then, you can load the files by clicking Load files into workspace (C):","title":"File loader"},{"location":"freesurfer/#view-the-t1-weighted-mri","text":"Next, we will visualize the input T1-weighted MRI dataset. First, click the box next to brain.nii.gz (A). Next, we will isolate a coronal slice by first unchecking the Slice I and Slice K checkboxes (B) and then changing the Slice J index to 155 (C):","title":"View the T1-weighted MRI"},{"location":"freesurfer/#view-the-volumetric-segmentation","text":"Next, we\u2019ll visualize the volumetric segmentation. First, you should select aparc+aseg.nii.gz and click the box next to it. Next, you should change the colormap to freesurfer . Now you should see an overlay segmentation on the T1 image:","title":"View the volumetric segmentation"},{"location":"freesurfer/#extract-subcortical-surfaces","text":"Next, we\u2019ll visualize the subcortical structures. First, you should select scgm.rois.nii.gz and click the box next to it. Then, you should open the module MaskMarchingCubes . You should set the input to scgm.rois.nii.gz (A) and select the Apply button (B). The module will then ask you for a name for the new mesh object, which you can call scgm :","title":"Extract subcortical surfaces"},{"location":"freesurfer/#smooth-the-subcortical-surfaces","text":"The surface models created by the previous step are jagged due to aliasing, so next, we\u2019ll smooth surface to reduce the discretization effects. First, you should open the model MeshSmooth . You should ensure the input is scgm (A) and then set the output to scgm . This will replace the original scgm with the new smoothed version. Then select the Apply button to run the module (C):","title":"Smooth the subcortical surfaces"},{"location":"freesurfer/#view-the-subcortical-surfaces","text":"Next, we will visualize the surface models of the subcortical structures. First, you should select the scgm object and select the box next to the name (A). This needs some color to distinguish the different structures, so we can change the Attribute to label (B) and then change the Colormap to discrete (C). This should now show meshes with distinct colors for each structure:","title":"View the subcortical surfaces"},{"location":"freesurfer/#view-the-cortical-surface","text":"Next, we will visualize the cortical surface. First, you should hide the other objects by unchecking their visibility boxes. Then, you should check the boxes next to lh.pial.vtk and rh.pial.vtk. . This should show the two hemispheres:","title":"View the cortical surface"},{"location":"freesurfer/#view-cortical-thickness","text":"Next, we will visualize cortical thickness on the surface. First, you should change the Attribute to thickness (A), then change the Colortype to scalar (B), and then change the Colormap to scalar2 . We need to change the scalar colormap because the T1 MRI is being visualized using scalar1 . Next, you should select the Auto Min/Max button to adjust the colormap range to match the cortical thickness values. Currently, it\u2019s showing grayscale, so you should select the Edit Scalar Colormap to pick something else (E):","title":"View cortical thickness"},{"location":"freesurfer/#change-the-cortical-thickness-coloring","text":"Now, you should see a colormap editor window. You should first change the Name to scalar2 (A) and then change the Colormap to diverging (B). You can also experiment with other colormaps besides that one. Then, to apply the colormap, you should select the Apply button (C):","title":"Change the cortical thickness coloring"},{"location":"freesurfer/#view-the-cortical-parcellation","text":"Finally, we will view the cortical parcellation from the Desikan-Killiany atlas. You should first make sure the lh.pial.vtk object is selected. Then, you should change the Attribute to aparc (A), and change the Colortype to discrete (B), and then change the Colormap to freesurfer . You should then see regions visualized on the cortical surface:","title":"View the cortical parcellation"},{"location":"freesurfer/#extract-statistics-optional","text":"You can also use QIT to extract statistical tables from a FreeSurfer subject directory: $ qfsmeas freesurfer fsmap The output directory fsmap will contain a number of CSV files storing morphometric variables for a variety of cortical, white matter, and subcortical structures. These are a bit more friendly for loading data into R, etc. than what FreeSurfer provides by default. For example: $ ls fsmap BA.area.csv aparc.a2009s.volume.csv BA.meancurv.csv aparc.area.csv BA.thickness.csv aparc.meancurv.csv BA.volume.csv aparc.thickness.csv aparc.a2009s.area.csv aparc.volume.csv aparc.a2009s.meancurv.csv aseg.stats.csv aparc.a2009s.thickness.csv wmparc.stats.csv $ head fsmap/aseg.stats.csv name,value Left-Lateral-Ventricle,4355 Left-Inf-Lat-Vent,289 Left-Cerebellum-White-Matter,12850 Left-Cerebellum-Cortex,42203 Left-Thalamus-Proper,6235 Left-Caudate,3086 Left-Putamen,5655 Left-Pallidum,1594 3rd-Ventricle,677","title":"Extract statistics (Optional)"},{"location":"freesurfer/#conclusion","text":"Congratulations! You\u2019ve completed the tutorial for visualizing FreeSurfer results. Feel free to experiment with other surface attributes and settings in qitview or ask around if you\u2019d like to know more about what else you can do.","title":"Conclusion"},{"location":"gallery/","text":"A Gallery of QIT Visualizations","title":"Gallery"},{"location":"gallery/#a-gallery-of-qit-visualizations","text":"","title":"A Gallery of QIT Visualizations"},{"location":"install/","text":"How to install QIT This page provides instructions for installing QIT. This has been tested on Mac, Windows, and Debian and Ubuntu GNU/Linux, but your mileage may vary, based on the specific version of the OS. You can find instructions and download links below. You can also find an archive of previous versions of QIT from https://github.com/cabeen/qit/releases . Installation for Windows You can download the latest version of QIT for Windows from: https://github.com/cabeen/qit/releases/download/latest/qit-build-win-latest.zip Once the archive has been expanded, you can move the QIT directory to your preferred installation location. The program files are located in the bin subdirectory. You can open the viewer by double-clicking qitview.bat , and you can access the command line interface by running qit.bat from the command prompt. Note: these scripts are only used for Windows. Installation for Mac You can download the latest version of QIT for Mac from: https://github.com/cabeen/qit/releases/download/latest/qit-build-mac-latest.zip Once the archive has been expanded, you can move the QIT directory to your preferred installation location. The program files are located in the bin subdirectory. You can open the viewer by double-clicking qitview , and you can access the command line interface by running qit from the command prompt. If you have restrictions on what apps can be run, e.g. the default on macOS Catalina, you may get a message about app signing. You can fix this issue by opening the System Preferences > Security & Privacy and clicking the button to allow it. You can read more about this [https://support.apple.com/en-us/HT202491 here]. You can also try running the script bin/qitfixmac , which fixes quarantine attributes that can cause problems in macOS Big Sur. Note: if for some reason you do not have Python installed, you could also start qitview by running the script qitview.sh Installation for Linux You can download the latest version of QIT for Linux here: https://github.com/cabeen/qit/releases/download/latest/qit-build-linux-latest.zip Once the archive has been expanded, you can move the QIT directory to your preferred installation location. The program files are located in the bin subdirectory. You can open the viewer by double-clicking qitview , and you can access the command line interface by running qit from the command prompt. Note: if for some reason you do not have Python installed, you could also start qitview by running the script qitview.sh Advanced Dependencies The following instructions are \u2018\u2019\u2018optional\u2019\u2018\u2019, but if you would like to integrate QIT with other neuroimaging software packages or use QIT automated workflows, please read on. Some modules in QIT are integrated with 3rd party software pacakages. The documentation for those modules should indicate whether it has this kind of dependency. Here is a list of all the software that you might need and their homepages: DTI-TK , developed with version 2.3.3 FSL , developed with version 5.0 Freesurfer , developed with version 5.0 MRtrix , developed with version 3.0 dcm2nii MATLAB Advances Normalization Tools (ANTs) For QIT to use these packages, the associated programs have to be on your system path. That means that you can run them on the command line without specifying the full path. For example, you should be able to execute Freesurfer\u2019s mri_convert from any working directory. Each package should have instructions for adding its programs to the path, so please check their documentation pages. After that, you should be good to go!","title":"Installation"},{"location":"install/#how-to-install-qit","text":"This page provides instructions for installing QIT. This has been tested on Mac, Windows, and Debian and Ubuntu GNU/Linux, but your mileage may vary, based on the specific version of the OS. You can find instructions and download links below. You can also find an archive of previous versions of QIT from https://github.com/cabeen/qit/releases .","title":"How to install QIT"},{"location":"install/#installation-for-windows","text":"You can download the latest version of QIT for Windows from: https://github.com/cabeen/qit/releases/download/latest/qit-build-win-latest.zip Once the archive has been expanded, you can move the QIT directory to your preferred installation location. The program files are located in the bin subdirectory. You can open the viewer by double-clicking qitview.bat , and you can access the command line interface by running qit.bat from the command prompt. Note: these scripts are only used for Windows.","title":"Installation for Windows"},{"location":"install/#installation-for-mac","text":"You can download the latest version of QIT for Mac from: https://github.com/cabeen/qit/releases/download/latest/qit-build-mac-latest.zip Once the archive has been expanded, you can move the QIT directory to your preferred installation location. The program files are located in the bin subdirectory. You can open the viewer by double-clicking qitview , and you can access the command line interface by running qit from the command prompt. If you have restrictions on what apps can be run, e.g. the default on macOS Catalina, you may get a message about app signing. You can fix this issue by opening the System Preferences > Security & Privacy and clicking the button to allow it. You can read more about this [https://support.apple.com/en-us/HT202491 here]. You can also try running the script bin/qitfixmac , which fixes quarantine attributes that can cause problems in macOS Big Sur. Note: if for some reason you do not have Python installed, you could also start qitview by running the script qitview.sh","title":"Installation for Mac"},{"location":"install/#installation-for-linux","text":"You can download the latest version of QIT for Linux here: https://github.com/cabeen/qit/releases/download/latest/qit-build-linux-latest.zip Once the archive has been expanded, you can move the QIT directory to your preferred installation location. The program files are located in the bin subdirectory. You can open the viewer by double-clicking qitview , and you can access the command line interface by running qit from the command prompt. Note: if for some reason you do not have Python installed, you could also start qitview by running the script qitview.sh","title":"Installation for Linux"},{"location":"install/#advanced-dependencies","text":"The following instructions are \u2018\u2019\u2018optional\u2019\u2018\u2019, but if you would like to integrate QIT with other neuroimaging software packages or use QIT automated workflows, please read on. Some modules in QIT are integrated with 3rd party software pacakages. The documentation for those modules should indicate whether it has this kind of dependency. Here is a list of all the software that you might need and their homepages: DTI-TK , developed with version 2.3.3 FSL , developed with version 5.0 Freesurfer , developed with version 5.0 MRtrix , developed with version 3.0 dcm2nii MATLAB Advances Normalization Tools (ANTs) For QIT to use these packages, the associated programs have to be on your system path. That means that you can run them on the command line without specifying the full path. For example, you should be able to execute Freesurfer\u2019s mri_convert from any working directory. Each package should have instructions for adding its programs to the path, so please check their documentation pages. After that, you should be good to go!","title":"Advanced Dependencies"},{"location":"interaction/","text":"Interacting with data in QIT qitview is a 3D data exploration tool that supports many ways to interact with data. The following sections describe qitview \u2018s interaction model and list the various specific interactions that are available for each Datasets . Interaction Model qitview \u2018s user interface consists of three main panels (shown above): the Data Panel , the Control Panel , and the Viewer Panel . You can interact with all of these panels to change how your data is rendered on the screen, but you should know a little bit about how they work to use them effectively. The Data Panel is important for choosing which Datasets you are controlling, specifically you have to select the item in the list so that it is \u2018\u2019highlighted\u2019\u2018. Once highlighted, the Control Panel will be updated to show controls for that specific dataset under the Info , View , and Edit tabs. After highlighting a dataset, the Viewer Panel will also accept mouse interactions that are applied only to the selected dataset. For example, if you use a mouse action for drawing, it will only apply to the selected dataset. There are also a few \u2018\u2019global\u2019\u2018 mouse interactions in the Viewer Panel that are not applied to the highlighted datasets (for example rotation, panning, and zooming), and these are clearly indicated below. When using the mouse in qitview , you can choose among different interactions by pressing mouse buttons, by moving the mouse, and holding down modifier keys while you do either. The Primary , Secondary , and Tertiary mouse buttons are described below, and these are the same as the Left , Right , and Middle buttons on a typical mouse. When moving the mouse, there are a few actions: a Hover of the pointer over a data object, a Click of a mouse button on a specific part in the scene, or a Drag of the pointer across the viewer panel while holding down the mouse button. You can also modify the mouse action by holding either Control , Shift , Alt , or some combination of them (note: on macOS, you can use Option instead of Alt ). Besides using the mouse, there are also menus and keyboard shortcuts that can change the view and modify datasets. The keyboard shortcuts are discussed below, and you are encouraged to explore the menu options, which can be found both on the system menu bar and in contextual menus that can be shown by right clicking on specific datasets. So that\u2019s the big picture for interacting with data in qitview . Next, we\u2019ll dig into the various specific interactions that are available. Global Interactions You can use the following interactions to change the camera: Primary Button + Drag : Rotate the camera Control + Primary Button + Drag : Zoom in the camera (translate it towards the stage) Secondary Button + Drag : Zoom in the camera (translate it towards the stage) Shift + Primary Button + Drag : Pan the camera (translate in the plane orthogonal to the view) Tertiary Button + Drag : Pan the camera (translate in the plane orthogonal to the view) You can also double click the primary mouse button on the 3D model in the Viewer Panel and that will select it in the Data Panel . There are data-specific interactions that are only available when a data object is selected in the list, so the double-click feature will allow you to quickly select and then interact with your data object of choice. Dataset interactions The following interactions will be applied only to the dataset that is highlighted in the Data Panel . While the options are complex, one general rule is that Alt is used for mouse interactions that are applied to the highlighted dataset, and otherwise, they mouse interaction is global in nature. Volume Interactions Alt + Drag : Slice the volume (based on where you clicked). This will show a red line indicating which slice is being modified. Alt + Shift + Hover : Query the coordinates and data value of the selected voxel. This will show red crosshairs and box around the selected voxel. Right/Up Arrow : Increment the slice of the selected Volume or Mask . Left/Down Arrow : Decrement the slice of the selected Volume or Mask . Shift+Right/Up Arrow : Fast Increment the slice of the selected Volume or Mask . Shift+Left/Down Arrow : Fast Decrement the slice of the selected Volume or Mask . Mask Interactions Alt + Drag : Slice the volume (based on where you clicked). This will show a red line indicating which slice is being modified. Alt + Shift + Hover : Query the coordinates and data value of the selected voxel. This will show red crosshairs and box around the selected voxel. Alt + Control + Drag : Draw on the Mask . Depending on the drawing mode, this will show either a stencil grid or the freehand polygon path. The drawing label and mode can be specified in the Mask control panel. Alt + Shift + Control + Drag : Erase the labels of the Mask . Depending on the mode, this will show either a black stencil or a black freehand polygon path. Solids Interactions Alt + Drag : Move the selected solid (i.e. a translation). A red circle will be shown in the center of the solid to indicate which one was selected. Alt + Shift + Drag : Resize the solid (i.e. change the sphere radius or box dimensions). If you select a sphere, it will change the radius, and if you select a box, it will grow or shink the side you selected. A red line will indicate what you selected. Alt + Control + Click : Create a new solid . The type and size of the new solid can be specified in the control panel. A blinking red circle will be shown before clicking the mouse to indicate where the new solid will be placed. Alt + Shift + Control + Click : Remove the selected solid . Before you click the mouse, the selected solid will blink to indicate it is about to be removed. Curves Interactions Alt + Shift + Drag : Query the curves . The selected curves will be highlighted in red. Alt + Shift + Control + Drag : Remove the selected curves . Before you click the mouse, the selected curves will be highlighted in red. Mesh Interactions Alt + Shift + Hover : Query the mesh vertices . The size of the query can be specified in the control panel. The selected vertices will be highlighted in red. Alt + Control + Drag : Show only the selected mesh component . This assumes the mesh has a label attribute, which is specified in the Mesh control panel. Alt + Shift + Control + Drag : Hide the selected mesh component . This assumes the mesh has a label attribute, which is specified in the Mesh control panel. Vects Interactions Alt + Drag : Move the selected vect (i.e. a translation). A red circle will be shown in the center of the vect to indicate which one was selected. Alt + Control + Click : Create a new vect . A blinking red circle will be shown before clicking the mouse to indicate where the new vect will be placed. Alt + Shift + Control + Click : Remove the selected vect . Before you click the mouse, the selected dot will blink to indicate it is about to be removed. Manual settings If for any reason you have difficulty using the mouse and keyboard at the same time, you can also specify the interaction mode using the mouse. You can find this 3D Interaction combo box in the Global tab of the Control Panel . This will include camera interactions, such as Rotate , Pan , and Zoom . You can quickly reset the interaction mode by pressing the Escape key. Note, the 3D Interaction menu will change based on the dataset you have highlighted for example, when a volume is highlighted, there is an interaction mode for changing the image slice.","title":"Interaction"},{"location":"interaction/#interacting-with-data-in-qit","text":"qitview is a 3D data exploration tool that supports many ways to interact with data. The following sections describe qitview \u2018s interaction model and list the various specific interactions that are available for each Datasets .","title":"Interacting with data in QIT"},{"location":"interaction/#interaction-model","text":"qitview \u2018s user interface consists of three main panels (shown above): the Data Panel , the Control Panel , and the Viewer Panel . You can interact with all of these panels to change how your data is rendered on the screen, but you should know a little bit about how they work to use them effectively. The Data Panel is important for choosing which Datasets you are controlling, specifically you have to select the item in the list so that it is \u2018\u2019highlighted\u2019\u2018. Once highlighted, the Control Panel will be updated to show controls for that specific dataset under the Info , View , and Edit tabs. After highlighting a dataset, the Viewer Panel will also accept mouse interactions that are applied only to the selected dataset. For example, if you use a mouse action for drawing, it will only apply to the selected dataset. There are also a few \u2018\u2019global\u2019\u2018 mouse interactions in the Viewer Panel that are not applied to the highlighted datasets (for example rotation, panning, and zooming), and these are clearly indicated below. When using the mouse in qitview , you can choose among different interactions by pressing mouse buttons, by moving the mouse, and holding down modifier keys while you do either. The Primary , Secondary , and Tertiary mouse buttons are described below, and these are the same as the Left , Right , and Middle buttons on a typical mouse. When moving the mouse, there are a few actions: a Hover of the pointer over a data object, a Click of a mouse button on a specific part in the scene, or a Drag of the pointer across the viewer panel while holding down the mouse button. You can also modify the mouse action by holding either Control , Shift , Alt , or some combination of them (note: on macOS, you can use Option instead of Alt ). Besides using the mouse, there are also menus and keyboard shortcuts that can change the view and modify datasets. The keyboard shortcuts are discussed below, and you are encouraged to explore the menu options, which can be found both on the system menu bar and in contextual menus that can be shown by right clicking on specific datasets. So that\u2019s the big picture for interacting with data in qitview . Next, we\u2019ll dig into the various specific interactions that are available.","title":"Interaction Model"},{"location":"interaction/#global-interactions","text":"You can use the following interactions to change the camera: Primary Button + Drag : Rotate the camera Control + Primary Button + Drag : Zoom in the camera (translate it towards the stage) Secondary Button + Drag : Zoom in the camera (translate it towards the stage) Shift + Primary Button + Drag : Pan the camera (translate in the plane orthogonal to the view) Tertiary Button + Drag : Pan the camera (translate in the plane orthogonal to the view) You can also double click the primary mouse button on the 3D model in the Viewer Panel and that will select it in the Data Panel . There are data-specific interactions that are only available when a data object is selected in the list, so the double-click feature will allow you to quickly select and then interact with your data object of choice.","title":"Global Interactions"},{"location":"interaction/#dataset-interactions","text":"The following interactions will be applied only to the dataset that is highlighted in the Data Panel . While the options are complex, one general rule is that Alt is used for mouse interactions that are applied to the highlighted dataset, and otherwise, they mouse interaction is global in nature.","title":"Dataset interactions"},{"location":"interaction/#volume-interactions","text":"Alt + Drag : Slice the volume (based on where you clicked). This will show a red line indicating which slice is being modified. Alt + Shift + Hover : Query the coordinates and data value of the selected voxel. This will show red crosshairs and box around the selected voxel. Right/Up Arrow : Increment the slice of the selected Volume or Mask . Left/Down Arrow : Decrement the slice of the selected Volume or Mask . Shift+Right/Up Arrow : Fast Increment the slice of the selected Volume or Mask . Shift+Left/Down Arrow : Fast Decrement the slice of the selected Volume or Mask .","title":"Volume Interactions"},{"location":"interaction/#mask-interactions","text":"Alt + Drag : Slice the volume (based on where you clicked). This will show a red line indicating which slice is being modified. Alt + Shift + Hover : Query the coordinates and data value of the selected voxel. This will show red crosshairs and box around the selected voxel. Alt + Control + Drag : Draw on the Mask . Depending on the drawing mode, this will show either a stencil grid or the freehand polygon path. The drawing label and mode can be specified in the Mask control panel. Alt + Shift + Control + Drag : Erase the labels of the Mask . Depending on the mode, this will show either a black stencil or a black freehand polygon path.","title":"Mask Interactions"},{"location":"interaction/#solids-interactions","text":"Alt + Drag : Move the selected solid (i.e. a translation). A red circle will be shown in the center of the solid to indicate which one was selected. Alt + Shift + Drag : Resize the solid (i.e. change the sphere radius or box dimensions). If you select a sphere, it will change the radius, and if you select a box, it will grow or shink the side you selected. A red line will indicate what you selected. Alt + Control + Click : Create a new solid . The type and size of the new solid can be specified in the control panel. A blinking red circle will be shown before clicking the mouse to indicate where the new solid will be placed. Alt + Shift + Control + Click : Remove the selected solid . Before you click the mouse, the selected solid will blink to indicate it is about to be removed.","title":"Solids Interactions"},{"location":"interaction/#curves-interactions","text":"Alt + Shift + Drag : Query the curves . The selected curves will be highlighted in red. Alt + Shift + Control + Drag : Remove the selected curves . Before you click the mouse, the selected curves will be highlighted in red.","title":"Curves Interactions"},{"location":"interaction/#mesh-interactions","text":"Alt + Shift + Hover : Query the mesh vertices . The size of the query can be specified in the control panel. The selected vertices will be highlighted in red. Alt + Control + Drag : Show only the selected mesh component . This assumes the mesh has a label attribute, which is specified in the Mesh control panel. Alt + Shift + Control + Drag : Hide the selected mesh component . This assumes the mesh has a label attribute, which is specified in the Mesh control panel.","title":"Mesh Interactions"},{"location":"interaction/#vects-interactions","text":"Alt + Drag : Move the selected vect (i.e. a translation). A red circle will be shown in the center of the vect to indicate which one was selected. Alt + Control + Click : Create a new vect . A blinking red circle will be shown before clicking the mouse to indicate where the new vect will be placed. Alt + Shift + Control + Click : Remove the selected vect . Before you click the mouse, the selected dot will blink to indicate it is about to be removed.","title":"Vects Interactions"},{"location":"interaction/#manual-settings","text":"If for any reason you have difficulty using the mouse and keyboard at the same time, you can also specify the interaction mode using the mouse. You can find this 3D Interaction combo box in the Global tab of the Control Panel . This will include camera interactions, such as Rotate , Pan , and Zoom . You can quickly reset the interaction mode by pressing the Escape key. Note, the 3D Interaction menu will change based on the dataset you have highlighted for example, when a volume is highlighted, there is an interaction mode for changing the image slice.","title":"Manual settings"},{"location":"license/","text":"Software License for QIT Use of the Quantitative Imaging Toolkit (QIT) is limited by the following license terms: Quantitative Imaging Toolkit (QIT) (c) 2012-2021 Ryan Cabeen All rights reserved. The Software remains the property of Ryan Cabeen (\"the Author\"). The Software is distributed \"AS IS\" under this Licence solely for non-commercial use in the hope that it will be useful, but in order that the Author as a charitable foundation protects its assets for the benefit of its educational and research purposes, the Author makes clear that no condition is made or to be implied, nor is any warranty given or to be implied, as to the accuracy of the Software, or that it will be suitable for any particular purpose or for use under any specific conditions. Furthermore, the Author disclaims all responsibility for the use which is made of the Software. It further disclaims any liability for the outcomes arising from using the Software. The Licensee agrees to indemnify the Author and hold the Author harmless from and against any and all claims, damages and liabilities asserted by third parties (including claims for negligence) which arise directly or indirectly from the use of the Software or the sale of any products based on the Software. No part of the Software may be reproduced, modified, transmitted or transferred in any form or by any means, electronic or mechanical, without the express permission of the Author. The permission of the Author is not required if the said reproduction, modification, transmission or transference is done without financial return, the conditions of this Licence are imposed upon the receiver of the product, and all original and amended source code is included in any transmitted product. You may be held legally responsible for any copyright infringement that is caused or encouraged by your failure to abide by these terms and conditions. You are not permitted under this Licence to use this Software commercially. Use for which any financial return is received shall be defined as commercial use, and includes (1) integration of all or part of the source code or the Software into a product for sale or license by or on behalf of Licensee to third parties or (2) use of the Software or any derivative of it for research with the final aim of developing software products for sale or license to a third party or (3) use of the Software or any derivative of it for research with the final aim of developing non-software products for sale or license to a third party, or (4) use of the Software to provide any service to an external organisation for which payment is received. Third-Party Libraries QIT links to numerous software libraries, including: Apache Commons CLI 1.2 , released under the Apache v2 license Apache Commons IO 2.1 , released under the Apache v2 license Apache Commons Lang 3.0.1 , released under the Apache v2 license Gson , released under the Apache v2 license log4j 1.2.9 , released under the Apache v2 license Guava 11.0.2 , released under the Apache v2 license Jama 1.0.2 , released into the public domain opencsv 2.3 , released under the Apache v2 license reflections-0.9.8 , released under the WTFPL license Jython 2.5.0 , released under the Jython license jythonconsole-0.0.7 , released under the LGPL v3 license Camino , released under the Artistic 2.0 license matrix-toolkits-java , released under the LGPL v3 license smile , released under the Apache 2.0 license jogl , released under the BSD license JPOP: Java parallel optimization package , released under the LGPL license object-explorer , released under Apache v2 license Logo , released under the Creative Commons license The license documentation associated with each of these can be found in the QIT software package under doc/licensing .","title":"License"},{"location":"license/#software-license-for-qit","text":"Use of the Quantitative Imaging Toolkit (QIT) is limited by the following license terms: Quantitative Imaging Toolkit (QIT) (c) 2012-2021 Ryan Cabeen All rights reserved. The Software remains the property of Ryan Cabeen (\"the Author\"). The Software is distributed \"AS IS\" under this Licence solely for non-commercial use in the hope that it will be useful, but in order that the Author as a charitable foundation protects its assets for the benefit of its educational and research purposes, the Author makes clear that no condition is made or to be implied, nor is any warranty given or to be implied, as to the accuracy of the Software, or that it will be suitable for any particular purpose or for use under any specific conditions. Furthermore, the Author disclaims all responsibility for the use which is made of the Software. It further disclaims any liability for the outcomes arising from using the Software. The Licensee agrees to indemnify the Author and hold the Author harmless from and against any and all claims, damages and liabilities asserted by third parties (including claims for negligence) which arise directly or indirectly from the use of the Software or the sale of any products based on the Software. No part of the Software may be reproduced, modified, transmitted or transferred in any form or by any means, electronic or mechanical, without the express permission of the Author. The permission of the Author is not required if the said reproduction, modification, transmission or transference is done without financial return, the conditions of this Licence are imposed upon the receiver of the product, and all original and amended source code is included in any transmitted product. You may be held legally responsible for any copyright infringement that is caused or encouraged by your failure to abide by these terms and conditions. You are not permitted under this Licence to use this Software commercially. Use for which any financial return is received shall be defined as commercial use, and includes (1) integration of all or part of the source code or the Software into a product for sale or license by or on behalf of Licensee to third parties or (2) use of the Software or any derivative of it for research with the final aim of developing software products for sale or license to a third party or (3) use of the Software or any derivative of it for research with the final aim of developing non-software products for sale or license to a third party, or (4) use of the Software to provide any service to an external organisation for which payment is received.","title":"Software License for QIT"},{"location":"license/#third-party-libraries","text":"QIT links to numerous software libraries, including: Apache Commons CLI 1.2 , released under the Apache v2 license Apache Commons IO 2.1 , released under the Apache v2 license Apache Commons Lang 3.0.1 , released under the Apache v2 license Gson , released under the Apache v2 license log4j 1.2.9 , released under the Apache v2 license Guava 11.0.2 , released under the Apache v2 license Jama 1.0.2 , released into the public domain opencsv 2.3 , released under the Apache v2 license reflections-0.9.8 , released under the WTFPL license Jython 2.5.0 , released under the Jython license jythonconsole-0.0.7 , released under the LGPL v3 license Camino , released under the Artistic 2.0 license matrix-toolkits-java , released under the LGPL v3 license smile , released under the Apache 2.0 license jogl , released under the BSD license JPOP: Java parallel optimization package , released under the LGPL license object-explorer , released under Apache v2 license Logo , released under the Creative Commons license The license documentation associated with each of these can be found in the QIT software package under doc/licensing .","title":"Third-Party Libraries"},{"location":"masters-workshop/","text":"Masters Program Workshop for QIT This page provides a guide for a QIT workshop held as part of the USC Master\u2019s of Science in Neuroimaging program . The goal of the workshop is to provide an opportunity to work directly with neuroimaging data and to gain hands-on experience with various diffusion MRI techniques. The workshop doesn\u2019t require any prior programming experience, and it is designed to work on most laptops. That said, an interested reader can also use this as a starting point for more advanced scripting of image analysis tasks or integrating QIT in compute-heavy workflows with other datasets using the LONI Pipeline. Setup If you have not already, you can start by downloading the latest version from the Installation page. You may also benefit from checking out the Concepts pages linked on the left side panel, which will help you become familiar with the design and capabilities of QIT. Next, you should download the sample dataset, decompress the archive, and keep it somewhere accessible: http://cabeen.io/download/qit-workshop-average.zip Dataset The data included in this workshop was created by averaging a collection of MRIs from typical adults, which were acquired as part of the Human Connectome Project . The images and data were moderately downsampled from the full resolution to make the workshop run smoothly, but most anatomical features present in the original data can be found in this data as well. The archive contains the following groups of data: models.dti : the diffusion tensor image volume (a Volume dataset) models.xfib : the multi-fiber image volume (a Volume dataset) models.noddi : the NODDI image volume (a Volume dataset) regions : various atlas image labels (a set of Mask datasets) tracts : various tractography reconstructions (a set of Curves datasets) meshes : various surface models (a set of Mesh datasets) masks : brain and white matter masks (Mask datasets) Volume and Mask datasets are typically stored in the nifti format with the nii.gz extension. Mask files typically have a csv stored alongside them that indicates the name of each label. Diffusion models are stored as multiple volumes inside a directory, and while you can load the individual files, to do tractography or render glyphs you need to load the directory itself, e.g. models.xfib . Mesh and Curves data are \u2018\u2019both\u2019\u2018 stored as VTK files with the vtk.gz extension. qitview should detect which is which based on the filename, but you should make sure the file loader correctly identifies the dataset type (the type can be changed using the combo box to the left of the filename). You can read more about the various dataset types [[Datasets|here]]. Agenda In this section, we provide instructions and video guides (no audio) for using QIT in various diffusion MRI tasks. You will only need the data provided in the above mentioned archive, and after the first step, the others are mostly self-contained. In total, the tutorial should take from 30-45 minutes to complete, but please feel free to take your time experimenting and to ask questions along the way! Start Up and Loading Data The first step will be to start qitview and load some data. You should start by double clicking qitview (on macOS) or qitview.py (if you are on Windows). You should see a terminal window open that prints status messages followed by a graphical interface with several panels. You can load data by dragging data into the qitview window (or by navigating through the File>Load Files menu item). You should load the file named models.dti/dti_S0.nii.gz and try exploring the dataset. \u2018\u2019Tip: you can quickly change slices by hold Alt while clicking and dragging on an image slice.\u2019\u2018 Your browser does not support the video tag. Visualizing Surface Anatomy Next, we\u2019ll examine some 3D models of anatomy represented by surfaces. You should load the data in the meshes directory into the viewer, along with an image volume for reference. The surfaces show the boundaries of various anatomical structures. The larger structure is a white matter surface, and the smaller structures are subcortical nuclei. \u2018\u2019Tip: try to change the coloring of the subcortical meshes to the label attribute like in the video.\u2019\u2018 Your browser does not support the video tag. Visualizing Diffusion Glyphs Next, we\u2019ll examine 3D rendering that depict diffusion modeling. You should load models.dti and models.xfib and follow the steps shown in the video below to create diffusion glyphs. \u2018\u2019Tip: watch for the step with a combo box incrementing from 0 to 1. This nudges the glyphs off the image slice so they are completely visible.\u2019\u2018 Your browser does not support the video tag. Visualizing Track Density Next, we\u2019ll go over how to visualize fiber bundles in two ways. You should load a fiber bundle model, e.g. tracts/arcuate.vtk.gz and a reference volume. You can see the bundle is represented by curves. The video shows how to convert it to a density map that can be viewed on top of the reference volume. \u2018\u2019Tip: try changing the colormap and opacity of the density map.\u2019\u2018 Your browser does not support the video tag. Comparing Diffusion Models Next we\u2019ll create new tractography reconstructions and compare two different diffusion models. You should load two diffusion models, models.dti and models.xfib , along with a corpus callosum region mask regions/fsa.ccwm/rois.nii.gz . The video shows how you can open the tractography module and create tracks for each of the diffusion models, and then observe the differences in the depicted anatomy. \u2018\u2019Tip: the toggle keyboard shortcut (Command-T on macOS or Control-T otherwise) will let you switch on and off the visibility of the highlighted datasets. You can also try changing the tractography parameters, e.g. increasing the samples or changing angle threshold.\u2019\u2018 Your browser does not support the video tag. Extracting Tracks Manually Next, we\u2019ll show how you can extract tracks using a manually drawn seed mask. You should open a diffusion model volume and a reference dataset. models.dti/dti_RGB.nii.gz is an efficient way to visualize white matter anatomy. You can also load atlas labels, e.g. regions/jhu.regions/rois.nii.gz . You should create a new mask for seeding by right clicking on the model data and selecting the option shown in the video. You can then draw on the mask: first, select the mask in the Data Panel , then click the Edit tab in the Control Panel to see the options, then drag the mouse over the volume to draw the mask while holding down Alt+Control . Try drawing a region of interest on the corticospinal tract in the brainstem. \u2018\u2019Tip: you can erase a mask by dragging the mouse while holding down Alt+Control+Shift .\u2019\u2018 Your browser does not support the video tag. Filtering Tracks Interactively Finally, we\u2019ll try interactively filtering tracks. You should load a reference volume and one of the fiber bundles, e.g. tracts/arcuate.vtk.gz . You should create a sphere object as indicated in the video. You can then modify the sphere by first selecting it in the the Data Panel and then clicking and dragging the mouse while holding down either Alt (to move the sphere) or Alt+Shift (to resize the sphere). Then, you can filter the fiber bundle using the sphere by selecting the curves in the Data Panel and opening the Edit tab in the Control Panel . You should select the sphere as an \u2018\u2019include\u2019\u2018 criteria as shown in the video. Now only the subset of curves that go through the sphere will be shown. You can similarly set the sphere to \u2018\u2019exclude\u2019\u2018 curves, as shown in the video. \u2018\u2019Tip: the filtering only changes what is shown on the screen. If you want to save the filtered curves, you must either Export them or Retain them using the buttons in the Edit tab.\u2019\u2018 Your browser does not support the video tag. Fin That\u2019s it, congratulations on finishing the workshop! Hopefully, you\u2019re now more familiar with a few common diffusion MRI tasks, and you have an idea of how to use qitview for other tasks you may encounter in the future. Many of the tasks you completed in qitview can be scripted or run on the command line using the qit program. To learn more, please feel free to explore the rest of the documentation on this site, experiment the features in qitview , or [mailto:rcabeen@loni.usc.edu email me] with any questions!","title":"Masters Workshop"},{"location":"masters-workshop/#masters-program-workshop-for-qit","text":"This page provides a guide for a QIT workshop held as part of the USC Master\u2019s of Science in Neuroimaging program . The goal of the workshop is to provide an opportunity to work directly with neuroimaging data and to gain hands-on experience with various diffusion MRI techniques. The workshop doesn\u2019t require any prior programming experience, and it is designed to work on most laptops. That said, an interested reader can also use this as a starting point for more advanced scripting of image analysis tasks or integrating QIT in compute-heavy workflows with other datasets using the LONI Pipeline.","title":"Masters Program Workshop for QIT"},{"location":"masters-workshop/#setup","text":"If you have not already, you can start by downloading the latest version from the Installation page. You may also benefit from checking out the Concepts pages linked on the left side panel, which will help you become familiar with the design and capabilities of QIT. Next, you should download the sample dataset, decompress the archive, and keep it somewhere accessible: http://cabeen.io/download/qit-workshop-average.zip","title":"Setup"},{"location":"masters-workshop/#dataset","text":"The data included in this workshop was created by averaging a collection of MRIs from typical adults, which were acquired as part of the Human Connectome Project . The images and data were moderately downsampled from the full resolution to make the workshop run smoothly, but most anatomical features present in the original data can be found in this data as well. The archive contains the following groups of data: models.dti : the diffusion tensor image volume (a Volume dataset) models.xfib : the multi-fiber image volume (a Volume dataset) models.noddi : the NODDI image volume (a Volume dataset) regions : various atlas image labels (a set of Mask datasets) tracts : various tractography reconstructions (a set of Curves datasets) meshes : various surface models (a set of Mesh datasets) masks : brain and white matter masks (Mask datasets) Volume and Mask datasets are typically stored in the nifti format with the nii.gz extension. Mask files typically have a csv stored alongside them that indicates the name of each label. Diffusion models are stored as multiple volumes inside a directory, and while you can load the individual files, to do tractography or render glyphs you need to load the directory itself, e.g. models.xfib . Mesh and Curves data are \u2018\u2019both\u2019\u2018 stored as VTK files with the vtk.gz extension. qitview should detect which is which based on the filename, but you should make sure the file loader correctly identifies the dataset type (the type can be changed using the combo box to the left of the filename). You can read more about the various dataset types [[Datasets|here]].","title":"Dataset"},{"location":"masters-workshop/#agenda","text":"In this section, we provide instructions and video guides (no audio) for using QIT in various diffusion MRI tasks. You will only need the data provided in the above mentioned archive, and after the first step, the others are mostly self-contained. In total, the tutorial should take from 30-45 minutes to complete, but please feel free to take your time experimenting and to ask questions along the way!","title":"Agenda"},{"location":"masters-workshop/#start-up-and-loading-data","text":"The first step will be to start qitview and load some data. You should start by double clicking qitview (on macOS) or qitview.py (if you are on Windows). You should see a terminal window open that prints status messages followed by a graphical interface with several panels. You can load data by dragging data into the qitview window (or by navigating through the File>Load Files menu item). You should load the file named models.dti/dti_S0.nii.gz and try exploring the dataset. \u2018\u2019Tip: you can quickly change slices by hold Alt while clicking and dragging on an image slice.\u2019\u2018 Your browser does not support the video tag.","title":"Start Up and Loading Data"},{"location":"masters-workshop/#visualizing-surface-anatomy","text":"Next, we\u2019ll examine some 3D models of anatomy represented by surfaces. You should load the data in the meshes directory into the viewer, along with an image volume for reference. The surfaces show the boundaries of various anatomical structures. The larger structure is a white matter surface, and the smaller structures are subcortical nuclei. \u2018\u2019Tip: try to change the coloring of the subcortical meshes to the label attribute like in the video.\u2019\u2018 Your browser does not support the video tag.","title":"Visualizing Surface Anatomy"},{"location":"masters-workshop/#visualizing-diffusion-glyphs","text":"Next, we\u2019ll examine 3D rendering that depict diffusion modeling. You should load models.dti and models.xfib and follow the steps shown in the video below to create diffusion glyphs. \u2018\u2019Tip: watch for the step with a combo box incrementing from 0 to 1. This nudges the glyphs off the image slice so they are completely visible.\u2019\u2018 Your browser does not support the video tag.","title":"Visualizing Diffusion Glyphs"},{"location":"masters-workshop/#visualizing-track-density","text":"Next, we\u2019ll go over how to visualize fiber bundles in two ways. You should load a fiber bundle model, e.g. tracts/arcuate.vtk.gz and a reference volume. You can see the bundle is represented by curves. The video shows how to convert it to a density map that can be viewed on top of the reference volume. \u2018\u2019Tip: try changing the colormap and opacity of the density map.\u2019\u2018 Your browser does not support the video tag.","title":"Visualizing Track Density"},{"location":"masters-workshop/#comparing-diffusion-models","text":"Next we\u2019ll create new tractography reconstructions and compare two different diffusion models. You should load two diffusion models, models.dti and models.xfib , along with a corpus callosum region mask regions/fsa.ccwm/rois.nii.gz . The video shows how you can open the tractography module and create tracks for each of the diffusion models, and then observe the differences in the depicted anatomy. \u2018\u2019Tip: the toggle keyboard shortcut (Command-T on macOS or Control-T otherwise) will let you switch on and off the visibility of the highlighted datasets. You can also try changing the tractography parameters, e.g. increasing the samples or changing angle threshold.\u2019\u2018 Your browser does not support the video tag.","title":"Comparing Diffusion Models"},{"location":"masters-workshop/#extracting-tracks-manually","text":"Next, we\u2019ll show how you can extract tracks using a manually drawn seed mask. You should open a diffusion model volume and a reference dataset. models.dti/dti_RGB.nii.gz is an efficient way to visualize white matter anatomy. You can also load atlas labels, e.g. regions/jhu.regions/rois.nii.gz . You should create a new mask for seeding by right clicking on the model data and selecting the option shown in the video. You can then draw on the mask: first, select the mask in the Data Panel , then click the Edit tab in the Control Panel to see the options, then drag the mouse over the volume to draw the mask while holding down Alt+Control . Try drawing a region of interest on the corticospinal tract in the brainstem. \u2018\u2019Tip: you can erase a mask by dragging the mouse while holding down Alt+Control+Shift .\u2019\u2018 Your browser does not support the video tag.","title":"Extracting Tracks Manually"},{"location":"masters-workshop/#filtering-tracks-interactively","text":"Finally, we\u2019ll try interactively filtering tracks. You should load a reference volume and one of the fiber bundles, e.g. tracts/arcuate.vtk.gz . You should create a sphere object as indicated in the video. You can then modify the sphere by first selecting it in the the Data Panel and then clicking and dragging the mouse while holding down either Alt (to move the sphere) or Alt+Shift (to resize the sphere). Then, you can filter the fiber bundle using the sphere by selecting the curves in the Data Panel and opening the Edit tab in the Control Panel . You should select the sphere as an \u2018\u2019include\u2019\u2018 criteria as shown in the video. Now only the subset of curves that go through the sphere will be shown. You can similarly set the sphere to \u2018\u2019exclude\u2019\u2018 curves, as shown in the video. \u2018\u2019Tip: the filtering only changes what is shown on the screen. If you want to save the filtered curves, you must either Export them or Retain them using the buttons in the Edit tab.\u2019\u2018 Your browser does not support the video tag.","title":"Filtering Tracks Interactively"},{"location":"masters-workshop/#fin","text":"That\u2019s it, congratulations on finishing the workshop! Hopefully, you\u2019re now more familiar with a few common diffusion MRI tasks, and you have an idea of how to use qitview for other tasks you may encounter in the future. Many of the tasks you completed in qitview can be scripted or run on the command line using the qit program. To learn more, please feel free to explore the rest of the documentation on this site, experiment the features in qitview , or [mailto:rcabeen@loni.usc.edu email me] with any questions!","title":"Fin"},{"location":"models/","text":"What voxel models are supported by QIT? QIT has special features for working with model-based imaging data. Most imaging data represents colors or grayscale intensities at each pixel, but scientific imaging data can also represent physical measurements. Magnetic resonance (MR) imaging can also be used to depict a spatial distribution of such physical measurements. There are many types of physical processes that can be characterized using MR, for example relaxation rates, diffusivities, compartment proportions, etc. QIT is designed to support many of the commonly encountered models, particularly those obtained from diffusion MR imaging. In this section, we will discuss the way models are supported by QIT and how you can use them. What is a Model ? A Model object represents a collection of parameters of some physical model, typically stored in an image voxel. We use this type of object-oriented approach because most useful models are multivariate and have some constraints on what model parameters are valid. QIT is designed to make it possible to treat these special imaging datasets like any other data, but it allows allows more advanced visualization techniques, like creating 3D renderings of model imaging data using glyphs. There are several characteristics shared by all models: A Model can be encoded (or parameterized) by a Vect A Model has any number of named Vect valued features A Model has a way to compute the distance between it any other model of the same type This representation is flexible and can be used for storing model data in Volume datasets, or any other datatype that uses Vect object. This simplifies things greatly, as all of their existing file formats and Module objects are available for use with model data. It also allows model data to be treated in special ways, for example, in specially designed image processing algorithms or in glyph-based visualization. However, even though a Model can be converted to a Vect , it is important to remember that they often cannot be treated like vectors algebraically. For example, you may be tempted to add together, scale, or compute the magnitude of Vect s derived from a Model ; however, if the model parameters store a vector representing a 3D direction as a point on a sphere, then adding those values may leave you with model parameters that don\u2019t make any sense, i.e. no longer lie on a sphere. What types of Model are available? Below is a detailed list of the models supported by QIT. A general description of each model is provided and any peculiarities are noted as well. This list is a work in progress and will eventually be expanded to provide more detail regarding the parameters, features, and motivation behind each model. Tensor The Tensor model is the basis for diffusion tensor imaging (DTI). DTI depicts the decay of the diffusion signal using a 3x3 positive definite matrix, which provides a way to depict an ellipsoid-shaped pattern of diffusion. The shape of the tensor can be used to characterize the tissue being modeled, and this is usually done by extracting a variety of features. The primary orientation of the tensor is typically visualized to depict the dominant orientation of axonal fibers within the vowel. Fractional anisotropic is a feature that conveys the degree to which the tensor is anisotropic, or cigar-shaped. Mean diffusivity describes the overal shape of the tensor (regardless of its anisotropic). The tensor diffusivity can also be depicted in specific directions, for example, axial diffusivity describes the diffusivity in the direction of the primary orientation, and radial diffusivity describes the diffusivity in the plane orthogonal to the primary orientation. Fibers The Fibers model is used for representing the multi-compartment ball and sticks model. This model was developed to address shortcomings of the tensor model, namely that it cannot accurately depict voxels with multiple distinct components. For example, a voxel may contain some gray matter or cerebrospinal fluid in addition to white matter fibers, or it may combine white matter fibers from distinct fascicles that cross. Multi-compartment models address this issue by depicting the diffusion signal with a linear combination of tissue compartments. The ball and sticks model s a simple but powerful special case of this approach, in which the signal is decomposed into compartments for isotopic diffusion and some arbitrary number of fibers populations. The diffusivities of the compartments are assumed to be identical and each fiber compartment is formulated as an infinitely thin cylinder. This enables the model to distinguish between crossing fibers, as well as more complex mixtures with gray matter and CSF. Each compartment is assigned a volume fraction, which indicates the proportion of the signal that it explains. Two common features to extract are the total fiber volume fraction (the sum of the individual fiber compartment volume fractions), the isotopic volume fraction, and the diffusivity. Spharm The Spharm model represents an arbitrary spherical function describing a fiber orientation distribution\u2026 Noddi The Noddi model is a biophysical model representing multiple compartments for intra- and extra-axonal water and fiber orientation dispersion\u2026 Kurtosis The Kurtosis model is the basis for a higher-order extension of the diffusion tensor model\u2026 ExpDecay The ExpDecay model is a general model for representing processes that exhibit a pattern of exponential decay in the MR signal over time.","title":"Models"},{"location":"models/#what-voxel-models-are-supported-by-qit","text":"QIT has special features for working with model-based imaging data. Most imaging data represents colors or grayscale intensities at each pixel, but scientific imaging data can also represent physical measurements. Magnetic resonance (MR) imaging can also be used to depict a spatial distribution of such physical measurements. There are many types of physical processes that can be characterized using MR, for example relaxation rates, diffusivities, compartment proportions, etc. QIT is designed to support many of the commonly encountered models, particularly those obtained from diffusion MR imaging. In this section, we will discuss the way models are supported by QIT and how you can use them.","title":"What voxel models are supported by QIT?"},{"location":"models/#what-is-a-model","text":"A Model object represents a collection of parameters of some physical model, typically stored in an image voxel. We use this type of object-oriented approach because most useful models are multivariate and have some constraints on what model parameters are valid. QIT is designed to make it possible to treat these special imaging datasets like any other data, but it allows allows more advanced visualization techniques, like creating 3D renderings of model imaging data using glyphs. There are several characteristics shared by all models: A Model can be encoded (or parameterized) by a Vect A Model has any number of named Vect valued features A Model has a way to compute the distance between it any other model of the same type This representation is flexible and can be used for storing model data in Volume datasets, or any other datatype that uses Vect object. This simplifies things greatly, as all of their existing file formats and Module objects are available for use with model data. It also allows model data to be treated in special ways, for example, in specially designed image processing algorithms or in glyph-based visualization. However, even though a Model can be converted to a Vect , it is important to remember that they often cannot be treated like vectors algebraically. For example, you may be tempted to add together, scale, or compute the magnitude of Vect s derived from a Model ; however, if the model parameters store a vector representing a 3D direction as a point on a sphere, then adding those values may leave you with model parameters that don\u2019t make any sense, i.e. no longer lie on a sphere.","title":"What is a Model?"},{"location":"models/#what-types-of-model-are-available","text":"Below is a detailed list of the models supported by QIT. A general description of each model is provided and any peculiarities are noted as well. This list is a work in progress and will eventually be expanded to provide more detail regarding the parameters, features, and motivation behind each model.","title":"What types of Model are available?"},{"location":"models/#tensor","text":"The Tensor model is the basis for diffusion tensor imaging (DTI). DTI depicts the decay of the diffusion signal using a 3x3 positive definite matrix, which provides a way to depict an ellipsoid-shaped pattern of diffusion. The shape of the tensor can be used to characterize the tissue being modeled, and this is usually done by extracting a variety of features. The primary orientation of the tensor is typically visualized to depict the dominant orientation of axonal fibers within the vowel. Fractional anisotropic is a feature that conveys the degree to which the tensor is anisotropic, or cigar-shaped. Mean diffusivity describes the overal shape of the tensor (regardless of its anisotropic). The tensor diffusivity can also be depicted in specific directions, for example, axial diffusivity describes the diffusivity in the direction of the primary orientation, and radial diffusivity describes the diffusivity in the plane orthogonal to the primary orientation.","title":"Tensor"},{"location":"models/#fibers","text":"The Fibers model is used for representing the multi-compartment ball and sticks model. This model was developed to address shortcomings of the tensor model, namely that it cannot accurately depict voxels with multiple distinct components. For example, a voxel may contain some gray matter or cerebrospinal fluid in addition to white matter fibers, or it may combine white matter fibers from distinct fascicles that cross. Multi-compartment models address this issue by depicting the diffusion signal with a linear combination of tissue compartments. The ball and sticks model s a simple but powerful special case of this approach, in which the signal is decomposed into compartments for isotopic diffusion and some arbitrary number of fibers populations. The diffusivities of the compartments are assumed to be identical and each fiber compartment is formulated as an infinitely thin cylinder. This enables the model to distinguish between crossing fibers, as well as more complex mixtures with gray matter and CSF. Each compartment is assigned a volume fraction, which indicates the proportion of the signal that it explains. Two common features to extract are the total fiber volume fraction (the sum of the individual fiber compartment volume fractions), the isotopic volume fraction, and the diffusivity.","title":"Fibers"},{"location":"models/#spharm","text":"The Spharm model represents an arbitrary spherical function describing a fiber orientation distribution\u2026","title":"Spharm"},{"location":"models/#noddi","text":"The Noddi model is a biophysical model representing multiple compartments for intra- and extra-axonal water and fiber orientation dispersion\u2026","title":"Noddi"},{"location":"models/#kurtosis","text":"The Kurtosis model is the basis for a higher-order extension of the diffusion tensor model\u2026","title":"Kurtosis"},{"location":"models/#expdecay","text":"The ExpDecay model is a general model for representing processes that exhibit a pattern of exponential decay in the MR signal over time.","title":"ExpDecay"},{"location":"modules/","text":"What are QIT modules? QIT includes a module framework for making making it easy to analyze data. This page describes how modules are used, outlines the basic components of the module framework, and provides an example of a module. If you\u2019re not interested in the software development aspects, you can probably just read the first section. Why use a Module ? The QIT module system is meant to make it easier to use the many useful data processing algorithms that are out there. Many researchers develop sophisticated new methods, but unfortunately, they often end up as prototypes that aren\u2019t more widely used. The module framework is an attempt to bridge the gap between these methods and people who can use them in their research. The basic idea is to provide a simple interface for implementing an algorithm that doesn\u2019t require the developer to worry about file formats, user interfaces, etc. All of these things are automatically handled by QIT once a Module has been implemented, making it easy to share a new tool with a wider audience. Any QIT Module can be run on the command line, applied to data interactively in qitview , or run on the [http://pipeline.loni.usc.edu LONI Pipeline]. In addition, documentation can be made from a Module and a preferred set of parameter settings can be saved for later use or archival purposes. What is a Module ? A Module is an object that processes data in some way. That may sound too general to be useful, but there are a number of elements common to each Module that make life easier for both software developers and users. From the perspective of a developer, a Module makes life easier by providing a framework for implementing both simple and complex algorithms that use any of the available Datasets . This framework isn\u2019t specific to any file format, so any new Module automatically supports the file formats listed in for each of the Datasets . It also provides automatically generated command line and graphical user interfaces for using the Module . For command line applications, each Module will have an automatically generated usage page and option parsing available. For graphical applications, each Module will be listed in 3d data viewer qitview , along with a dialog for controlling how the it operates on data loaded in the viewer. For the user, this framework also simplifies the analysis of data by providing a consistent interface across algorithms, reducing the need to learn how to use each tool as it comes along. It also means that an algorithm is more widely available than if it was developed as an independent program or as a MATLAB script, which are not typically integrated into data viewers or support many file formats. Finally, each Module also supports meta-data that includes the author, associated publications, and plain english descriptions. What makes up a Module ? Each Module has some number of fields with annotations to indicate how they should be treated. There are three main types: @ModuleInput : a Datasets object that will be processed, which is provided by the user when they run the module @ModuleParameter : a numerical, string, or boolean parameter, which is provided by the user @ModuleOutput : a Datasets object that will be the result, which will be received by the user There can be multiple @ModuleInput and @ModuleOutput fields, each of which belongs to one of the types listed in Datatypes . There can be any number of @ModuleParameter components, but they must also be primitive types, such Integer , Double , String , or Boolean . @ModuleParameter field can also have default values. There are also modifiers on fields, including: @ModuleOptional : a component is not required for the Module to run @ModuleAdvanced : a component is not typically modified (only used when making documentation) There can also be annotations for provenance, for example: @ModuleDescription : a plain english description of a module or field @ModuleAuthor : the person implementing the Module (not necessarily the author of the underlying algorithm or paper) @ModuleCitation : a reference that should be cited when using this module in analysis for academic publication Beyond this, each Module also contains the procedures for data processing, which is why you would use the Module in the first place! What is an example of a Module ? Below, we provide an example Module named VolumeThreshold that thresholds a Volume to produce a binary Mask . While there are many more complex examples, this demonstrates the basic features of a Module . The code for the module looks like this: @ModuleDescription(\"Threshold a volume to make a mask\") @ModuleAuthor(\"Ryan Cabeen\") public class VolumeThreshold implements Module { @ModuleInput @ModuleDescription(\"input volume\") public Volume input; @ModuleInput @ModuleDescription(\"a mask restricting which voxels are processed\") public Mask mask; @ModuleParameter @ModuleDescription(\"threshold value\") public double threshold = 0.5; @ModuleOutput @ModuleDescription(\"output mask\") public Mask output; public VolumeThreshold run() { Mask out = new Mask(this.input.getSampling()); for (Sample sample : this.input.getSampling()) { if (this.input.valid(sample, this.mask)) { double value = this.input.get(sample, 0); if (value >= this.threshold) { out.set(sample, 1); } } } this.output = out; return this; } } The command line interface for this module looks like this: $ qit VolumeThreshold --help Name: VolumeThreshold Description: threshold a volume to make a mask Required Arguments: --input <Volume> input volume --output <Mask> output mask Optional Arguments: --mask <Mask> a mask restricting which voxels are processed --threshold <double> threshold value (Default: 0.5) Author: Ryan Cabeen The graphical user interface looks like this: Note: the module dialog supports tooltips, which means if you hover the mouse over a @Parameter it will pop up a box showing more information.","title":"Modules"},{"location":"modules/#what-are-qit-modules","text":"QIT includes a module framework for making making it easy to analyze data. This page describes how modules are used, outlines the basic components of the module framework, and provides an example of a module. If you\u2019re not interested in the software development aspects, you can probably just read the first section.","title":"What are QIT modules?"},{"location":"modules/#why-use-a-module","text":"The QIT module system is meant to make it easier to use the many useful data processing algorithms that are out there. Many researchers develop sophisticated new methods, but unfortunately, they often end up as prototypes that aren\u2019t more widely used. The module framework is an attempt to bridge the gap between these methods and people who can use them in their research. The basic idea is to provide a simple interface for implementing an algorithm that doesn\u2019t require the developer to worry about file formats, user interfaces, etc. All of these things are automatically handled by QIT once a Module has been implemented, making it easy to share a new tool with a wider audience. Any QIT Module can be run on the command line, applied to data interactively in qitview , or run on the [http://pipeline.loni.usc.edu LONI Pipeline]. In addition, documentation can be made from a Module and a preferred set of parameter settings can be saved for later use or archival purposes.","title":"Why use a Module?"},{"location":"modules/#what-is-a-module","text":"A Module is an object that processes data in some way. That may sound too general to be useful, but there are a number of elements common to each Module that make life easier for both software developers and users. From the perspective of a developer, a Module makes life easier by providing a framework for implementing both simple and complex algorithms that use any of the available Datasets . This framework isn\u2019t specific to any file format, so any new Module automatically supports the file formats listed in for each of the Datasets . It also provides automatically generated command line and graphical user interfaces for using the Module . For command line applications, each Module will have an automatically generated usage page and option parsing available. For graphical applications, each Module will be listed in 3d data viewer qitview , along with a dialog for controlling how the it operates on data loaded in the viewer. For the user, this framework also simplifies the analysis of data by providing a consistent interface across algorithms, reducing the need to learn how to use each tool as it comes along. It also means that an algorithm is more widely available than if it was developed as an independent program or as a MATLAB script, which are not typically integrated into data viewers or support many file formats. Finally, each Module also supports meta-data that includes the author, associated publications, and plain english descriptions.","title":"What is a Module?"},{"location":"modules/#what-makes-up-a-module","text":"Each Module has some number of fields with annotations to indicate how they should be treated. There are three main types: @ModuleInput : a Datasets object that will be processed, which is provided by the user when they run the module @ModuleParameter : a numerical, string, or boolean parameter, which is provided by the user @ModuleOutput : a Datasets object that will be the result, which will be received by the user There can be multiple @ModuleInput and @ModuleOutput fields, each of which belongs to one of the types listed in Datatypes . There can be any number of @ModuleParameter components, but they must also be primitive types, such Integer , Double , String , or Boolean . @ModuleParameter field can also have default values. There are also modifiers on fields, including: @ModuleOptional : a component is not required for the Module to run @ModuleAdvanced : a component is not typically modified (only used when making documentation) There can also be annotations for provenance, for example: @ModuleDescription : a plain english description of a module or field @ModuleAuthor : the person implementing the Module (not necessarily the author of the underlying algorithm or paper) @ModuleCitation : a reference that should be cited when using this module in analysis for academic publication Beyond this, each Module also contains the procedures for data processing, which is why you would use the Module in the first place!","title":"What makes up a Module?"},{"location":"modules/#what-is-an-example-of-a-module","text":"Below, we provide an example Module named VolumeThreshold that thresholds a Volume to produce a binary Mask . While there are many more complex examples, this demonstrates the basic features of a Module . The code for the module looks like this: @ModuleDescription(\"Threshold a volume to make a mask\") @ModuleAuthor(\"Ryan Cabeen\") public class VolumeThreshold implements Module { @ModuleInput @ModuleDescription(\"input volume\") public Volume input; @ModuleInput @ModuleDescription(\"a mask restricting which voxels are processed\") public Mask mask; @ModuleParameter @ModuleDescription(\"threshold value\") public double threshold = 0.5; @ModuleOutput @ModuleDescription(\"output mask\") public Mask output; public VolumeThreshold run() { Mask out = new Mask(this.input.getSampling()); for (Sample sample : this.input.getSampling()) { if (this.input.valid(sample, this.mask)) { double value = this.input.get(sample, 0); if (value >= this.threshold) { out.set(sample, 1); } } } this.output = out; return this; } } The command line interface for this module looks like this: $ qit VolumeThreshold --help Name: VolumeThreshold Description: threshold a volume to make a mask Required Arguments: --input <Volume> input volume --output <Mask> output mask Optional Arguments: --mask <Mask> a mask restricting which voxels are processed --threshold <double> threshold value (Default: 0.5) Author: Ryan Cabeen The graphical user interface looks like this: Note: the module dialog supports tooltips, which means if you hover the mouse over a @Parameter it will pop up a box showing more information.","title":"What is an example of a Module?"},{"location":"quickstart/","text":"Quickly getting QIT up and running This page provides some tips for getting quickly up to speed with using QIT on the command line and the viewer. Starting the Command Line This section will help you get started with the toolkit\u2019s command line interface. This can help you script various parts of your data analysis when you have a complex workflow, or you want to make your process reproducible. Main Interface The command line interface to the toolkit can be accessed through a single program named qit . There are a few global options available, which can be listed by running qit --help . These most important ones are the following: --verbose : print messages about progress --debug : print debugging information when an error occurs --rseed value : specify a random seed There are many modules for processing data that can be accessed through this interface. You can get a list of available modules by running qit --list . Once you find one that might be useful, you can learn more about it by adding it to the qit command. For example, if you are intent on thresholding a volume, you could run: $ qit VolumeThreshold Name: VolumeThreshold Description: Threshold a volume to make a mask Required Arguments: --input <Volume> input volume --output <Mask> output mask Optional Arguments: --mask <Mask> mask --threshold <double> threshold value (Default: 0.5) Author: Ryan Cabeen Then, you can plug in your data to run the command, for example: $ qit VolumeThreshold --input volume.nii.gz --threshold 0.5 --output threshold.nii.gz Handling file formats One benefit of the toolkit is that it handles many file formats automatically. In the above help page, you\u2019ll notice that the input and output flags only ask for Volume and Mask objects. These could be NIFTI, VTK, or a variety of others. There is a more comprehensive discussion of this in the page on [[Datasets]]. Scripting You can use your favorite programs to write scripts using qit , but there is also a convenient Python scripting interface that is built-in. This uses a Java implementation of Python called [http://www.jython.org Jython] to directly connect your script with the data objects available in the toolkit. This is a advanced topic that will be described in later pages, but you can see and example script at lib/modules/qit/VolumeBrainExtract.py . Starting the Viewer This section will help you get started with the toolkit\u2019s viewer, and with any luck, you\u2019ll be able to explore and visualize your data in no time! Starting the Program The viewer is a single 3D rendering application named qitview . If you followed the [[Installation]], you should only have to type this command in your terminal. Then, the graphical interface will open and look like this: There are three main components to the user interface: Data Workspace (\u2018\u2019top left\u2019\u2018): a list of data in your workspace Control panel (\u2018\u2019bottom left\u2019\u2018): Information and settings about the currently select data Viewing Stage (\u2018\u2019right\u2019\u2018): a 3D rendering of the data There are also five menu options available: File : load and save data, take screenshots, etc View : toggle what is visible, move the camera, change the order of data objects, Modules : a list of modules for processing data Settings : modify colormaps, change rending options, show/hide messages and interpreter Help : open up the user manual (what you\u2019re reading now) Without data, this isn\u2019t very useful, so let\u2019s try that next! Loading data Data can be loaded in two ways: either interactively through the GUI or on the command line when starting the program. To load data in the GUI, select File>Open>${filetype} , where ${filetype} , may be one of the available datatypes. To load data on the command line, you list the files after a flag indicating the type. For example, to load a volume named volume.nii.gz and a mask named mask.nii.gz , you would execute: $ qitview --volume volume.nii.gz --mask mask.nii.gz It can take some time to load large datasets, so it can take some time to show up in the data workspace. Once that data is loaded, you can select it in the data panel. This should populate the selection panel with various collapsable folders for working with that object. If more than one object is selected, the first selection is the one shown in the panel. Interacting with data The viewer is based on a 3D rendering engine. You can imagine your datasets is sitting on a stage surrounding by lights and a camera through which you are viewing the scene. When you interact with the viewing panel, you are either changing the pose of the camera or interacting directly with the selected data object. Here are the basic mouse interactions for changing the camera: Mouse Primary Button + Drag : rotate the camera Mouse Secondary Button + Drag : zoom in the camera (translate it towards the stage) Mouse Middle Button + Drag : pan the camera (translate in orthogonal to the view) If you only have one mouse button, you can also use keyboard options: Mouse Primary Button + Control + Drag : zoom in the camera Mouse Primary Button + Shift + Drag : pan the camera (translate in orthogonal to the view) Again, the specific functions will depend on the datatype, and you can learn more in the [[Interaction]] section. Processing data There are many modules available for processing data, and they are listed in Modules menu. The modules are organized by the primary data type they operate on; however, many modules combine datatypes. When you select a module, a window will open that will let you specify inputs, parameters, and outputs. For example, the VolumeFilterMedian module will look like this: Each module includes documentation, which can be shown by clicking the About button. Each parameter also has a brief description that can be revealed by hovering the mouse over the parameter name, e.g. window shown above. There are three options for output files: Existing data object : write output to an existing data object (this will delete anything that was previously stored there) New\u2026 : create a new data object for the output with a default name New named\u2026 : create a new data object for the output and prompt the user for the new name Checking progress qitview produces a lot of messages, either about what you are currently doing or what is happening in the background. These are all printed to the command line, so it can be helpful to make the console visible. Alternatively, you can open a messages window by selecting Settings>Messages... . Saving your work After you\u2019ve explored and analyzed your data, there are several ways to save your work. The simplest is to take a screenshot using File>Take Screenshot (1x)... . This will prompt you for a filename and save the screenshot there. The tool also supports high resolution screenshots, for example, at twice the resolution of the viewing window (2x). You can also save the data objects from your analysis using File>Save Selected File(s) . This will prompt you for a filename and save the selected data object there. Once you have saved a data object, it will also remember the filename and save to the same place next time. If you want to save the data object to a new place, you can instead select File>Save Selected File(s) As... . If you loaded your data from a file, the input filename will be associated with the file for saving. You will see a warning before overwriting an existing file (such as the original input), but the more adventurous user can disable such warnings by selecting File>File clobbering . If you are more of a packrat, you can enable file backups by selecting File>File backups , which will rename the existing file with a timestamp before writing the new data. Finally, if you just want to save everything, you can select File>Save Entire Scene , which will write everything to a single directory with some additional metadata about the scene.","title":"Quickstart"},{"location":"quickstart/#quickly-getting-qit-up-and-running","text":"This page provides some tips for getting quickly up to speed with using QIT on the command line and the viewer.","title":"Quickly getting QIT up and running"},{"location":"quickstart/#starting-the-command-line","text":"This section will help you get started with the toolkit\u2019s command line interface. This can help you script various parts of your data analysis when you have a complex workflow, or you want to make your process reproducible.","title":"Starting the Command Line"},{"location":"quickstart/#main-interface","text":"The command line interface to the toolkit can be accessed through a single program named qit . There are a few global options available, which can be listed by running qit --help . These most important ones are the following: --verbose : print messages about progress --debug : print debugging information when an error occurs --rseed value : specify a random seed There are many modules for processing data that can be accessed through this interface. You can get a list of available modules by running qit --list . Once you find one that might be useful, you can learn more about it by adding it to the qit command. For example, if you are intent on thresholding a volume, you could run: $ qit VolumeThreshold Name: VolumeThreshold Description: Threshold a volume to make a mask Required Arguments: --input <Volume> input volume --output <Mask> output mask Optional Arguments: --mask <Mask> mask --threshold <double> threshold value (Default: 0.5) Author: Ryan Cabeen Then, you can plug in your data to run the command, for example: $ qit VolumeThreshold --input volume.nii.gz --threshold 0.5 --output threshold.nii.gz","title":"Main Interface"},{"location":"quickstart/#handling-file-formats","text":"One benefit of the toolkit is that it handles many file formats automatically. In the above help page, you\u2019ll notice that the input and output flags only ask for Volume and Mask objects. These could be NIFTI, VTK, or a variety of others. There is a more comprehensive discussion of this in the page on [[Datasets]].","title":"Handling file formats"},{"location":"quickstart/#scripting","text":"You can use your favorite programs to write scripts using qit , but there is also a convenient Python scripting interface that is built-in. This uses a Java implementation of Python called [http://www.jython.org Jython] to directly connect your script with the data objects available in the toolkit. This is a advanced topic that will be described in later pages, but you can see and example script at lib/modules/qit/VolumeBrainExtract.py .","title":"Scripting"},{"location":"quickstart/#starting-the-viewer","text":"This section will help you get started with the toolkit\u2019s viewer, and with any luck, you\u2019ll be able to explore and visualize your data in no time!","title":"Starting the Viewer"},{"location":"quickstart/#starting-the-program","text":"The viewer is a single 3D rendering application named qitview . If you followed the [[Installation]], you should only have to type this command in your terminal. Then, the graphical interface will open and look like this: There are three main components to the user interface: Data Workspace (\u2018\u2019top left\u2019\u2018): a list of data in your workspace Control panel (\u2018\u2019bottom left\u2019\u2018): Information and settings about the currently select data Viewing Stage (\u2018\u2019right\u2019\u2018): a 3D rendering of the data There are also five menu options available: File : load and save data, take screenshots, etc View : toggle what is visible, move the camera, change the order of data objects, Modules : a list of modules for processing data Settings : modify colormaps, change rending options, show/hide messages and interpreter Help : open up the user manual (what you\u2019re reading now) Without data, this isn\u2019t very useful, so let\u2019s try that next!","title":"Starting the Program"},{"location":"quickstart/#loading-data","text":"Data can be loaded in two ways: either interactively through the GUI or on the command line when starting the program. To load data in the GUI, select File>Open>${filetype} , where ${filetype} , may be one of the available datatypes. To load data on the command line, you list the files after a flag indicating the type. For example, to load a volume named volume.nii.gz and a mask named mask.nii.gz , you would execute: $ qitview --volume volume.nii.gz --mask mask.nii.gz It can take some time to load large datasets, so it can take some time to show up in the data workspace. Once that data is loaded, you can select it in the data panel. This should populate the selection panel with various collapsable folders for working with that object. If more than one object is selected, the first selection is the one shown in the panel.","title":"Loading data"},{"location":"quickstart/#interacting-with-data","text":"The viewer is based on a 3D rendering engine. You can imagine your datasets is sitting on a stage surrounding by lights and a camera through which you are viewing the scene. When you interact with the viewing panel, you are either changing the pose of the camera or interacting directly with the selected data object. Here are the basic mouse interactions for changing the camera: Mouse Primary Button + Drag : rotate the camera Mouse Secondary Button + Drag : zoom in the camera (translate it towards the stage) Mouse Middle Button + Drag : pan the camera (translate in orthogonal to the view) If you only have one mouse button, you can also use keyboard options: Mouse Primary Button + Control + Drag : zoom in the camera Mouse Primary Button + Shift + Drag : pan the camera (translate in orthogonal to the view) Again, the specific functions will depend on the datatype, and you can learn more in the [[Interaction]] section.","title":"Interacting with data"},{"location":"quickstart/#processing-data","text":"There are many modules available for processing data, and they are listed in Modules menu. The modules are organized by the primary data type they operate on; however, many modules combine datatypes. When you select a module, a window will open that will let you specify inputs, parameters, and outputs. For example, the VolumeFilterMedian module will look like this: Each module includes documentation, which can be shown by clicking the About button. Each parameter also has a brief description that can be revealed by hovering the mouse over the parameter name, e.g. window shown above. There are three options for output files: Existing data object : write output to an existing data object (this will delete anything that was previously stored there) New\u2026 : create a new data object for the output with a default name New named\u2026 : create a new data object for the output and prompt the user for the new name","title":"Processing data"},{"location":"quickstart/#checking-progress","text":"qitview produces a lot of messages, either about what you are currently doing or what is happening in the background. These are all printed to the command line, so it can be helpful to make the console visible. Alternatively, you can open a messages window by selecting Settings>Messages... .","title":"Checking progress"},{"location":"quickstart/#saving-your-work","text":"After you\u2019ve explored and analyzed your data, there are several ways to save your work. The simplest is to take a screenshot using File>Take Screenshot (1x)... . This will prompt you for a filename and save the screenshot there. The tool also supports high resolution screenshots, for example, at twice the resolution of the viewing window (2x). You can also save the data objects from your analysis using File>Save Selected File(s) . This will prompt you for a filename and save the selected data object there. Once you have saved a data object, it will also remember the filename and save to the same place next time. If you want to save the data object to a new place, you can instead select File>Save Selected File(s) As... . If you loaded your data from a file, the input filename will be associated with the file for saving. You will see a warning before overwriting an existing file (such as the original input), but the more adventurous user can disable such warnings by selecting File>File clobbering . If you are more of a packrat, you can enable file backups by selecting File>File backups , which will rename the existing file with a timestamp before writing the new data. Finally, if you just want to save everything, you can select File>Save Entire Scene , which will write everything to a single directory with some additional metadata about the scene.","title":"Saving your work"},{"location":"support/","text":"How to get help using QIT Direct Support I\u2019m always happy to help answer questions about QIT, so please feel free reach out via email to cabeen@gmail.com . Otherwise, there are some frequently asked questions listed below that might help. Frequently Asked Questions Why does qitview close immediately after opening it? This is most likely because you have a 32-bit version of Java installed. Imaging datasets can easily exceed the maximum memory allowed by a 32-bit address space, so you should try installing a 64-bit version of Java Note: on the Java download page, 64-bit is usually named x64, and 32-bit is usually named x86. Why did I get an error \u201cOut of Memory Error (Java heap space)\u201d? This happens when you process a dataset that is larger than the available memory. Each time qit and qitview is executed, a Java virtual machine is created with a given amount of available heap space for storing data. The default amount is kept smallish at 2 gigabytes to avoid bogging down the system; however, you sometimes have to start it with more. Here\u2019s an example command using 8 gigabytes instead: qit -Xmx8G VolumeCopy --input input.nii.gz --output output.nii.gz 8G can be replaced with whatever other amount you like. You can also specify other -X style flags supported by your system\u2019s Java virtual machine. If that doesn\u2019t help, then you may have a huge dataset. In that case, you can also switch the data precision. By default the tools use double for all storage and processing, but you can change that to float instead: qit -Xmx8G --dtype float VolumeCopy --input input.nii.gz --output output.nii.gz This will cut the memory usage roughly in half, at the cost of some precision. Mathematical operations will still be performed using double precision, but when things are saved back to volume, they will be in float. Why did I get an error about exceeding the heap size? Some systems have a Java installation that limits the amount of memory available, particularly when a 32-bit version is installed. qitview uses a heap of 4 gigabytes by default, which can often exceed that. You can reduce the heap size, e.g. by adding \u201c-Xmx1G\u201d to the command; however, this may be too little to load your data. The better option is to upgrade your Java installation to a newer 64-bit version. Why do I get an error about a missing main class? You may have gotten one of the following errors: Error: Could not find or load main class qitview.QitViewMain Error: Could not find or load main class qit.main.QitMain This is usually because you are executing the tools from the source directory instead of the installation directory. You should check your paths to make sure the binary directory is yourpath/qit-install/bin and not yourpath/qit-main/bin . Why did my program print \u201cerror: null\u201d? This means the program had an error that was not well-documented. This could be a bug or it could mean the input was incorrect. It can be helpful to run the program with the verbose and debug flags, for example: qit --verbose --debug VolumeCopy --input input.nii.gz --output output.nii.gz This will give you very detailed information about the problem. If you are still stumped or the output seems arcane, please ask around or forward it along as a bug report. How do I file a bug report? Bug reports can improve the quality of the toolkit for everyone, so please don\u2019t hesitate to send them along. At the moment, the best way is to just email me (Ryan). If the error is on the command line, please include the stack trace of the error by running it with the verbose and debug flags, e.g. qit --verbose --debug VolumeCopy --input input.nii.gz --output output.nii.gz If it is a bug is the viewer, a screen capture can be helpful. It is also useful to have a copy of the associated data and command to reproduce the problem, if they can be shared. Why is my data only showing up in the bottom left portion of the viewer? This happens when you you use Java 9 on a HiDPI display on Windows (not sure about GNU/Linux). This is a bug in either the JVM or JOGL, so we have to work around it. One solution is to downgrade to Java 8; however, if you have a 4k display, the text will be very small, since Java 8 does not support interface scaling on Windows. Alternatively, you can manually specify your display scaling factor by opening \u2018\u2019\u2018Advanced Settings\u2019\u2018\u2019 menu item, expanding the \u2018\u2019\u2018Rendering\u2019\u2018\u2019 panel, and entering your scaling factor. You look up your scaling factor by searching for \u201cDisplay\u201d in the Windows search bar and then finding the setting listed under \u201cChange the size of text, apps, and other items\u201d. Why doesn\u2019t X11 forwarding work? If you are trying to use X11 forwarding on a Mac, you may have encountered the something like the following error: libGL error: No matching fbConfigs or visuals found libGL error: failed to load driver: swrast Caught handled GLException: X11GLXDrawableFactory - Could not initialize shared resources for X11GraphicsDevice[type .x11, connection localhost:12.0, unitID 0, handle 0x0, owner false, ResourceToolkitLock[obj 0xb5f603a, isOwner false, <664d7475, 61e286b2>[count 0, qsz 0, owner <NULL>]]] on thread AWT-EventQueue-0-SharedResourceRunner [0]: jogamp.opengl.x11.glx.X11GLXDrawableFactory$SharedResourceImplementation.createSharedResource(X11GLXDrawableFactory.java:306) [1]: jogamp.opengl.SharedResourceRunner.run(SharedResourceRunner.java:353) [2]: java.lang.Thread.run(Thread.java:745) Caused[0] by GLException: AWT-EventQueue-0-SharedResourceRunner: Error making temp context(1) current: display 0x7ff92c00b120, context 0x7ff92c03efa0, drawable X11OnscreenGLXDrawable[Realized true, Factory jogamp.opengl.x11.glx.X11GLXDrawableFactory@23aa3d6e, Handle 0xc00002, Surface WrappedSurface[ displayHandle 0x7ff92c00b120 , surfaceHandle 0xc00002 , size 64x64 , UOB[ OWNS_SURFACE | WINDOW_INVISIBLE ] , X11GLXGraphicsConfiguration[X11GraphicsScreen[X11GraphicsDevice[type .x11, connection localhost:12.0, unitID 0, handle 0x7ff92c00b120, owner true, ResourceToolkitLock[obj 0x7a54e00c, isOwner true, <48ad288f, 5ad1a4e>[count 2, qsz 0, owner <AWT-EventQueue-0-SharedResourceRunner>]]], idx 0], visualID 0xf1, fbConfigID 0x83, requested GLCaps[rgba 8/8/8/0, opaque, accum-rgba 0/0/0/0, dp/st/ms 16/0/0, dbl, mono , hw, GLProfile[GL2/GL2.sw], on-scr[.]], chosen GLCaps[glx vid 0xf1, fbc 0x83: rgba 8/8/8/8, opaque, accum-rgba 0/0/0/0, dp/st/ms 24/0/0, dbl, mono , hw, GLProfile[GL2/GL2.sw], on-scr[.]]] , surfaceLock <779e16d8, 2a17cca7>[count 1, qsz 0, owner <AWT-EventQueue-0-SharedResourceRunner>] , X11DummyUpstreamSurfaceHook[pixel 64x64] , upstreamSurface false ]] on thread AWT-EventQueue-0-SharedResourceRunner [0]: jogamp.opengl.x11.glx.X11GLXContext.createImpl(X11GLXContext.java:393) [1]: jogamp.opengl.GLContextImpl.makeCurrentWithinLock(GLContextImpl.java:765) [2]: jogamp.opengl.GLContextImpl.makeCurrent(GLContextImpl.java:648) [3]: jogamp.opengl.GLContextImpl.makeCurrent(GLContextImpl.java:586) [4]: jogamp.opengl.x11.glx.X11GLXDrawableFactory$SharedResourceImplementation.createSharedResource(X11GLXDrawableFactory.java:277) [5]: jogamp.opengl.SharedResourceRunner.run(SharedResourceRunner.java:353) [6]: java.lang.Thread.run(Thread.java:745) 1667 [qit] Profile GL2 is not available on null, but: [] This occurs when using a newish version of XQuartz. For some (unknown) reason, versions 2.7.9 and greater do not play well with OpenGL. You can resolve this issue by the following steps: Close XQuartz Open Terminal.app (inside Applications -> Utilities) Copy/paste the following commands into Terminal, and enter password where applicable. Run the following in the terminal: launchctl unload /Library/LaunchAgents/org.macosforge.xquartz.startx.plist && \\ sudo launchctl unload /Library/LaunchDaemons/org.macosforge.xquartz.privileged_startx.plist && \\ sudo rm -rf /opt/X11* /Library/Launch*/org.macosforge.xquartz.* /Applications/Utilities/XQuartz.app /etc/*paths.d/*XQuartz && \\ sudo pkgutil --forget org.macosforge.xquartz.pkg && \\ rm -rf ~/.serverauth* && rm -rf ~/.Xauthorit* && rm -rf ~/.cache && rm -rf ~/.rnd && \\ rm -rf ~/Library/Caches/org.macosforge.xquartz.X11 && rm -rf ~/Library/Logs/X11 Download XQuartz 2.7.8 from https://dl.bintray.com/xquartz/downloads/XQuartz-2.7.8.dmg and install the package. Log out, and log back in.","title":"Support"},{"location":"support/#how-to-get-help-using-qit","text":"","title":"How to get help using QIT"},{"location":"support/#direct-support","text":"I\u2019m always happy to help answer questions about QIT, so please feel free reach out via email to cabeen@gmail.com . Otherwise, there are some frequently asked questions listed below that might help.","title":"Direct Support"},{"location":"support/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"support/#why-does-qitview-close-immediately-after-opening-it","text":"This is most likely because you have a 32-bit version of Java installed. Imaging datasets can easily exceed the maximum memory allowed by a 32-bit address space, so you should try installing a 64-bit version of Java Note: on the Java download page, 64-bit is usually named x64, and 32-bit is usually named x86.","title":"Why does qitview close immediately after opening it?"},{"location":"support/#why-did-i-get-an-error-out-of-memory-error-java-heap-space","text":"This happens when you process a dataset that is larger than the available memory. Each time qit and qitview is executed, a Java virtual machine is created with a given amount of available heap space for storing data. The default amount is kept smallish at 2 gigabytes to avoid bogging down the system; however, you sometimes have to start it with more. Here\u2019s an example command using 8 gigabytes instead: qit -Xmx8G VolumeCopy --input input.nii.gz --output output.nii.gz 8G can be replaced with whatever other amount you like. You can also specify other -X style flags supported by your system\u2019s Java virtual machine. If that doesn\u2019t help, then you may have a huge dataset. In that case, you can also switch the data precision. By default the tools use double for all storage and processing, but you can change that to float instead: qit -Xmx8G --dtype float VolumeCopy --input input.nii.gz --output output.nii.gz This will cut the memory usage roughly in half, at the cost of some precision. Mathematical operations will still be performed using double precision, but when things are saved back to volume, they will be in float.","title":"Why did I get an error \"Out of Memory Error (Java heap space)\"?"},{"location":"support/#why-did-i-get-an-error-about-exceeding-the-heap-size","text":"Some systems have a Java installation that limits the amount of memory available, particularly when a 32-bit version is installed. qitview uses a heap of 4 gigabytes by default, which can often exceed that. You can reduce the heap size, e.g. by adding \u201c-Xmx1G\u201d to the command; however, this may be too little to load your data. The better option is to upgrade your Java installation to a newer 64-bit version.","title":"Why did I get an error about exceeding the heap size?"},{"location":"support/#why-do-i-get-an-error-about-a-missing-main-class","text":"You may have gotten one of the following errors: Error: Could not find or load main class qitview.QitViewMain Error: Could not find or load main class qit.main.QitMain This is usually because you are executing the tools from the source directory instead of the installation directory. You should check your paths to make sure the binary directory is yourpath/qit-install/bin and not yourpath/qit-main/bin .","title":"Why do I get an error about a missing main class?"},{"location":"support/#why-did-my-program-print-error-null","text":"This means the program had an error that was not well-documented. This could be a bug or it could mean the input was incorrect. It can be helpful to run the program with the verbose and debug flags, for example: qit --verbose --debug VolumeCopy --input input.nii.gz --output output.nii.gz This will give you very detailed information about the problem. If you are still stumped or the output seems arcane, please ask around or forward it along as a bug report.","title":"Why did my program print \"error: null\"?"},{"location":"support/#how-do-i-file-a-bug-report","text":"Bug reports can improve the quality of the toolkit for everyone, so please don\u2019t hesitate to send them along. At the moment, the best way is to just email me (Ryan). If the error is on the command line, please include the stack trace of the error by running it with the verbose and debug flags, e.g. qit --verbose --debug VolumeCopy --input input.nii.gz --output output.nii.gz If it is a bug is the viewer, a screen capture can be helpful. It is also useful to have a copy of the associated data and command to reproduce the problem, if they can be shared.","title":"How do I file a bug report?"},{"location":"support/#why-is-my-data-only-showing-up-in-the-bottom-left-portion-of-the-viewer","text":"This happens when you you use Java 9 on a HiDPI display on Windows (not sure about GNU/Linux). This is a bug in either the JVM or JOGL, so we have to work around it. One solution is to downgrade to Java 8; however, if you have a 4k display, the text will be very small, since Java 8 does not support interface scaling on Windows. Alternatively, you can manually specify your display scaling factor by opening \u2018\u2019\u2018Advanced Settings\u2019\u2018\u2019 menu item, expanding the \u2018\u2019\u2018Rendering\u2019\u2018\u2019 panel, and entering your scaling factor. You look up your scaling factor by searching for \u201cDisplay\u201d in the Windows search bar and then finding the setting listed under \u201cChange the size of text, apps, and other items\u201d.","title":"Why is my data only showing up in the bottom left portion of the viewer?"},{"location":"support/#why-doesnt-x11-forwarding-work","text":"If you are trying to use X11 forwarding on a Mac, you may have encountered the something like the following error: libGL error: No matching fbConfigs or visuals found libGL error: failed to load driver: swrast Caught handled GLException: X11GLXDrawableFactory - Could not initialize shared resources for X11GraphicsDevice[type .x11, connection localhost:12.0, unitID 0, handle 0x0, owner false, ResourceToolkitLock[obj 0xb5f603a, isOwner false, <664d7475, 61e286b2>[count 0, qsz 0, owner <NULL>]]] on thread AWT-EventQueue-0-SharedResourceRunner [0]: jogamp.opengl.x11.glx.X11GLXDrawableFactory$SharedResourceImplementation.createSharedResource(X11GLXDrawableFactory.java:306) [1]: jogamp.opengl.SharedResourceRunner.run(SharedResourceRunner.java:353) [2]: java.lang.Thread.run(Thread.java:745) Caused[0] by GLException: AWT-EventQueue-0-SharedResourceRunner: Error making temp context(1) current: display 0x7ff92c00b120, context 0x7ff92c03efa0, drawable X11OnscreenGLXDrawable[Realized true, Factory jogamp.opengl.x11.glx.X11GLXDrawableFactory@23aa3d6e, Handle 0xc00002, Surface WrappedSurface[ displayHandle 0x7ff92c00b120 , surfaceHandle 0xc00002 , size 64x64 , UOB[ OWNS_SURFACE | WINDOW_INVISIBLE ] , X11GLXGraphicsConfiguration[X11GraphicsScreen[X11GraphicsDevice[type .x11, connection localhost:12.0, unitID 0, handle 0x7ff92c00b120, owner true, ResourceToolkitLock[obj 0x7a54e00c, isOwner true, <48ad288f, 5ad1a4e>[count 2, qsz 0, owner <AWT-EventQueue-0-SharedResourceRunner>]]], idx 0], visualID 0xf1, fbConfigID 0x83, requested GLCaps[rgba 8/8/8/0, opaque, accum-rgba 0/0/0/0, dp/st/ms 16/0/0, dbl, mono , hw, GLProfile[GL2/GL2.sw], on-scr[.]], chosen GLCaps[glx vid 0xf1, fbc 0x83: rgba 8/8/8/8, opaque, accum-rgba 0/0/0/0, dp/st/ms 24/0/0, dbl, mono , hw, GLProfile[GL2/GL2.sw], on-scr[.]]] , surfaceLock <779e16d8, 2a17cca7>[count 1, qsz 0, owner <AWT-EventQueue-0-SharedResourceRunner>] , X11DummyUpstreamSurfaceHook[pixel 64x64] , upstreamSurface false ]] on thread AWT-EventQueue-0-SharedResourceRunner [0]: jogamp.opengl.x11.glx.X11GLXContext.createImpl(X11GLXContext.java:393) [1]: jogamp.opengl.GLContextImpl.makeCurrentWithinLock(GLContextImpl.java:765) [2]: jogamp.opengl.GLContextImpl.makeCurrent(GLContextImpl.java:648) [3]: jogamp.opengl.GLContextImpl.makeCurrent(GLContextImpl.java:586) [4]: jogamp.opengl.x11.glx.X11GLXDrawableFactory$SharedResourceImplementation.createSharedResource(X11GLXDrawableFactory.java:277) [5]: jogamp.opengl.SharedResourceRunner.run(SharedResourceRunner.java:353) [6]: java.lang.Thread.run(Thread.java:745) 1667 [qit] Profile GL2 is not available on null, but: [] This occurs when using a newish version of XQuartz. For some (unknown) reason, versions 2.7.9 and greater do not play well with OpenGL. You can resolve this issue by the following steps: Close XQuartz Open Terminal.app (inside Applications -> Utilities) Copy/paste the following commands into Terminal, and enter password where applicable. Run the following in the terminal: launchctl unload /Library/LaunchAgents/org.macosforge.xquartz.startx.plist && \\ sudo launchctl unload /Library/LaunchDaemons/org.macosforge.xquartz.privileged_startx.plist && \\ sudo rm -rf /opt/X11* /Library/Launch*/org.macosforge.xquartz.* /Applications/Utilities/XQuartz.app /etc/*paths.d/*XQuartz && \\ sudo pkgutil --forget org.macosforge.xquartz.pkg && \\ rm -rf ~/.serverauth* && rm -rf ~/.Xauthorit* && rm -rf ~/.cache && rm -rf ~/.rnd && \\ rm -rf ~/Library/Caches/org.macosforge.xquartz.X11 && rm -rf ~/Library/Logs/X11 Download XQuartz 2.7.8 from https://dl.bintray.com/xquartz/downloads/XQuartz-2.7.8.dmg and install the package. Log out, and log back in.","title":"Why doesn't X11 forwarding work?"},{"location":"tractography/","text":"Tractography Analysis with QIT This page provides a tutorial for doing tractography with QIT. The tutorial will show you how to extract a fiber bundle model of the arcuate fasciculus from a diffusion MRI dataset. This will include glyph visualization, interactive seeding with a manually placed sphere, streamline integration, bundle isolation, and creating a high resolution rendering. The [[Diffusion MRI Tutorial]]] provides some useful introductory material, and if you are new to tractography, you may want to go through that first. Setup Before starting the tutorial, you\u2019ll need a few things. First, we will make sure you have the necessary dependencies; then, we will download and install QIT; finally, we will download the sample data. Installation First, make sure you\u2019ve installed QIT and its dependencies by following the [[Installation]] instructions. You won\u2019t need the advanced dependencies for this tutorial, so you can skip that. Downloading the sample dataset Next, we\u2019ll download the sample dataset, which is available here: http://cabeen.io/download/dmri.tutorial.data.zip After you decompress the archive, you should find these files inside: input/dwi.nii.gz : a diffusion weighted MR image volume input/mask.nii.gz : a brain mask input/bvecs.txt : a b-vectors file input/bvals.txt : a b-values file diff.models.dti : a diffusion tensor volume directory diff.models.xfib : a ball and sticks volume directory diff.models.fod.nii.gz : a spherical harmonic FOD volume directory There are other files in the archive, but the ones above are strictly required for the tutorial. This dataset represents a basic 60 direction single shell diffusion MRI scan acquired on a 1.5T scanner. The dataset is described in more detail here Optional: Fitting models Note: this section is not required for completing the tutorial. The tutorial dataset provides diffusion model data (e.g. dti, fod, xfib) to expedite things, but if you want to repeat this analysis with your own data, you will have to fit these models yourself. Below are the commands you can use for that: $ qit VolumeTensorFit --input input/dwi.nii.gz --gradients input/bvecs.txt --mask input/mask.nii.gz --output diff.models.dti $ qit VolumeFibersFitFsl --input input/dwi.nii.gz --gradients input/bvecs.txt --mask input/mask.nii.gz --output diff.models.xfib $ qit VolumeSpharmFitMrtrix --input input/dwi.nii.gz --gradients input/bvecs.txt --mask input/mask.nii.gz --output diff.models.fod.nii.gz There are many ways to adjust the model fitting, and the usage page will show you how to adjust those parameters. For example, you can change the algorithm used for fitting, exclude certain scans/shells, or speed up the fitting with multiple threads. Note: to run the above commands, you will have to install FSL and MRtrix. Please see the advanced section of the [[Installation]] page for more details. They are only required for creating those files, so you can run the following tutorial without installing those 3rd party packages. Tutorial Now that we have installed QIT and downloaded the sample data, we\u2019ll go over how to do tractography analysis. Loading data Our first step is to load the input data. You should open File>Load Files from the menu, click the Add more files button, select diff.models.xfib , and then click Load files into workspace . After it has loaded, you can show the dataset by clicking the box next to its name and explore it to your liking. Try to show only a coronal slice. Rendering glyphs Next, we will visualize the diffusion models using glyphs. First, select diff.models.xfib , then open the Glyph Rendering panel. For the Type , select Fibers and then check the Visible box. This should show some lines representing fiber orientations. They are currently being cut by the background image slice , so you can nudge it by opening the Slice Rendering panel and incrementing the counter next to the slice index. Then you can show better 3D glyphs by checking the Ball , Sticks , and Glyph checkboxes in the Glyph Rendering panel. This should now depict the fiber volume fraction by the stick thickness and show a ball depicting the isotropic compartment. Creating a seed sphere Next, we will create a sphere for interactive seed-based tractography. You can add a sphere to the workspace by right clicking on diff.models.xfib and selecting the Create Sphere menu item. This will create a sphere as big as the volume, so we need to shrink it and place near our bundle of interest. You can resize it by clicking on it holding down Alt+Shift and dragging the mouse. You can translate it by clicking on it while holding down Alt and dragging the mouse. You can read more about this on the [[Interaction]] page. You should try to position the sphere around the lateral green fibers of the arcuate, as shown in the video below: Generating tracks Next, we will use the sphere for seed-based tractography. First, you should open the Modules>Seach\u2026 menu item, and then find and open VolumeModelTrackStreamline . Then, you should make sure the desired volume is selected as input , and then open the Optional Input panel and set the seedSolids to seed . Then open the Parameters panel and change the samples to 5000 . This is the number of tractography curves initiated from inside the sphere. Then you can select Update to run and leave the window open, or Apply to run it and close the window. This should leave you with a tracks object in your workspace. You can check the box to show it and rotate the view to see it from a lateral view. At this point you can also hide the diffusion model volume by unchecking its box. Isolating Tracks The bundle we have now includes the arcuate, but we need to isolate it from the others. To do this, we will create a new spheres object (similar to before) and name it include . We will use this to select the only the arcuate fibers from tracks . You should resize and position the sphere in the anterior part of the arcuate, as shown in the animation below. Then, we will set this sphere as an inclusion criteria. You should make sure tracks is selected and then open the Selection panel. You can then set the Solids Include combobox to include . This will isolate the fibers in the frontal portion. You can interactively move the sphere to see which fibers are included or excluded based on the position of the sphere. We still have some stray fibers, so we will add another sphere to include by first selecting it in the workspace and then Alt+Control clicking on the bundle. You can then resize and position the sphere to select the temporal portion. Now, we should have mostly isolated the arcuate. You can finalize these changes by opening the Selection panel of tracks and then selecting the Retain Full Selection button (this will delete all curves not in the selection). You can then change the Solids Include to None , since we are no longer using it. Rendering Tracks Finally, we will create a visualization of our tracks. First, you should open the Rendering panel of the tracks object and select the Tubes checkbox. Then, you can open the Settings>Advanced Settings menu item, open the Rendering panel, and change the background color to black. Then you can position the curves to your liking and select the File>Take Screenshot (2x) to save a high resolution screenshot (twice the screen resolution). Then you should have a PNG image of your bundle that is ready for your poster or publication: Conclusion Congratulations! You\u2019ve completed a basic tractography reconstruction from diffusion MRI data. Feel free to experiment with other modules and settings in qitview or ask around if you\u2019d like to know more about what else you can do. In particular, you can try the other diffusion model data, e.g. diff.models.dti and diff.models.fod.nii.gz . The only modifications you have to make is the Type of glyph, which should be Tensor and Spharm , respectively.","title":"Tractography Tutorial"},{"location":"tractography/#tractography-analysis-with-qit","text":"This page provides a tutorial for doing tractography with QIT. The tutorial will show you how to extract a fiber bundle model of the arcuate fasciculus from a diffusion MRI dataset. This will include glyph visualization, interactive seeding with a manually placed sphere, streamline integration, bundle isolation, and creating a high resolution rendering. The [[Diffusion MRI Tutorial]]] provides some useful introductory material, and if you are new to tractography, you may want to go through that first.","title":"Tractography Analysis with QIT"},{"location":"tractography/#setup","text":"Before starting the tutorial, you\u2019ll need a few things. First, we will make sure you have the necessary dependencies; then, we will download and install QIT; finally, we will download the sample data.","title":"Setup"},{"location":"tractography/#installation","text":"First, make sure you\u2019ve installed QIT and its dependencies by following the [[Installation]] instructions. You won\u2019t need the advanced dependencies for this tutorial, so you can skip that.","title":"Installation"},{"location":"tractography/#downloading-the-sample-dataset","text":"Next, we\u2019ll download the sample dataset, which is available here: http://cabeen.io/download/dmri.tutorial.data.zip After you decompress the archive, you should find these files inside: input/dwi.nii.gz : a diffusion weighted MR image volume input/mask.nii.gz : a brain mask input/bvecs.txt : a b-vectors file input/bvals.txt : a b-values file diff.models.dti : a diffusion tensor volume directory diff.models.xfib : a ball and sticks volume directory diff.models.fod.nii.gz : a spherical harmonic FOD volume directory There are other files in the archive, but the ones above are strictly required for the tutorial. This dataset represents a basic 60 direction single shell diffusion MRI scan acquired on a 1.5T scanner. The dataset is described in more detail here","title":"Downloading the sample dataset"},{"location":"tractography/#optional-fitting-models","text":"Note: this section is not required for completing the tutorial. The tutorial dataset provides diffusion model data (e.g. dti, fod, xfib) to expedite things, but if you want to repeat this analysis with your own data, you will have to fit these models yourself. Below are the commands you can use for that: $ qit VolumeTensorFit --input input/dwi.nii.gz --gradients input/bvecs.txt --mask input/mask.nii.gz --output diff.models.dti $ qit VolumeFibersFitFsl --input input/dwi.nii.gz --gradients input/bvecs.txt --mask input/mask.nii.gz --output diff.models.xfib $ qit VolumeSpharmFitMrtrix --input input/dwi.nii.gz --gradients input/bvecs.txt --mask input/mask.nii.gz --output diff.models.fod.nii.gz There are many ways to adjust the model fitting, and the usage page will show you how to adjust those parameters. For example, you can change the algorithm used for fitting, exclude certain scans/shells, or speed up the fitting with multiple threads. Note: to run the above commands, you will have to install FSL and MRtrix. Please see the advanced section of the [[Installation]] page for more details. They are only required for creating those files, so you can run the following tutorial without installing those 3rd party packages.","title":"Optional: Fitting models"},{"location":"tractography/#tutorial","text":"Now that we have installed QIT and downloaded the sample data, we\u2019ll go over how to do tractography analysis.","title":"Tutorial"},{"location":"tractography/#loading-data","text":"Our first step is to load the input data. You should open File>Load Files from the menu, click the Add more files button, select diff.models.xfib , and then click Load files into workspace . After it has loaded, you can show the dataset by clicking the box next to its name and explore it to your liking. Try to show only a coronal slice.","title":"Loading data"},{"location":"tractography/#rendering-glyphs","text":"Next, we will visualize the diffusion models using glyphs. First, select diff.models.xfib , then open the Glyph Rendering panel. For the Type , select Fibers and then check the Visible box. This should show some lines representing fiber orientations. They are currently being cut by the background image slice , so you can nudge it by opening the Slice Rendering panel and incrementing the counter next to the slice index. Then you can show better 3D glyphs by checking the Ball , Sticks , and Glyph checkboxes in the Glyph Rendering panel. This should now depict the fiber volume fraction by the stick thickness and show a ball depicting the isotropic compartment.","title":"Rendering glyphs"},{"location":"tractography/#creating-a-seed-sphere","text":"Next, we will create a sphere for interactive seed-based tractography. You can add a sphere to the workspace by right clicking on diff.models.xfib and selecting the Create Sphere menu item. This will create a sphere as big as the volume, so we need to shrink it and place near our bundle of interest. You can resize it by clicking on it holding down Alt+Shift and dragging the mouse. You can translate it by clicking on it while holding down Alt and dragging the mouse. You can read more about this on the [[Interaction]] page. You should try to position the sphere around the lateral green fibers of the arcuate, as shown in the video below:","title":"Creating a seed sphere"},{"location":"tractography/#generating-tracks","text":"Next, we will use the sphere for seed-based tractography. First, you should open the Modules>Seach\u2026 menu item, and then find and open VolumeModelTrackStreamline . Then, you should make sure the desired volume is selected as input , and then open the Optional Input panel and set the seedSolids to seed . Then open the Parameters panel and change the samples to 5000 . This is the number of tractography curves initiated from inside the sphere. Then you can select Update to run and leave the window open, or Apply to run it and close the window. This should leave you with a tracks object in your workspace. You can check the box to show it and rotate the view to see it from a lateral view. At this point you can also hide the diffusion model volume by unchecking its box.","title":"Generating tracks"},{"location":"tractography/#isolating-tracks","text":"The bundle we have now includes the arcuate, but we need to isolate it from the others. To do this, we will create a new spheres object (similar to before) and name it include . We will use this to select the only the arcuate fibers from tracks . You should resize and position the sphere in the anterior part of the arcuate, as shown in the animation below. Then, we will set this sphere as an inclusion criteria. You should make sure tracks is selected and then open the Selection panel. You can then set the Solids Include combobox to include . This will isolate the fibers in the frontal portion. You can interactively move the sphere to see which fibers are included or excluded based on the position of the sphere. We still have some stray fibers, so we will add another sphere to include by first selecting it in the workspace and then Alt+Control clicking on the bundle. You can then resize and position the sphere to select the temporal portion. Now, we should have mostly isolated the arcuate. You can finalize these changes by opening the Selection panel of tracks and then selecting the Retain Full Selection button (this will delete all curves not in the selection). You can then change the Solids Include to None , since we are no longer using it.","title":"Isolating Tracks"},{"location":"tractography/#rendering-tracks","text":"Finally, we will create a visualization of our tracks. First, you should open the Rendering panel of the tracks object and select the Tubes checkbox. Then, you can open the Settings>Advanced Settings menu item, open the Rendering panel, and change the background color to black. Then you can position the curves to your liking and select the File>Take Screenshot (2x) to save a high resolution screenshot (twice the screen resolution). Then you should have a PNG image of your bundle that is ready for your poster or publication:","title":"Rendering Tracks"},{"location":"tractography/#conclusion","text":"Congratulations! You\u2019ve completed a basic tractography reconstruction from diffusion MRI data. Feel free to experiment with other modules and settings in qitview or ask around if you\u2019d like to know more about what else you can do. In particular, you can try the other diffusion model data, e.g. diff.models.dti and diff.models.fod.nii.gz . The only modifications you have to make is the Type of glyph, which should be Tensor and Spharm , respectively.","title":"Conclusion"},{"location":"workflow/","text":"Automated image analysis with QIT Welcome! This is a guide for automated quantitative analysis of diffusion MRI data using QIT. If you have collected many diffusion MRI scans in an imaging study, this workflow can help you extract quantitative measures of structural anatomical characteristics across your group and ultimately use these measures to explore their statistical relationship with demographic and behavioral variables. Overview The workflow is an end-to-end tool that supports a variety of ways to analyze diffusion MRI data, in which you start from raw imaging data and produce geometric models and quantitative summaries of tissue microstructure, morphometry, and connectivity. Specifically, you can perform region-based and tractography-based analysis and combine these with tissue parameter mapping using diffusion tensor imaging and advanced multi-shell approaches. In addition to diffusion MRI, the workflow has components that streamline the use of state-of-the-art packages for morphometric analysis using T1-weighted MRI data, and further, allow them to be combined with diffusion MRI data. If you are new to diffusion MRI and would like to learn more, Diffusion Tensor Imaging: a Practical Handbook and NMR in Biomedicine: Special Issue on Diffusion MRI of the brain are both good starting points. In the following sections, this guide will cover how to install the necessary software on your computer, how to prepare your imaging data, how to run various analyses, and how to combine the results across subjects. Installing software This section will guide you through the installation of QIT and its dependencies. The workflow is designed for the command line, and if you need need to learn about it, you can check out the tutorials at Software Carpentry . Note: while the workflow primarily uses QIT, it also depends on several other 3rd party tools that are designed to run only on Linux and Mac. So while QIT itself can run on Windows, to run the complete workflow on Windows you would need a virtualization solution, such as VMware , VirtualBox , or the Windows Subsystem for Linux . Installing QIT You should first follow the instructions on the Installation page, and make sure that the bin directory is included in your shell PATH variable. You can read [https://linuxize.com/post/how-to-add-directory-to-path-in-linux/ this page] to learn more about adding a directory to your path. After installing, you can check that things worked by running the following command and seeing if it produces something analogous (your version should be this one or later): $ qit --version <br> QIT build revision: 2005\\:2019, build time: 03/25/2020 05\\:09 PM` Installing dependencies Next, you should install the other software packages required by the workflow, which are two common and publicly available 3rd party neuroimaging tools: DTI-TK and FSL . You can follow the installation instructions on the linked pages, and afterwards, you can test that they are correctly installed by trying these commands: $ which bedpostx /opt/share/fsl-5.0.10/bin/bedpostx $ which TVMean /opt/share/dtitk/bin/TVMean Note: there is a GPU version of FSL bedpostx that can dramatically speed things up. If you install it using the instructions on the linked page, you can enable it in the QIT workflow by adding GPU=1 to the qitdiff command. Installing more dependencies (optional) As an optional step, if want to use T1 MRI data in the analysis, you should also install [https://surfer.nmr.mgh.harvard.edu/ FreeSurfer] and [https://stnava.github.io/ANTs/ ANTs]. These are not required for most parts of the pipeline, but they can offer more precise segmentation of certain brain structures, which are described in more detail later in the guide. You can follow the installation instructions on the linked pages, and you can test that they are correctly installed by trying these commands: $ which recon-all /opt/share/freesurfer/bin/recon-all $ which antsRegistrationSyn.sh /opt/share/ANTs-install/bin/antsRegistrationSyn.sh Preparing your data This section provides instructions for converting your data into a format that is compatible with the workflow. You may already have data in a suitable format, so not all of these steps will be necessary. The workflow expects the input to be a diffusion-weighted MRI in the NIfTI and associated text files for the b-vectors and b-values. If you need data to try out, you can download this example clinical quality single shell dataset: http://cabeen.io/download/dmri.tutorial.data.zip If you\u2019d like to skip the processing, you can also check out these example results: http://cabeen.io/download/qitdiff.demo.tar.gz File format conversion Data from the scanner typically is stored in the DICOM file format , which stores the all data from each session in a directory that combines imaging, sequence parameters, and patient information across many files contained within. You will need to convert DICOM data to the NIfTI file format , a simpler format that is more amenable to imaging research. There are a few tools available for converting between NIfTI and DICOM, but a good starting point is the dcm2niix tool. The linked page shares instructions for installing the software and applying it to your DICOM data to produce NIfTI volumes. Afterwards, you should be left with nii.gz files from your session, one of which will be a multi-channel diffusion MRI. You should also find bvecs and bvals files, which list the b-vectors and b-values sequence parameters from the scan, respectively. Artifact Correction There are a variety of artifact that can be corrected in diffusion MRI, and it is recommended that you apply any corrections that are compatible with your data collection scheme. QIT offers some basic corrections for denoising, signal drift correction, and motion correction, and these will be noted in the next section. However, if possible, it is recommended that you use advanced preprocessing tools when possible. In particular, if you have collected data with an additional set of reversed-phase encoding scans, you can use FSL EDDY to greatly improve the quality of your data by simultaneously correcting for motion, eddy-current, susceptibility-induced geometric distortion. You should explore using an OpenMP accelerated version, as it can be quite time consuming. To apply it to your data, you can use the FSL or the HCP pipelines . Once you\u2019ve finished processing it, you should find the corrected diffusion MRI and b-vectors, which will be named eddy_out.nii.gz and eddy_out.eddy_rotated_bvecs , respectively. The b-values are not changed by the correction, so you should use the raw values. If you are interested in other artifact correction features of QIT, you can check out these modules: VolumeFilterNLM , VolumeDwiCorrectDrift , and VolumeDwiCorrect , which provide denoising, drift correction, and motion correction, respectively. General considerations In this section and the following three, we describe the different ways that you can run the QIT diffusion workflow. We\u2019ll start by discussing the general interface to the script, and then describe specific types of analysis in detail. The primary interface to the QIT diffusion workflow is a program named qitdiff . If you run this command without any arguments, you\u2019ll find a usage page, like so: $ qitdiff Name: qitdiff Description: Process diffusion-weighted imaging data using the qit diffusion workflow. The first time you run this script, you must provide the DWI, bvecs, and bvals for your subject. After that, you only need to specify the targets. Usage: qitdiff [opts] --subject subject [targets] Input Parameters (must be include in the first run, but not afterwards): --dwi dwi.nii.gz: the diffusion weighted MRI file, stored in NIFTI --bvecs bvecs: the diffusion b-vectors file, stored as text --bvals bvals: the diffusion b-values file, stored as text --subject subject: the subject directory where the results will be saved Optional Parameters (may be included in the first run, but not afterwards): --denoise <num>: denoise the dwi with the given noise level --mask <fn>: use the given precomputed brain mask --bet <num>: use the given FSL BET fraction parameter --erode <num>: erode the brain mask by the given number of voxels --motion: correct for motion and create a summary report --nomatch: skip gradient matching step --tone <fn>: specify a T1-weighted MRI --freesurfer <dir>: use the given precomputed freesurfer results --tracts <dir>: include user-defined tracts in the workflow Author: Ryan Cabeen There are three flags for specifying the input to the workflow: --dwi , --bvecs , and --bvals . The b-vectors and b-values can be organized in column or row form, and QIT will automatically determine the coordinate transform necessary to align the b-vectors to the imaging data (so there is no need to flip or transpose b-vectors). You must also specify a subject directory with --subject , but if you omit this flag, the current working directory will be used. There are several other optional flags that are listed after the required flags. You may skip these for now if you like. Besides the input data, you must also specify one or more targets . A target is a string that identifies what you want the workflow to produce, e.g. regional summaries of FA or fiber bundle geometry. In the next section of the tutorial we\u2019ll describe the different targets that are available. But to give a concrete example, you could produce region DTI parameter statistics with the JHU atlas using a command like so: $ qitdiff --subject qitsubject --dwi scan/dwi.nii.gz --bvecs scan/bvecs.txt --bvals scan/bvals.txt diff.region/jhu.labels.dti.map One of the features of the QIT workflow is that you can add additional analyses and re-use the previous work you did. So for example, if you wanted to add TBSS to your analysis, you could simply run a command like this: $ qitdiff --subject qitsubject atlas.region/jhu.labels.tbss.dti.map For example, this will skip the data import and atlas registration steps, as they were already completed. Note that you can also omit the flags for the input data, since they have already been imported. Region-of-interest analysis Region-of-interest (ROI) analysis is a simple and widely used way to summarize diffusion MRI data. In this approach, the data is spatially normalized with an atlas that contains a parcellation of brain areas, stored in a 3D mask. Then, diffusion model parameters are statistically summarized in each ROI of the atlas; for example, by computing the average fractional anisotropy in each brain area. There are quite a few variations on this approach, and QIT supports many of them. For example, you can combine the ROI approach with Tract-based Spatial Statistics (TBSS) , you can apply erosion of the masks, and you can compute the statistics in either native or atlas space. Each of these possibilities is available with a different target that you can provide to qitdiff . Here is a table summarizing the different kinds of ROI targets that are available: diff.region/jhu.labels.dti.map : DTI parameters statistics for each region in native space diff.region/jhu.labels.erode.dti.map : DTI parameters statistics for each region in native space with additional erosion step atlas.region/jhu.labels.dti.map : DTI parameters statistics for each region in atlas space atlas.region/jhu.labels.erode.dti.map : DTI parameters statistics for each region in atlas space with an additional erosion step atlas.region/jhu.labels.tbss.dti.map : DTI parameters statistics for each region in atlas space with additional TBSS processing The above examples use the Johns Hopkins University white matter atlas, but there are many other alternatives in qitdiff . Below is a table summarizing these other options. There are targets available for these, in which you simply replace jhu.labels in the targets above. jhu.labels : Johns Hopkins University deep white matter atlas) jhu.tracts : Johns Hopkins University white matter tract atlas fsa.ccwm : FreeSurfer-based corpus callosum parcellation fsa.scgm : FreeSurfer-based subcortical gray matter fsa.dkwm : FreeSurfer-based superficial white matter hox.sub : Harvard-Oxford subcortical parcellation ini.bstem : USC INI brainstem parcellation cit.amy : Caltech amygdala subfields cit.sub : Caltech subcortical parcellation Furthermore, there are many types of diffusion parameters that can be used in an ROI analysis. The examples listed above used diffusion tensor imaging (DTI) parameters, but if you have multi-shell data, there are other possibilities supported by qitdiff . The table below lists these other possibilities, and you can create targets for them by substituting dti with the appropriate model identifier. dti : Diffusion Tensor Imaging (DTI) fwdti : Free-water Elimination DTI (with a fixed diffusivity ball) dki : Diffusion Kurtosis Imaging noddi : Neurite Orientation Dispersion and Density Imaging mcsmt : Multi-compartment Spherical Mean Technique bti : Bi-tensor Imaging (DTI with an unconstrained ball) For a concrete example, if you wanted to compute NODDI parameters in subcortical brain areas, you could run something like so: $ qitdiff --subject qitsubject --dwi scan/dwi.nii.gz --bvecs scan/bvecs.txt --bvals scan/bvals.txt diff.region/fsa.scgm.noddi.map With all of these options, you may be wondering what to pick, and in this case, a good starting point is the target atlas.region/jhu.region.tbss.dti.map . This is a fairly standardized approach that summarizes DTI parameters in white matter areas from the JHU atlas with preprocessing using TBSS. For example, this is similar to the protocol used in the ENIGMA network (note: the template and registration algorithm are not identical). Tractography-based analysis Tractography analysis is an alternative approach for diffusion MRI analysis that is focused on modeling connectivity as opposed to regions. In this approach, geometric models of white matter connectivity are reconstructed from fiber orientation data estimated from diffusion MRI. With sufficiently high quality data, this approach can extract the major pathways of the brain, known as fiber bundles. The QIT diffusion workflow includes such a bundle-specific analysis, enabling the quantitative characterization of both whole bundles and subdivisions along their length. These can be run using the following qitdiff targets: diff.tract/bundles.map : whole bundle parameters, such as volume, length, etc. diff.tract/bundles.dti.map : whole bundle DTI parameters, such as FA, MD, etc. diff.tract/bundles.along.dti.map : along-bundle DTI parameters, e.g. FA measured at each of a sequence of bundle subdivisions Similar to ROI analysis, you can also extract multi-shell diffusion MRI parameters using any of the model identifiers from the previous section. For a concrete example, if you wanted to compute NODDI parameters along each bundle, you could run something like so: $ qitdiff --subject qitsubject --dwi scan/dwi.nii.gz --bvecs scan/bvecs.txt --bvals scan/bvals.txt diff.tract/bundles.along.noddi.map Multi-modal analysis Besides diffusion MRI, the QIT workflow has integrated morphometric analysis using T1-weighted MRI. These steps require ANTs and FreeSurfer to be installed (as described above). After that, you can perform cortical surface based analysis using FreeSurfer using the following command: $ qitdiff --subject qitsubject --tone t1.nii.gz tone.fs.map If you\u2019ve already run FreeSurfer, you can also import the previous results, e.g. $ qitdiff --subject qitsubject --freesurfer fs_subject_dir tone.fs.map You can also perform a multi-modal image analysis that combines the T1-weighted and diffusion MRI data. For example, you could compute DTI parameter statistics in subject-specific subcortical and superficial white matter ROIs using this command: $ qitdiff --subject qitsubject --dwi scan/dwi.nii.gz --bvecs scan/bvecs.txt --bvals scan/bvals.txt --tone t1.nii.gz tone.region/fs.scgm.dti.map tone.region/fs.dkwm.dti.map There are various other T1-related targets available, which are summarized here: tone.fs.brain : Freesurfer brain image and tissue types converted into nifti tone.fs.region : Freesurfer regions-of-interest converted into nifti tone.fs.surfaces : Freesurfer meshes converted into VTK diff.fs.brain : Freesurfer brain image and tissue types deformed into native diffusion space diff.fs.region : Freesurfer regions-of-interest deformed into native diffusion space diff.fs.surfaces : Freesurfer meshes deformed into native diffusion space tone.region/fs.scgm.dti.map : DTI parameters statistics for each Freesurfer subcortical ROI tone.region/fs.dkwm.dti.map : DTI parameters statistics for each Freesurfer Desikan white matter ROI tone.region/fs.dkgm.dti.map : DTI parameters statistics for each Freesurfer Desikan gray matter ROI Aggregating results Once you\u2019ve finished processing your data, you may want to combine the results into data tables, that is, aggregating metrics from all of the research subjects into a spreadsheet. The QIT workflow was designed to make this easy by saving results in a standardized format. You may notice that each target ends in map ; this indicates that the target is a directory that contains summary statistics. Each map directory contains an array of CSV files store simple name-value pairs. For example, the bundle-specific analysis produces this CSV file among many others: $ head diff.tract/bundles.map/volume.csv name,value lh_acoustic,13570.0 lh_thal_tempinf,9663.0 rh_arc_ant,51518.0 rh_thal_pma,21428.0 rh_med_lem,11603.0 rh_aft,6183.0 ... Suppose you have many subjects, and you have organized the data such that each subject has a QIT workflow subject directory, e.g. test_103818 , inside a directory named subjects . Further, you have a list of subject identifiers that correspond to the subject directory names, and these names are stored one-per-line in a text file named sids.txt . Then, you can create a data table and inspect the output using these commands: $ qit --verbose MapCat --pattern subjects/%{subject}/diff.tract/bundles.map/volume.csv --vars subject=sids.txt --skip --output bundles.volume.csv $ cat bundles.volume.csv | grep lh_acoustic | head test_103818,lh_acoustic,13386.0 test_105923,lh_acoustic,17255.0 test_111312,lh_acoustic,12189.0 test_114823,lh_acoustic,15710.0 test_115320,lh_acoustic,14599.0 ... The resulting table is in a long format, but you can also convert it to a wide one: $ qit TableWiden --input bundles.volume.csv --output bundles.volume.wide.csv $ head bundles.volume.wide.csv subject,lh_acoustic,lh_arc,... test_103818,13386.0,8950.0,... test_105923,17255.0,13213.0,... test_111312,12189.0,11033.0,... ... In this table, each column is a different brain area, and each row is a subject. We used bundle volume in the example, but the same approach works for all of the other metrics stored in map directories. If you would like to create tables for many different variables, there is a more complex but powerful script called qitmaketables that you can check out. Acknowledgements If you find QIT is valuable in your work, we ask that you clearly acknowledge it any publications or grant proposals. This is greatly appreciated, as it improves the reproducibility of your findings, and it helps our team maintain resources for continued support and development. You can cite QIT by including a link to the main website at http://cabeen.io/qitwiki and including the following citation in the manuscript: Cabeen RP, Laidlaw DH, Toga AW. 2018. Quantitative Imaging Toolkit: Software for Interactive 3D Visualization, Data Exploration, and Computational Analysis of Neuroimaging Datasets. Proceedings of the Annual Meeting of the Society for Magnetic Resonance in Medicine (ISMRM). Paris, France 2018 (p. 2854) If you use the region-of-interest analysis, you can include the following reference, which describes and evaluates these components: Cabeen, R.P., Bastin, M.E. and Laidlaw, D.H., 2017. A Comparative evaluation of voxel-based spatial mapping in diffusion tensor imaging. Neuroimage, 146, pp.100-112. If you used the tractography-based analysis, you can include the following references, which describe and evaluate the methods for tractography and template construction: Cabeen, R.P., Bastin, M.E. and Laidlaw, D.H., 2016. Kernel regression estimation of fiber orientation mixtures in diffusion MRI. Neuroimage, 127, pp.158-172. Cabeen, R.P., Toga, A.W., 2020. Reinforcement tractography: a hybrid approach for robust segmentation of complex fiber bundles. International Symposium on Biomedical Imaging (ISBI) 2020 Besides QIT references, you should also please cite the other dependent tools and resources where appropriate: Jenkinson, M., Beckmann, C.F., Behrens, T.E., Woolrich, M.W. and Smith, S.M., 2012. Fsl. Neuroimage, 62(2), pp.782-790. Behrens, T.E., Berg, H.J., Jbabdi, S., Rushworth, M.F. and Woolrich, M.W., 2007. Probabilistic diffusion tractography with multiple fibre orientations: What can we gain?. Neuroimage, 34(1), pp.144-155. Zhang, H., Yushkevich, P.A., Alexander, D.C. and Gee, J.C., 2006. Deformable registration of diffusion tensor MR images with explicit orientation optimization. Medical image analysis, 10(5), pp.764-785. Zhang, S., Peng, H., Dawe, R.J. and Arfanakis, K., 2011. Enhanced ICBM diffusion tensor template of the human brain. Neuroimage, 54(2), pp.974-984. Zhang, H., Yushkevich, P.A., Alexander, D.C. and Gee, J.C., 2006. Deformable registration of diffusion tensor MR images with explicit orientation optimization. Medical image analysis, 10(5), pp.764-785. Basser, P.J., Mattiello, J. and LeBihan, D., 1994. MR diffusion tensor spectroscopy and imaging. Biophysical journal, 66(1), pp.259-267. Zhang, H., Schneider, T., Wheeler-Kingshott, C.A. and Alexander, D.C., 2012. NODDI: practical in vivo neurite orientation dispersion and density imaging of the human brain. Neuroimage, 61(4), pp.1000-1016. Hoy, A.R., Koay, C.G., Kecskemeti, S.R. and Alexander, A.L., 2014. Optimization of a free water elimination two-compartment model for diffusion tensor imaging. Neuroimage, 103, pp.323-333. Kaden, E., Kelm, N.D., Carson, R.P., Does, M.D. and Alexander, D.C., 2016. Multi-compartment microscopic diffusion imaging. NeuroImage, 139, pp.346-359. Fieremans, E., Jensen, J. H., & Helpern, J. A. (2011). White matter characterization with diffusional kurtosis imaging. Neuroimage, 58(1), 177-188. Sepehrband, F., Cabeen, R.P., Choupan, J., Barisano, G., Law, M., Toga, A.W. and Alzheimer's Disease Neuroimaging Initiative, 2019. Perivascular space fluid contributes to diffusion tensor imaging changes in white matter. NeuroImage, 197, pp.243-254. Mori, S., Oishi, K., Jiang, H., Jiang, L., Li, X., Akhter, K., Hua, K., Faria, A.V., Mahmood, A., Woods, R. and Toga, A.W., 2008. Stereotaxic white matter atlas based on diffusion tensor imaging in an ICBM template. Neuroimage, 40(2), pp.570-582. Smith, S.M., Jenkinson, M., Johansen-Berg, H., Rueckert, D., Nichols, T.E., Mackay, C.E., Watkins, K.E., Ciccarelli, O., Cader, M.Z., Matthews, P.M. and Behrens, T.E., 2006. Tract-based spatial statistics: voxelwise analysis of multi-subject diffusion data. Neuroimage, 31(4), pp.1487-1505. Pauli, W.M., Nili, A.N. and Tyszka, J.M., 2018. A high-resolution probabilistic in vivo atlas of human subcortical brain nuclei. Scientific data, 5, p.180063. Tyszka, J.M. and Pauli, W.M., 2016. In vivo delineation of subdivisions of the human amygdaloid complex in a high\u2010resolution group template. Human brain mapping, 37(11), pp.3979-3998. Tang, Y., Sun, W., Toga, A.W., Ringman, J.M. and Shi, Y., 2018. A probabilistic atlas of human brainstem pathways based on connectome imaging data. Neuroimage, 169, pp.227-239. Desikan, R.S., S\u00e9gonne, F., Fischl, B., Quinn, B.T., Dickerson, B.C., Blacker, D., Buckner, R.L., Dale, A.M., Maguire, R.P., Hyman, B.T. and Albert, M.S., 2006. An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest. Neuroimage, 31(3), pp.968-980. Fischl, B., 2012. FreeSurfer. Neuroimage, 62(2), pp.774-781. Avants, B.B., Tustison, N. and Song, G., 2009. Advanced normalization tools (ANTS). Insight j, 2(365), pp.1-35. Closing This concludes the tutorial for the QIT diffusion workflow. This was written to provide only a brief overview, so you may be left wondering about how some things work. If so, please feel free to reach out with any questions or thoughts via cabeen@gmail.com .","title":"Workflow Tutorial"},{"location":"workflow/#automated-image-analysis-with-qit","text":"Welcome! This is a guide for automated quantitative analysis of diffusion MRI data using QIT. If you have collected many diffusion MRI scans in an imaging study, this workflow can help you extract quantitative measures of structural anatomical characteristics across your group and ultimately use these measures to explore their statistical relationship with demographic and behavioral variables.","title":"Automated image analysis with QIT"},{"location":"workflow/#overview","text":"The workflow is an end-to-end tool that supports a variety of ways to analyze diffusion MRI data, in which you start from raw imaging data and produce geometric models and quantitative summaries of tissue microstructure, morphometry, and connectivity. Specifically, you can perform region-based and tractography-based analysis and combine these with tissue parameter mapping using diffusion tensor imaging and advanced multi-shell approaches. In addition to diffusion MRI, the workflow has components that streamline the use of state-of-the-art packages for morphometric analysis using T1-weighted MRI data, and further, allow them to be combined with diffusion MRI data. If you are new to diffusion MRI and would like to learn more, Diffusion Tensor Imaging: a Practical Handbook and NMR in Biomedicine: Special Issue on Diffusion MRI of the brain are both good starting points. In the following sections, this guide will cover how to install the necessary software on your computer, how to prepare your imaging data, how to run various analyses, and how to combine the results across subjects.","title":"Overview"},{"location":"workflow/#installing-software","text":"This section will guide you through the installation of QIT and its dependencies. The workflow is designed for the command line, and if you need need to learn about it, you can check out the tutorials at Software Carpentry . Note: while the workflow primarily uses QIT, it also depends on several other 3rd party tools that are designed to run only on Linux and Mac. So while QIT itself can run on Windows, to run the complete workflow on Windows you would need a virtualization solution, such as VMware , VirtualBox , or the Windows Subsystem for Linux .","title":"Installing software"},{"location":"workflow/#installing-qit","text":"You should first follow the instructions on the Installation page, and make sure that the bin directory is included in your shell PATH variable. You can read [https://linuxize.com/post/how-to-add-directory-to-path-in-linux/ this page] to learn more about adding a directory to your path. After installing, you can check that things worked by running the following command and seeing if it produces something analogous (your version should be this one or later): $ qit --version <br> QIT build revision: 2005\\:2019, build time: 03/25/2020 05\\:09 PM`","title":"Installing QIT"},{"location":"workflow/#installing-dependencies","text":"Next, you should install the other software packages required by the workflow, which are two common and publicly available 3rd party neuroimaging tools: DTI-TK and FSL . You can follow the installation instructions on the linked pages, and afterwards, you can test that they are correctly installed by trying these commands: $ which bedpostx /opt/share/fsl-5.0.10/bin/bedpostx $ which TVMean /opt/share/dtitk/bin/TVMean Note: there is a GPU version of FSL bedpostx that can dramatically speed things up. If you install it using the instructions on the linked page, you can enable it in the QIT workflow by adding GPU=1 to the qitdiff command.","title":"Installing dependencies"},{"location":"workflow/#installing-more-dependencies-optional","text":"As an optional step, if want to use T1 MRI data in the analysis, you should also install [https://surfer.nmr.mgh.harvard.edu/ FreeSurfer] and [https://stnava.github.io/ANTs/ ANTs]. These are not required for most parts of the pipeline, but they can offer more precise segmentation of certain brain structures, which are described in more detail later in the guide. You can follow the installation instructions on the linked pages, and you can test that they are correctly installed by trying these commands: $ which recon-all /opt/share/freesurfer/bin/recon-all $ which antsRegistrationSyn.sh /opt/share/ANTs-install/bin/antsRegistrationSyn.sh","title":"Installing more dependencies (optional)"},{"location":"workflow/#preparing-your-data","text":"This section provides instructions for converting your data into a format that is compatible with the workflow. You may already have data in a suitable format, so not all of these steps will be necessary. The workflow expects the input to be a diffusion-weighted MRI in the NIfTI and associated text files for the b-vectors and b-values. If you need data to try out, you can download this example clinical quality single shell dataset: http://cabeen.io/download/dmri.tutorial.data.zip If you\u2019d like to skip the processing, you can also check out these example results: http://cabeen.io/download/qitdiff.demo.tar.gz","title":"Preparing your data"},{"location":"workflow/#file-format-conversion","text":"Data from the scanner typically is stored in the DICOM file format , which stores the all data from each session in a directory that combines imaging, sequence parameters, and patient information across many files contained within. You will need to convert DICOM data to the NIfTI file format , a simpler format that is more amenable to imaging research. There are a few tools available for converting between NIfTI and DICOM, but a good starting point is the dcm2niix tool. The linked page shares instructions for installing the software and applying it to your DICOM data to produce NIfTI volumes. Afterwards, you should be left with nii.gz files from your session, one of which will be a multi-channel diffusion MRI. You should also find bvecs and bvals files, which list the b-vectors and b-values sequence parameters from the scan, respectively.","title":"File format conversion"},{"location":"workflow/#artifact-correction","text":"There are a variety of artifact that can be corrected in diffusion MRI, and it is recommended that you apply any corrections that are compatible with your data collection scheme. QIT offers some basic corrections for denoising, signal drift correction, and motion correction, and these will be noted in the next section. However, if possible, it is recommended that you use advanced preprocessing tools when possible. In particular, if you have collected data with an additional set of reversed-phase encoding scans, you can use FSL EDDY to greatly improve the quality of your data by simultaneously correcting for motion, eddy-current, susceptibility-induced geometric distortion. You should explore using an OpenMP accelerated version, as it can be quite time consuming. To apply it to your data, you can use the FSL or the HCP pipelines . Once you\u2019ve finished processing it, you should find the corrected diffusion MRI and b-vectors, which will be named eddy_out.nii.gz and eddy_out.eddy_rotated_bvecs , respectively. The b-values are not changed by the correction, so you should use the raw values. If you are interested in other artifact correction features of QIT, you can check out these modules: VolumeFilterNLM , VolumeDwiCorrectDrift , and VolumeDwiCorrect , which provide denoising, drift correction, and motion correction, respectively.","title":"Artifact Correction"},{"location":"workflow/#general-considerations","text":"In this section and the following three, we describe the different ways that you can run the QIT diffusion workflow. We\u2019ll start by discussing the general interface to the script, and then describe specific types of analysis in detail. The primary interface to the QIT diffusion workflow is a program named qitdiff . If you run this command without any arguments, you\u2019ll find a usage page, like so: $ qitdiff Name: qitdiff Description: Process diffusion-weighted imaging data using the qit diffusion workflow. The first time you run this script, you must provide the DWI, bvecs, and bvals for your subject. After that, you only need to specify the targets. Usage: qitdiff [opts] --subject subject [targets] Input Parameters (must be include in the first run, but not afterwards): --dwi dwi.nii.gz: the diffusion weighted MRI file, stored in NIFTI --bvecs bvecs: the diffusion b-vectors file, stored as text --bvals bvals: the diffusion b-values file, stored as text --subject subject: the subject directory where the results will be saved Optional Parameters (may be included in the first run, but not afterwards): --denoise <num>: denoise the dwi with the given noise level --mask <fn>: use the given precomputed brain mask --bet <num>: use the given FSL BET fraction parameter --erode <num>: erode the brain mask by the given number of voxels --motion: correct for motion and create a summary report --nomatch: skip gradient matching step --tone <fn>: specify a T1-weighted MRI --freesurfer <dir>: use the given precomputed freesurfer results --tracts <dir>: include user-defined tracts in the workflow Author: Ryan Cabeen There are three flags for specifying the input to the workflow: --dwi , --bvecs , and --bvals . The b-vectors and b-values can be organized in column or row form, and QIT will automatically determine the coordinate transform necessary to align the b-vectors to the imaging data (so there is no need to flip or transpose b-vectors). You must also specify a subject directory with --subject , but if you omit this flag, the current working directory will be used. There are several other optional flags that are listed after the required flags. You may skip these for now if you like. Besides the input data, you must also specify one or more targets . A target is a string that identifies what you want the workflow to produce, e.g. regional summaries of FA or fiber bundle geometry. In the next section of the tutorial we\u2019ll describe the different targets that are available. But to give a concrete example, you could produce region DTI parameter statistics with the JHU atlas using a command like so: $ qitdiff --subject qitsubject --dwi scan/dwi.nii.gz --bvecs scan/bvecs.txt --bvals scan/bvals.txt diff.region/jhu.labels.dti.map One of the features of the QIT workflow is that you can add additional analyses and re-use the previous work you did. So for example, if you wanted to add TBSS to your analysis, you could simply run a command like this: $ qitdiff --subject qitsubject atlas.region/jhu.labels.tbss.dti.map For example, this will skip the data import and atlas registration steps, as they were already completed. Note that you can also omit the flags for the input data, since they have already been imported.","title":"General considerations"},{"location":"workflow/#region-of-interest-analysis","text":"Region-of-interest (ROI) analysis is a simple and widely used way to summarize diffusion MRI data. In this approach, the data is spatially normalized with an atlas that contains a parcellation of brain areas, stored in a 3D mask. Then, diffusion model parameters are statistically summarized in each ROI of the atlas; for example, by computing the average fractional anisotropy in each brain area. There are quite a few variations on this approach, and QIT supports many of them. For example, you can combine the ROI approach with Tract-based Spatial Statistics (TBSS) , you can apply erosion of the masks, and you can compute the statistics in either native or atlas space. Each of these possibilities is available with a different target that you can provide to qitdiff . Here is a table summarizing the different kinds of ROI targets that are available: diff.region/jhu.labels.dti.map : DTI parameters statistics for each region in native space diff.region/jhu.labels.erode.dti.map : DTI parameters statistics for each region in native space with additional erosion step atlas.region/jhu.labels.dti.map : DTI parameters statistics for each region in atlas space atlas.region/jhu.labels.erode.dti.map : DTI parameters statistics for each region in atlas space with an additional erosion step atlas.region/jhu.labels.tbss.dti.map : DTI parameters statistics for each region in atlas space with additional TBSS processing The above examples use the Johns Hopkins University white matter atlas, but there are many other alternatives in qitdiff . Below is a table summarizing these other options. There are targets available for these, in which you simply replace jhu.labels in the targets above. jhu.labels : Johns Hopkins University deep white matter atlas) jhu.tracts : Johns Hopkins University white matter tract atlas fsa.ccwm : FreeSurfer-based corpus callosum parcellation fsa.scgm : FreeSurfer-based subcortical gray matter fsa.dkwm : FreeSurfer-based superficial white matter hox.sub : Harvard-Oxford subcortical parcellation ini.bstem : USC INI brainstem parcellation cit.amy : Caltech amygdala subfields cit.sub : Caltech subcortical parcellation Furthermore, there are many types of diffusion parameters that can be used in an ROI analysis. The examples listed above used diffusion tensor imaging (DTI) parameters, but if you have multi-shell data, there are other possibilities supported by qitdiff . The table below lists these other possibilities, and you can create targets for them by substituting dti with the appropriate model identifier. dti : Diffusion Tensor Imaging (DTI) fwdti : Free-water Elimination DTI (with a fixed diffusivity ball) dki : Diffusion Kurtosis Imaging noddi : Neurite Orientation Dispersion and Density Imaging mcsmt : Multi-compartment Spherical Mean Technique bti : Bi-tensor Imaging (DTI with an unconstrained ball) For a concrete example, if you wanted to compute NODDI parameters in subcortical brain areas, you could run something like so: $ qitdiff --subject qitsubject --dwi scan/dwi.nii.gz --bvecs scan/bvecs.txt --bvals scan/bvals.txt diff.region/fsa.scgm.noddi.map With all of these options, you may be wondering what to pick, and in this case, a good starting point is the target atlas.region/jhu.region.tbss.dti.map . This is a fairly standardized approach that summarizes DTI parameters in white matter areas from the JHU atlas with preprocessing using TBSS. For example, this is similar to the protocol used in the ENIGMA network (note: the template and registration algorithm are not identical).","title":"Region-of-interest analysis"},{"location":"workflow/#tractography-based-analysis","text":"Tractography analysis is an alternative approach for diffusion MRI analysis that is focused on modeling connectivity as opposed to regions. In this approach, geometric models of white matter connectivity are reconstructed from fiber orientation data estimated from diffusion MRI. With sufficiently high quality data, this approach can extract the major pathways of the brain, known as fiber bundles. The QIT diffusion workflow includes such a bundle-specific analysis, enabling the quantitative characterization of both whole bundles and subdivisions along their length. These can be run using the following qitdiff targets: diff.tract/bundles.map : whole bundle parameters, such as volume, length, etc. diff.tract/bundles.dti.map : whole bundle DTI parameters, such as FA, MD, etc. diff.tract/bundles.along.dti.map : along-bundle DTI parameters, e.g. FA measured at each of a sequence of bundle subdivisions Similar to ROI analysis, you can also extract multi-shell diffusion MRI parameters using any of the model identifiers from the previous section. For a concrete example, if you wanted to compute NODDI parameters along each bundle, you could run something like so: $ qitdiff --subject qitsubject --dwi scan/dwi.nii.gz --bvecs scan/bvecs.txt --bvals scan/bvals.txt diff.tract/bundles.along.noddi.map","title":"Tractography-based analysis"},{"location":"workflow/#multi-modal-analysis","text":"Besides diffusion MRI, the QIT workflow has integrated morphometric analysis using T1-weighted MRI. These steps require ANTs and FreeSurfer to be installed (as described above). After that, you can perform cortical surface based analysis using FreeSurfer using the following command: $ qitdiff --subject qitsubject --tone t1.nii.gz tone.fs.map If you\u2019ve already run FreeSurfer, you can also import the previous results, e.g. $ qitdiff --subject qitsubject --freesurfer fs_subject_dir tone.fs.map You can also perform a multi-modal image analysis that combines the T1-weighted and diffusion MRI data. For example, you could compute DTI parameter statistics in subject-specific subcortical and superficial white matter ROIs using this command: $ qitdiff --subject qitsubject --dwi scan/dwi.nii.gz --bvecs scan/bvecs.txt --bvals scan/bvals.txt --tone t1.nii.gz tone.region/fs.scgm.dti.map tone.region/fs.dkwm.dti.map There are various other T1-related targets available, which are summarized here: tone.fs.brain : Freesurfer brain image and tissue types converted into nifti tone.fs.region : Freesurfer regions-of-interest converted into nifti tone.fs.surfaces : Freesurfer meshes converted into VTK diff.fs.brain : Freesurfer brain image and tissue types deformed into native diffusion space diff.fs.region : Freesurfer regions-of-interest deformed into native diffusion space diff.fs.surfaces : Freesurfer meshes deformed into native diffusion space tone.region/fs.scgm.dti.map : DTI parameters statistics for each Freesurfer subcortical ROI tone.region/fs.dkwm.dti.map : DTI parameters statistics for each Freesurfer Desikan white matter ROI tone.region/fs.dkgm.dti.map : DTI parameters statistics for each Freesurfer Desikan gray matter ROI","title":"Multi-modal analysis"},{"location":"workflow/#aggregating-results","text":"Once you\u2019ve finished processing your data, you may want to combine the results into data tables, that is, aggregating metrics from all of the research subjects into a spreadsheet. The QIT workflow was designed to make this easy by saving results in a standardized format. You may notice that each target ends in map ; this indicates that the target is a directory that contains summary statistics. Each map directory contains an array of CSV files store simple name-value pairs. For example, the bundle-specific analysis produces this CSV file among many others: $ head diff.tract/bundles.map/volume.csv name,value lh_acoustic,13570.0 lh_thal_tempinf,9663.0 rh_arc_ant,51518.0 rh_thal_pma,21428.0 rh_med_lem,11603.0 rh_aft,6183.0 ... Suppose you have many subjects, and you have organized the data such that each subject has a QIT workflow subject directory, e.g. test_103818 , inside a directory named subjects . Further, you have a list of subject identifiers that correspond to the subject directory names, and these names are stored one-per-line in a text file named sids.txt . Then, you can create a data table and inspect the output using these commands: $ qit --verbose MapCat --pattern subjects/%{subject}/diff.tract/bundles.map/volume.csv --vars subject=sids.txt --skip --output bundles.volume.csv $ cat bundles.volume.csv | grep lh_acoustic | head test_103818,lh_acoustic,13386.0 test_105923,lh_acoustic,17255.0 test_111312,lh_acoustic,12189.0 test_114823,lh_acoustic,15710.0 test_115320,lh_acoustic,14599.0 ... The resulting table is in a long format, but you can also convert it to a wide one: $ qit TableWiden --input bundles.volume.csv --output bundles.volume.wide.csv $ head bundles.volume.wide.csv subject,lh_acoustic,lh_arc,... test_103818,13386.0,8950.0,... test_105923,17255.0,13213.0,... test_111312,12189.0,11033.0,... ... In this table, each column is a different brain area, and each row is a subject. We used bundle volume in the example, but the same approach works for all of the other metrics stored in map directories. If you would like to create tables for many different variables, there is a more complex but powerful script called qitmaketables that you can check out.","title":"Aggregating results"},{"location":"workflow/#acknowledgements","text":"If you find QIT is valuable in your work, we ask that you clearly acknowledge it any publications or grant proposals. This is greatly appreciated, as it improves the reproducibility of your findings, and it helps our team maintain resources for continued support and development. You can cite QIT by including a link to the main website at http://cabeen.io/qitwiki and including the following citation in the manuscript: Cabeen RP, Laidlaw DH, Toga AW. 2018. Quantitative Imaging Toolkit: Software for Interactive 3D Visualization, Data Exploration, and Computational Analysis of Neuroimaging Datasets. Proceedings of the Annual Meeting of the Society for Magnetic Resonance in Medicine (ISMRM). Paris, France 2018 (p. 2854) If you use the region-of-interest analysis, you can include the following reference, which describes and evaluates these components: Cabeen, R.P., Bastin, M.E. and Laidlaw, D.H., 2017. A Comparative evaluation of voxel-based spatial mapping in diffusion tensor imaging. Neuroimage, 146, pp.100-112. If you used the tractography-based analysis, you can include the following references, which describe and evaluate the methods for tractography and template construction: Cabeen, R.P., Bastin, M.E. and Laidlaw, D.H., 2016. Kernel regression estimation of fiber orientation mixtures in diffusion MRI. Neuroimage, 127, pp.158-172. Cabeen, R.P., Toga, A.W., 2020. Reinforcement tractography: a hybrid approach for robust segmentation of complex fiber bundles. International Symposium on Biomedical Imaging (ISBI) 2020 Besides QIT references, you should also please cite the other dependent tools and resources where appropriate: Jenkinson, M., Beckmann, C.F., Behrens, T.E., Woolrich, M.W. and Smith, S.M., 2012. Fsl. Neuroimage, 62(2), pp.782-790. Behrens, T.E., Berg, H.J., Jbabdi, S., Rushworth, M.F. and Woolrich, M.W., 2007. Probabilistic diffusion tractography with multiple fibre orientations: What can we gain?. Neuroimage, 34(1), pp.144-155. Zhang, H., Yushkevich, P.A., Alexander, D.C. and Gee, J.C., 2006. Deformable registration of diffusion tensor MR images with explicit orientation optimization. Medical image analysis, 10(5), pp.764-785. Zhang, S., Peng, H., Dawe, R.J. and Arfanakis, K., 2011. Enhanced ICBM diffusion tensor template of the human brain. Neuroimage, 54(2), pp.974-984. Zhang, H., Yushkevich, P.A., Alexander, D.C. and Gee, J.C., 2006. Deformable registration of diffusion tensor MR images with explicit orientation optimization. Medical image analysis, 10(5), pp.764-785. Basser, P.J., Mattiello, J. and LeBihan, D., 1994. MR diffusion tensor spectroscopy and imaging. Biophysical journal, 66(1), pp.259-267. Zhang, H., Schneider, T., Wheeler-Kingshott, C.A. and Alexander, D.C., 2012. NODDI: practical in vivo neurite orientation dispersion and density imaging of the human brain. Neuroimage, 61(4), pp.1000-1016. Hoy, A.R., Koay, C.G., Kecskemeti, S.R. and Alexander, A.L., 2014. Optimization of a free water elimination two-compartment model for diffusion tensor imaging. Neuroimage, 103, pp.323-333. Kaden, E., Kelm, N.D., Carson, R.P., Does, M.D. and Alexander, D.C., 2016. Multi-compartment microscopic diffusion imaging. NeuroImage, 139, pp.346-359. Fieremans, E., Jensen, J. H., & Helpern, J. A. (2011). White matter characterization with diffusional kurtosis imaging. Neuroimage, 58(1), 177-188. Sepehrband, F., Cabeen, R.P., Choupan, J., Barisano, G., Law, M., Toga, A.W. and Alzheimer's Disease Neuroimaging Initiative, 2019. Perivascular space fluid contributes to diffusion tensor imaging changes in white matter. NeuroImage, 197, pp.243-254. Mori, S., Oishi, K., Jiang, H., Jiang, L., Li, X., Akhter, K., Hua, K., Faria, A.V., Mahmood, A., Woods, R. and Toga, A.W., 2008. Stereotaxic white matter atlas based on diffusion tensor imaging in an ICBM template. Neuroimage, 40(2), pp.570-582. Smith, S.M., Jenkinson, M., Johansen-Berg, H., Rueckert, D., Nichols, T.E., Mackay, C.E., Watkins, K.E., Ciccarelli, O., Cader, M.Z., Matthews, P.M. and Behrens, T.E., 2006. Tract-based spatial statistics: voxelwise analysis of multi-subject diffusion data. Neuroimage, 31(4), pp.1487-1505. Pauli, W.M., Nili, A.N. and Tyszka, J.M., 2018. A high-resolution probabilistic in vivo atlas of human subcortical brain nuclei. Scientific data, 5, p.180063. Tyszka, J.M. and Pauli, W.M., 2016. In vivo delineation of subdivisions of the human amygdaloid complex in a high\u2010resolution group template. Human brain mapping, 37(11), pp.3979-3998. Tang, Y., Sun, W., Toga, A.W., Ringman, J.M. and Shi, Y., 2018. A probabilistic atlas of human brainstem pathways based on connectome imaging data. Neuroimage, 169, pp.227-239. Desikan, R.S., S\u00e9gonne, F., Fischl, B., Quinn, B.T., Dickerson, B.C., Blacker, D., Buckner, R.L., Dale, A.M., Maguire, R.P., Hyman, B.T. and Albert, M.S., 2006. An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest. Neuroimage, 31(3), pp.968-980. Fischl, B., 2012. FreeSurfer. Neuroimage, 62(2), pp.774-781. Avants, B.B., Tustison, N. and Song, G., 2009. Advanced normalization tools (ANTS). Insight j, 2(365), pp.1-35.","title":"Acknowledgements"},{"location":"workflow/#closing","text":"This concludes the tutorial for the QIT diffusion workflow. This was written to provide only a brief overview, so you may be left wondering about how some things work. If so, please feel free to reach out with any questions or thoughts via cabeen@gmail.com .","title":"Closing"}]}