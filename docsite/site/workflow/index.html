<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Workflow Tutorial - Quantitative Imaging Toolkit</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Workflow Tutorial";
    var mkdocs_page_input_path = "workflow.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-58725032-1', 'auto');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Quantitative Imaging Toolkit</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Usage</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../install/">Installation</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../citation/">Citation</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../license/">License</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../support/">Support</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../quickstart/">Quickstart</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Concepts</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../gallery/">Gallery</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../interaction/">Interaction</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../colormaps/">Colormaps</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../datasets/">Datasets</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../modules/">Modules</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../models/">Models</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Tutorials</span></p>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Workflow Tutorial</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#installing-software">Installing software</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#installing-qit">Installing QIT</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#installing-dependencies">Installing dependencies</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#installing-more-dependencies-optional">Installing more dependencies (optional)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#preparing-your-data">Preparing your data</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#file-format-conversion">File format conversion</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#artifact-correction">Artifact Correction</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#general-considerations">General considerations</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#region-of-interest-analysis">Region-of-interest analysis</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#tractography-based-analysis">Tractography-based analysis</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#multi-modal-analysis">Multi-modal analysis</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#aggregating-results">Aggregating results</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#acknowledgements">Acknowledgements</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#closing">Closing</a>
    </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../masters-workshop/">Masters Workshop</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../dmri-tutorial/">Diffusion MRI Tutorial</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../tractography/">Tractography Tutorial</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../freesurfer/">Freesurfer Tutorial</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Quantitative Imaging Toolkit</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Tutorials &raquo;</li>
        
      
    
    <li>Workflow Tutorial</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/cabeen/qit/edit/master/docs/workflow.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="automated-image-analysis-with-qit">Automated image analysis with QIT</h1>
<p>Welcome! This is a guide for automated quantitative analysis of diffusion MRI
data using QIT.   If you have collected many diffusion MRI scans in an imaging
study, this workflow can help you extract quantitative measures of structural
anatomical characteristics across your group and ultimately use these measures
to explore their statistical relationship with demographic and behavioral
variables.  </p>
<h2 id="overview">Overview</h2>
<p>The workflow is an end-to-end tool that supports a variety of ways to analyze
diffusion MRI data, in which you start from raw imaging data and produce
geometric models and quantitative summaries of tissue microstructure,
morphometry, and connectivity.  Specifically, you can perform region-based and
tractography-based analysis and combine these with tissue parameter mapping
using diffusion tensor imaging and advanced multi-shell approaches.  In
addition to diffusion MRI, the workflow has components that streamline the use
of state-of-the-art packages for morphometric analysis using T1-weighted MRI
data, and further, allow them to be combined with diffusion MRI data.  If you
are new to diffusion MRI and would like to learn more, <a href="https://www.springer.com/gp/book/9781493931170">Diffusion Tensor
Imaging: a Practical Handbook</a>
and <a href="https://onlinelibrary.wiley.com/toc/10991492/2019/32/4">NMR in Biomedicine: Special Issue on Diffusion MRI of the
brain</a> are both good
starting points.</p>
<p>In the following sections, this guide will cover how to install the necessary
software on your computer, how to prepare your imaging data, how to run various
analyses, and how to combine the results across subjects.</p>
<h2 id="installing-software">Installing software</h2>
<p>This section will guide you through the installation of QIT and its
dependencies.  The workflow is designed for the command line, and if you need
need to learn about it, you can check out the tutorials at <a href="https://swcarpentry.github.io/shell-novice/">Software
Carpentry</a>.   Note: while the
workflow primarily uses QIT, it also depends on several other 3rd party tools
that are designed to run only on Linux and Mac.  So while QIT itself can run on
Windows, to run the complete workflow on Windows you would need a
virtualization solution, such as
<a href="https://www.vmware.com/products.html">VMware</a>,
<a href="https://www.virtualbox.org">VirtualBox</a>, or the <a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">Windows Subsystem for
Linux</a>.</p>
<h3 id="installing-qit">Installing QIT</h3>
<p>You should first follow the instructions on the <a href="../install/">Installation</a> page, and make sure that the <code>bin</code> directory is included in your shell <code>PATH</code> variable.  You can read [https://linuxize.com/post/how-to-add-directory-to-path-in-linux/ this page] to learn more about adding a directory to your path.  After installing, you can check that things worked by running the following command and seeing if it produces something analogous (your version should be this one or later):</p>
<pre><code>$ qit --version &lt;br&gt;
QIT build revision: 2005\:2019, build time: 03/25/2020 05\:09 PM`
</code></pre>
<h3 id="installing-dependencies">Installing dependencies</h3>
<p>Next, you should install the other software packages required by the workflow,
which are two common and publicly available 3rd party neuroimaging tools:
<a href="http://dti-tk.sourceforge.net/pmwiki/pmwiki.php">DTI-TK</a> and
<a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki">FSL</a>.  You can follow the installation
instructions on the linked pages, and afterwards, you can test that they are
correctly installed by trying these commands:</p>
<pre><code>$ which bedpostx
/opt/share/fsl-5.0.10/bin/bedpostx
</code></pre>
<pre><code>$ which TVMean
/opt/share/dtitk/bin/TVMean
</code></pre>
<p>Note: there is a <a href="https://users.fmrib.ox.ac.uk/~moisesf/Bedpostx_GPU/">GPU version of FSL
bedpostx</a> that can
dramatically speed things up.  If you install it using the instructions on the
linked page, you can enable it in the QIT workflow by adding <code>GPU=1</code> to the
<code>qitdiff</code> command.</p>
<h3 id="installing-more-dependencies-optional">Installing more dependencies (optional)</h3>
<p>As an optional step, if want to use T1 MRI data in the analysis, you should also install [https://surfer.nmr.mgh.harvard.edu/ FreeSurfer] and [https://stnava.github.io/ANTs/ ANTs].  These are not required for most parts of the pipeline, but they can offer more precise segmentation of certain brain structures, which are described in more detail later in the guide.  You can follow the installation instructions on the linked pages, and you can test that they are correctly installed by trying these commands:</p>
<pre><code>$ which recon-all
/opt/share/freesurfer/bin/recon-all
</code></pre>
<pre><code>$ which antsRegistrationSyn.sh
/opt/share/ANTs-install/bin/antsRegistrationSyn.sh
</code></pre>
<h2 id="preparing-your-data">Preparing your data</h2>
<p>This section provides instructions for converting your data into a format that
is compatible with the workflow.  You may already have data in a suitable
format, so not all of these steps will be necessary.  The workflow expects the
input to be a diffusion-weighted MRI in the
<a href="https://nifti.nimh.nih.gov/">NIfTI</a> and associated text files for the
b-vectors and b-values.  If you need data to try out, you can download this example clinical quality single shell dataset:</p>
<p><a href="http://cabeen.io/download/dmri.tutorial.data.zip"><img alt="Download" src="../images/download-icon.png" style="height:25px;width:25px" /> http://cabeen.io/download/dmri.tutorial.data.zip</a></p>
<p>If you&rsquo;d like to skip the processing, you can also check out these example
results:</p>
<p><a href="http://cabeen.io/download/qitdiff.demo.tar.gz"><img alt="Download" src="../images/download-icon.png" style="height:25px;width:25px" />
http://cabeen.io/download/qitdiff.demo.tar.gz</a></p>
<h3 id="file-format-conversion">File format conversion</h3>
<p>Data from the scanner typically is stored in the <a href="https://www.dicomstandard.org/">DICOM file
format</a>, which stores the all data from each
session in a directory that combines imaging, sequence parameters, and patient
information across many files contained within.  You will need to convert DICOM
data to the <a href="https://nifti.nimh.nih.gov/">NIfTI file format</a>, a simpler format
that is more amenable to imaging research.  There are a few tools available for
converting between NIfTI and DICOM, but a good starting point is the
<a href="https://github.com/rordenlab/dcm2niix">dcm2niix</a> tool.  The linked page shares
instructions for installing the software and applying it to your DICOM data to
produce NIfTI volumes.  Afterwards, you should be left with <code>nii.gz</code> files from
your session, one of which will be a multi-channel diffusion MRI.  You should
also find <code>bvecs</code> and <code>bvals</code> files, which list the b-vectors and b-values
sequence parameters from the scan, respectively.</p>
<h3 id="artifact-correction">Artifact Correction</h3>
<p>There are a variety of artifact that can be corrected in diffusion MRI, and it
is recommended that you apply any corrections that are compatible with your
data collection scheme.  QIT offers some basic corrections for denoising,
signal drift correction, and motion correction, and these will be noted in the
next section.   However, if possible, it is recommended that you use advanced
preprocessing tools when possible.  In particular, if you have collected data
with an additional set of reversed-phase encoding scans, you can use <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/eddy">FSL
EDDY</a> to greatly improve the
quality of your data by simultaneously correcting for motion, eddy-current,
susceptibility-induced geometric distortion.  You should explore using an
OpenMP accelerated version, as it can be quite time consuming. To apply it to
your data, you can use the <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/eddy">FSL</a>
or the <a href="https://www.humanconnectome.org/software/hcp-mr-pipelines">HCP
pipelines</a>.  Once
you&rsquo;ve finished processing it, you should find the corrected diffusion MRI and
b-vectors, which will be named <code>eddy_out.nii.gz</code> and
<code>eddy_out.eddy_rotated_bvecs</code>, respectively.  The b-values are not changed by
the correction, so you should use the raw values.</p>
<p>If you are interested in other artifact correction features of QIT, you can
check out these modules: <code>VolumeFilterNLM</code>, <code>VolumeDwiCorrectDrift</code>, and
<code>VolumeDwiCorrect</code>, which provide denoising, drift correction, and motion
correction, respectively.</p>
<h2 id="general-considerations">General considerations</h2>
<p>In this section and the following three, we describe the different ways that you can run the QIT diffusion workflow.  We&rsquo;ll start by discussing the general interface to the script, and then describe specific types of analysis in detail.  The primary interface to the QIT diffusion workflow is a program named <code>qitdiff</code>.  If you run this command without any arguments, you&rsquo;ll find a usage page, like so:</p>
<pre><code>$ qitdiff

  Name: qitdiff

  Description:

    Process diffusion-weighted imaging data using the qit diffusion workflow.
    The first time you run this script, you must provide the DWI, bvecs, and
    bvals for your subject.  After that, you only need to specify the targets. 

  Usage: 

    qitdiff [opts] --subject subject [targets]

  Input Parameters (must be include in the first run, but not afterwards):

     --dwi dwi.nii.gz:   the diffusion weighted MRI file, stored in NIFTI
     --bvecs bvecs:      the diffusion b-vectors file, stored as text
     --bvals bvals:      the diffusion b-values file, stored as text
     --subject subject:  the subject directory where the results will be saved

  Optional Parameters (may be included in the first run, but not afterwards):

     --denoise &lt;num&gt;:    denoise the dwi with the given noise level
     --mask &lt;fn&gt;:        use the given precomputed brain mask
     --bet &lt;num&gt;:        use the given FSL BET fraction parameter 
     --erode &lt;num&gt;:      erode the brain mask by the given number of voxels
     --motion:           correct for motion and create a summary report
     --nomatch:          skip gradient matching step
     --tone &lt;fn&gt;:        specify a T1-weighted MRI
     --freesurfer &lt;dir&gt;: use the given precomputed freesurfer results
     --tracts &lt;dir&gt;:     include user-defined tracts in the workflow 

  Author: Ryan Cabeen
</code></pre>
<p>There are three flags for specifying the input to the workflow: <code>--dwi</code>,
<code>--bvecs</code>, and <code>--bvals</code>.  The b-vectors and b-values can be organized in
column or row form, and QIT will automatically determine the coordinate
transform necessary to align the b-vectors to the imaging data (so there is no
need to flip or transpose b-vectors).  You must also specify a subject
directory with <code>--subject</code>, but if you omit this flag, the current working
directory will be used.  There are several other optional flags that are listed
after the required flags.  You may skip these for now if you like.</p>
<p>Besides the input data, you must also specify one or more <strong>targets</strong>.  A
target is a string that identifies what you want the workflow to produce, e.g.
regional summaries of FA or fiber bundle geometry.  In the next section of the
tutorial we&rsquo;ll describe the different targets that are available.  But to give
a concrete example, you could produce region DTI parameter statistics with the
JHU atlas using a command like so:</p>
<pre><code>$ qitdiff --subject qitsubject --dwi scan/dwi.nii.gz --bvecs scan/bvecs.txt --bvals scan/bvals.txt diff.region/jhu.labels.dti.map
</code></pre>
<p>One of the features of the QIT workflow is that you can add additional analyses and re-use the previous work you did.  So for example, if you wanted to add TBSS to your analysis, you could simply run a command like this:</p>
<pre><code>$ qitdiff --subject qitsubject atlas.region/jhu.labels.tbss.dti.map
</code></pre>
<p>For example, this will skip the data import and atlas registration steps, as
they were already completed.   Note that you can also omit the flags for the
input data, since they have already been imported.</p>
<h2 id="region-of-interest-analysis">Region-of-interest analysis</h2>
<p>Region-of-interest (ROI) analysis is a simple and widely used way to summarize
diffusion MRI data.   In this approach, the data is spatially normalized with
an atlas that contains a parcellation of brain areas, stored in a 3D mask.
Then, diffusion model parameters are statistically summarized in each ROI of
the atlas; for example, by computing the average fractional anisotropy in each
brain area.  There are quite a few variations on this approach, and QIT
supports many of them.  For example, you can combine the ROI approach with
<a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/TBSS">Tract-based Spatial Statistics
(TBSS)</a>, you can apply erosion of
the masks, and you can compute the statistics in either native or atlas space.
Each of these possibilities is available with a different <strong>target</strong> that you
can provide to <code>qitdiff</code>.  Here is a table summarizing the different kinds of
ROI targets that are available:</p>
<ul>
<li><strong>diff.region/jhu.labels.dti.map</strong>:<ul>
<li>DTI parameters statistics for each region in native space</li>
</ul>
</li>
<li><strong>diff.region/jhu.labels.erode.dti.map</strong>: <ul>
<li>DTI parameters statistics for each region in native space with additional erosion step</li>
</ul>
</li>
<li><strong>atlas.region/jhu.labels.dti.map</strong>: <ul>
<li>DTI parameters statistics for each region in atlas space</li>
</ul>
</li>
<li><strong>atlas.region/jhu.labels.erode.dti.map</strong>: <ul>
<li>DTI parameters statistics for each region in atlas space with an additional erosion step</li>
</ul>
</li>
<li><strong>atlas.region/jhu.labels.tbss.dti.map</strong>: <ul>
<li>DTI parameters statistics for each region in atlas space with additional TBSS processing</li>
</ul>
</li>
</ul>
<p>The above examples use the Johns Hopkins University white matter atlas, but
there are many other alternatives in <code>qitdiff</code>.  Below is a table summarizing
these other options.  There are targets available for these, in which you
simply replace <code>jhu.labels</code> in the targets above.</p>
<ul>
<li><strong>jhu.labels</strong>: Johns Hopkins University deep white matter atlas)</li>
<li><strong>jhu.tracts</strong>: Johns Hopkins University white matter tract atlas</li>
<li><strong>fsa.ccwm</strong>: FreeSurfer-based corpus callosum parcellation</li>
<li><strong>fsa.scgm</strong>: FreeSurfer-based subcortical gray matter</li>
<li><strong>fsa.dkwm</strong>: FreeSurfer-based superficial white matter</li>
<li><strong>hox.sub</strong>: Harvard-Oxford subcortical parcellation</li>
<li><strong>ini.bstem</strong>: USC INI brainstem parcellation</li>
<li><strong>cit.amy</strong>: Caltech amygdala subfields</li>
<li><strong>cit.sub</strong>: Caltech subcortical parcellation</li>
</ul>
<p>Furthermore, there are many types of diffusion parameters that can be used
in an ROI analysis.  The examples listed above used diffusion tensor imaging
(DTI) parameters, but if you have multi-shell data, there are other
possibilities supported by <code>qitdiff</code>. The table below lists these other
possibilities, and you can create targets for them by substituting <code>dti</code> with
the appropriate model identifier.</p>
<ul>
<li><strong>dti </strong>:  Diffusion Tensor Imaging (DTI)</li>
<li><strong>fwdti</strong>:  Free-water Elimination DTI (with a fixed diffusivity ball)</li>
<li><strong>dki</strong>:  Diffusion Kurtosis Imaging</li>
<li><strong>noddi</strong>:  Neurite Orientation Dispersion and Density Imaging</li>
<li><strong>mcsmt</strong>:  Multi-compartment Spherical Mean Technique</li>
<li><strong>bti</strong>:  Bi-tensor Imaging (DTI with an unconstrained ball)</li>
</ul>
<p>For a concrete example, if you wanted to compute NODDI parameters in subcortical brain areas, you could run something like so:</p>
<pre><code>$ qitdiff --subject qitsubject --dwi scan/dwi.nii.gz --bvecs scan/bvecs.txt --bvals scan/bvals.txt diff.region/fsa.scgm.noddi.map
</code></pre>
<p>With all of these options, you may be wondering what to pick, and in this case,
a good starting point is the target <code>atlas.region/jhu.region.tbss.dti.map</code>.
This is a fairly standardized approach that summarizes DTI parameters in white
matter areas from the JHU atlas with preprocessing using TBSS.  For example,
this is similar to the protocol used in the <a href="http://enigma.ini.usc.edu/protocols/dti-protocols/">ENIGMA
network</a> (note: the
template and registration algorithm are not identical).</p>
<h2 id="tractography-based-analysis">Tractography-based analysis</h2>
<p>Tractography analysis is an alternative approach for diffusion MRI analysis
that is focused on modeling connectivity as opposed to regions.  In this
approach, geometric models of white matter connectivity are reconstructed from
fiber orientation data estimated from diffusion MRI.  With sufficiently high
quality data, this approach can extract the major pathways of the brain, known
as fiber bundles.  The QIT diffusion workflow includes such a bundle-specific
analysis, enabling the quantitative characterization of both whole bundles and
subdivisions along their length.  These can be run using the following
<code>qitdiff</code> targets:</p>
<ul>
<li><strong>diff.tract/bundles.map</strong>:<ul>
<li>whole bundle parameters, such as volume, length, etc.</li>
</ul>
</li>
<li><strong>diff.tract/bundles.dti.map</strong>:  <ul>
<li>whole bundle DTI parameters, such as FA, MD, etc.</li>
</ul>
</li>
<li><strong>diff.tract/bundles.along.dti.map</strong>: <ul>
<li>along-bundle DTI parameters, e.g. FA measured at each of a sequence of bundle subdivisions</li>
</ul>
</li>
</ul>
<p>Similar to ROI analysis, you can also extract multi-shell diffusion MRI
parameters using any of the model identifiers from the previous section.  For a
concrete example, if you wanted to compute NODDI parameters along each bundle,
you could run something like so:</p>
<pre><code>$ qitdiff --subject qitsubject --dwi scan/dwi.nii.gz --bvecs scan/bvecs.txt --bvals scan/bvals.txt diff.tract/bundles.along.noddi.map
</code></pre>
<h2 id="multi-modal-analysis">Multi-modal analysis</h2>
<p>Besides diffusion MRI, the QIT workflow has integrated morphometric analysis
using T1-weighted MRI.  These steps require ANTs and FreeSurfer to be installed
(as described above).  After that, you can perform cortical surface based
analysis using FreeSurfer using the following command:</p>
<pre><code>$ qitdiff --subject qitsubject --tone t1.nii.gz tone.fs.map
</code></pre>
<p>If you&rsquo;ve already run FreeSurfer, you can also import the previous results, e.g.</p>
<pre><code>$ qitdiff --subject qitsubject --freesurfer fs_subject_dir tone.fs.map
</code></pre>
<p>You can also perform a multi-modal image analysis that combines the T1-weighted
and diffusion MRI data.  For example, you could compute DTI parameter
statistics in subject-specific subcortical and superficial white matter ROIs
using this command:</p>
<pre><code>$ qitdiff --subject qitsubject --dwi scan/dwi.nii.gz --bvecs scan/bvecs.txt --bvals scan/bvals.txt --tone t1.nii.gz tone.region/fs.scgm.dti.map tone.region/fs.dkwm.dti.map
</code></pre>
<p>There are various other T1-related targets available, which are summarized here:</p>
<ul>
<li><strong>tone.fs.brain</strong>:<ul>
<li>Freesurfer brain image and tissue types converted into nifti</li>
</ul>
</li>
<li><strong>tone.fs.region</strong>:<ul>
<li>Freesurfer regions-of-interest converted into nifti</li>
</ul>
</li>
<li><strong>tone.fs.surfaces</strong>:<ul>
<li>Freesurfer meshes converted into VTK</li>
</ul>
</li>
<li><strong>diff.fs.brain</strong>:<ul>
<li>Freesurfer brain image and tissue types deformed into native diffusion space</li>
</ul>
</li>
<li><strong>diff.fs.region</strong>:<ul>
<li>Freesurfer regions-of-interest deformed into native diffusion space</li>
</ul>
</li>
<li><strong>diff.fs.surfaces</strong>:<ul>
<li>Freesurfer meshes deformed into native diffusion space</li>
</ul>
</li>
<li><strong>tone.region/fs.scgm.dti.map</strong>:<ul>
<li>DTI parameters statistics for each Freesurfer subcortical ROI</li>
</ul>
</li>
<li><strong>tone.region/fs.dkwm.dti.map</strong>:<ul>
<li>DTI parameters statistics for each Freesurfer Desikan white matter ROI</li>
</ul>
</li>
<li><strong>tone.region/fs.dkgm.dti.map</strong>:<ul>
<li>DTI parameters statistics for each Freesurfer Desikan gray matter ROI</li>
</ul>
</li>
</ul>
<h2 id="aggregating-results">Aggregating results</h2>
<p>Once you&rsquo;ve finished processing your data, you may want to combine the results into data tables, that is, aggregating metrics from all of the research subjects into a spreadsheet.  The QIT workflow was designed to make this easy by saving results in a standardized format.  You may notice that each target ends in <code>map</code>; this indicates that the target is a directory that contains summary statistics.  Each <code>map</code> directory contains an array of CSV files store simple name-value pairs.  For example, the bundle-specific analysis produces this CSV file among many others:</p>
<pre><code>$ head diff.tract/bundles.map/volume.csv 
name,value
lh_acoustic,13570.0
lh_thal_tempinf,9663.0
rh_arc_ant,51518.0
rh_thal_pma,21428.0
rh_med_lem,11603.0
rh_aft,6183.0
...
</code></pre>
<p>Suppose you have many subjects, and you have organized the data such that each subject has a QIT workflow subject directory, e.g. <code>test_103818</code>, inside a directory named <code>subjects</code>.  Further, you have a list of subject identifiers that correspond to the subject directory names, and these names are stored one-per-line in a text file named <code>sids.txt</code>.  Then, you can create a data table and inspect the output using these commands:</p>
<pre><code>$ qit --verbose MapCat --pattern subjects/%{subject}/diff.tract/bundles.map/volume.csv  --vars subject=sids.txt --skip --output bundles.volume.csv
$ cat bundles.volume.csv | grep lh_acoustic | head
test_103818,lh_acoustic,13386.0
test_105923,lh_acoustic,17255.0
test_111312,lh_acoustic,12189.0
test_114823,lh_acoustic,15710.0
test_115320,lh_acoustic,14599.0
...
</code></pre>
<p>The resulting table is in a <em>long</em> format, but you can also convert it to a
<em>wide</em> one:</p>
<pre><code>$ qit TableWiden --input bundles.volume.csv --output bundles.volume.wide.csv
$ head bundles.volume.wide.csv
subject,lh_acoustic,lh_arc,...
test_103818,13386.0,8950.0,...
test_105923,17255.0,13213.0,...
test_111312,12189.0,11033.0,...
...
</code></pre>
<p>In this table, each column is a different brain area, and each row is a
subject.  We used bundle volume in the example, but the same approach works for
all of the other metrics stored in <code>map</code> directories.  If you would like to
create tables for many different variables, there is a more complex but
powerful script called <code>qitmaketables</code> that you can check out.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>If you find QIT is valuable in your work, we ask that you clearly
<a href="../citation/">acknowledge</a> it any publications or grant proposals.  This
is greatly appreciated, as it improves the reproducibility of your findings,
and it helps our team maintain resources for continued support and development.
You can cite QIT by including a link to the main website at
<a href="http://cabeen.io/qitwiki">http://cabeen.io/qitwiki</a> and including the
following citation in the manuscript:</p>
<pre><code class="language-lang-none">Cabeen RP, Laidlaw DH, Toga AW. 2018. Quantitative Imaging Toolkit: Software
for Interactive 3D Visualization, Data Exploration, and Computational Analysis
of Neuroimaging Datasets. Proceedings of the Annual Meeting of the Society for
Magnetic Resonance in Medicine (ISMRM).  Paris, France 2018 (p. 2854)
</code></pre>
<p>If you use the region-of-interest analysis, you can include the following reference, which describes and evaluates these components:</p>
<pre><code class="language-lang-none">Cabeen, R.P., Bastin, M.E. and Laidlaw, D.H., 2017. A Comparative evaluation of
voxel-based spatial mapping in diffusion tensor imaging. Neuroimage, 146,
pp.100-112.
</code></pre>
<p>If you used the tractography-based analysis, you can include the following references, which describe and evaluate the methods for tractography and template construction:</p>
<pre><code class="language-lang-none">Cabeen, R.P., Bastin, M.E. and Laidlaw, D.H., 2016. Kernel regression
estimation of fiber orientation mixtures in diffusion MRI. Neuroimage, 127,
pp.158-172.
</code></pre>
<pre><code class="language-lang-none">Cabeen, R.P., Toga, A.W., 2020. Reinforcement tractography: a hybrid approach
for robust segmentation of complex fiber bundles. International Symposium on
Biomedical Imaging (ISBI) 2020
</code></pre>
<p>Besides QIT references, you should also please cite the other dependent tools and resources where appropriate:</p>
<pre><code class="language-lang-none">Jenkinson, M., Beckmann, C.F., Behrens, T.E., Woolrich, M.W. and Smith, S.M.,
2012. Fsl. Neuroimage, 62(2), pp.782-790.
</code></pre>
<pre><code class="language-lang-none">Behrens, T.E., Berg, H.J., Jbabdi, S., Rushworth, M.F. and Woolrich, M.W.,
2007. Probabilistic diffusion tractography with multiple fibre orientations:
What can we gain?. Neuroimage, 34(1), pp.144-155.
</code></pre>
<pre><code class="language-lang-none">Zhang, H., Yushkevich, P.A., Alexander, D.C. and Gee, J.C., 2006. Deformable
registration of diffusion tensor MR images with explicit orientation
optimization. Medical image analysis, 10(5), pp.764-785.
</code></pre>
<pre><code class="language-lang-none">Zhang, S., Peng, H., Dawe, R.J. and Arfanakis, K., 2011. Enhanced ICBM
diffusion tensor template of the human brain. Neuroimage, 54(2), pp.974-984.
</code></pre>
<pre><code class="language-lang-none">Zhang, H., Yushkevich, P.A., Alexander, D.C. and Gee, J.C., 2006. Deformable
registration of diffusion tensor MR images with explicit orientation
optimization. Medical image analysis, 10(5), pp.764-785.
</code></pre>
<pre><code class="language-lang-none">Basser, P.J., Mattiello, J. and LeBihan, D., 1994. MR diffusion tensor
spectroscopy and imaging. Biophysical journal, 66(1), pp.259-267.
</code></pre>
<pre><code class="language-lang-none">Zhang, H., Schneider, T., Wheeler-Kingshott, C.A. and Alexander, D.C., 2012.
NODDI: practical in vivo neurite orientation dispersion and density imaging of
the human brain. Neuroimage, 61(4), pp.1000-1016.
</code></pre>
<pre><code class="language-lang-none">Hoy, A.R., Koay, C.G., Kecskemeti, S.R. and Alexander, A.L., 2014. Optimization
of a free water elimination two-compartment model for diffusion tensor imaging.
Neuroimage, 103, pp.323-333.
</code></pre>
<pre><code class="language-lang-none">Kaden, E., Kelm, N.D., Carson, R.P., Does, M.D. and Alexander, D.C., 2016.
Multi-compartment microscopic diffusion imaging. NeuroImage, 139, pp.346-359.
</code></pre>
<pre><code class="language-lang-none">Fieremans, E., Jensen, J. H., &amp; Helpern, J. A. (2011). White matter
characterization with diffusional kurtosis imaging. Neuroimage, 58(1), 177-188.
</code></pre>
<pre><code class="language-lang-none">Sepehrband, F., Cabeen, R.P., Choupan, J., Barisano, G., Law, M., Toga, A.W.
and Alzheimer's Disease Neuroimaging Initiative, 2019. Perivascular space fluid
contributes to diffusion tensor imaging changes in white matter. NeuroImage,
197, pp.243-254.
</code></pre>
<pre><code class="language-lang-none">Mori, S., Oishi, K., Jiang, H., Jiang, L., Li, X., Akhter, K., Hua, K., Faria,
A.V., Mahmood, A., Woods, R. and Toga, A.W., 2008. Stereotaxic white matter
atlas based on diffusion tensor imaging in an ICBM template. Neuroimage, 40(2),
pp.570-582.
</code></pre>
<pre><code class="language-lang-none">Smith, S.M., Jenkinson, M., Johansen-Berg, H., Rueckert, D., Nichols, T.E.,
Mackay, C.E., Watkins, K.E., Ciccarelli, O., Cader, M.Z., Matthews, P.M. and
Behrens, T.E., 2006. Tract-based spatial statistics: voxelwise analysis of
multi-subject diffusion data. Neuroimage, 31(4), pp.1487-1505.
</code></pre>
<pre><code class="language-lang-none">Pauli, W.M., Nili, A.N. and Tyszka, J.M., 2018. A high-resolution probabilistic
in vivo atlas of human subcortical brain nuclei. Scientific data, 5, p.180063.
</code></pre>
<pre><code class="language-lang-none">Tyszka, J.M. and Pauli, W.M., 2016. In vivo delineation of subdivisions of the
human amygdaloid complex in a high‐resolution group template. Human brain
mapping, 37(11), pp.3979-3998.
</code></pre>
<pre><code class="language-lang-none">Tang, Y., Sun, W., Toga, A.W., Ringman, J.M. and Shi, Y., 2018. A probabilistic
atlas of human brainstem pathways based on connectome imaging data. Neuroimage,
169, pp.227-239.
</code></pre>
<pre><code class="language-lang-none">Desikan, R.S., Ségonne, F., Fischl, B., Quinn, B.T., Dickerson, B.C., Blacker,
D., Buckner, R.L., Dale, A.M., Maguire, R.P., Hyman, B.T. and Albert, M.S.,
2006. An automated labeling system for subdividing the human cerebral cortex on
MRI scans into gyral based regions of interest. Neuroimage, 31(3), pp.968-980.
</code></pre>
<pre><code class="language-lang-none">Fischl, B., 2012. FreeSurfer. Neuroimage, 62(2), pp.774-781.
</code></pre>
<pre><code class="language-lang-none">Avants, B.B., Tustison, N. and Song, G., 2009. Advanced normalization tools
(ANTS). Insight j, 2(365), pp.1-35.
</code></pre>
<h2 id="closing">Closing</h2>
<p>This concludes the tutorial for the QIT diffusion workflow.  This was written to provide only a brief overview, so you may be left wondering about how some things work.  If so, please feel free to reach out with any questions or thoughts via <a href="mailto:cabeen@gmail.com">cabeen@gmail.com</a>.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../masters-workshop/" class="btn btn-neutral float-right" title="Masters Workshop">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../models/" class="btn btn-neutral" title="Models"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/cabeen/qit/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../models/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../masters-workshop/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
